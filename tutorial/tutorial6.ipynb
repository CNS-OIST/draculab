{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 6\n",
    "**PART 1**  \n",
    "Creating unique models:\n",
    "* Creating units with multiport inputs.\n",
    "* Creating new requirements.\n",
    "\n",
    "**PART 2**  \n",
    "Simulations using the **ei_net** class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "### Units with multiport inputs and new requirements\n",
    "\n",
    "For some unit models, there may be several types of qualitatively different inputs. An example can be found in the `delta_linear` class of `custom_units.py`. This class, together with the `delta_synapse` class in in `synapses.py` implements a continuous-time version of the _delta_ learning rule.\n",
    "\n",
    "The _delta_ learning rule is one of the most widespread forms of supervised learning in neural networks. Given a _training set_ consisting of  input vectors \n",
    "$(\\bf{x_1}, \\dots, \\bf{x_n})$ and their corresponding desired outptus $(y_1, \\dots, y_n)$, the purpose of the delta rule is to adjust the input weights of a unit so\n",
    "the response to $\\bf{x_i}$ is $y_i$. Without going into details, this can sometimes be achieved by presenting the inputs one by one, each time adjusting the weights using:\n",
    "\n",
    "$\\Delta \\omega_{j} = \\alpha (y - u)x_j$,\n",
    "\n",
    "where $y$ is the desired response to the input, $u$ is the actual response, $x_j$ is the $j$-th component of the input vector, $\\omega_j$ is the corresponding weight, and $\\alpha$ is a learning rate.  \n",
    "To implement this, we require to know not only the input to the unit, but also the desired output. Since the error $e \\equiv y - u$ is used by every synapse,\n",
    "it is computationally efficient to calculate it once in the unit class and make it available to all synapses. Thus, the error is a synaptic _requirement_ that must be updated at every simulation step.\n",
    "\n",
    "This tutorial is a guide to the source code in the `delta_linear`, and `delta_synapse` classes (found in `custom_units.py` and \n",
    "`synapses.py` respectively),\n",
    "which illustrate the tools used to create units with multiple input ports, and synapses that use custom requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We can begin by looking at the docstring of the two classes\n",
    "from draculab import *\n",
    "from units.custom_units import delta_linear\n",
    "from synapses.synapses import delta_synapse\n",
    "help(delta_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "help(delta_synapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a unit with many input ports is the same as creating any other type of unit. As was shown in tutorial 5, you just need to\n",
    "register its name in `draculab.py`, and also write `init` and `derivatives`or `dt_fun` methods. The difference comes when you write\n",
    "what's inside of the `derivatives` (or `dt_fun`) function.\n",
    "\n",
    "In tutorial 5, the binary unit received the input sum from the `get_input_sum` function in `derivatives`, and from `inp_sum` in `dt_fun`.\n",
    "Both `get_input_sum` and `inp_sum` ignore the input ports, but depending on the type of unit, this may not be appropriate.\n",
    "For example, for the `delta_linear` unit we don't want to add the error (at port 1) or the learning trigger (at port 2) into the input\n",
    "sum. Thus, the input sum should only consider inputs at port 0. Draculab offers tools for multiport units to deal with these type\n",
    "of situations.\n",
    "\n",
    "When the creator method of the `unit` class receives a parameter dictionary with the `n_ports` (number of ports) entry, and that entry has a \n",
    "value larger than 1, then the unit is considered a _multiport_ unit, and the `multiport` attribute is set to `True`. This\n",
    "causes the `init_pre_syn_update` method of the unit to add a new attribute, called `port_idx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***** Quoting the source code comments at 'init_pre_syn_update' *****\n",
    "\n",
    "# If we require support for multiple input ports, create the port_idx list.\n",
    "# port_idx is a list whose elements are lists of integers.\n",
    "# port_idx[i] contains the indexes in net.syns[self.ID]\n",
    "# (or net.delays[self.ID], or net.act[self.ID])\n",
    "# of the synapses whose input port is 'i'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `port_idx` list we can write a version of `get_input_sum` that considers only the inputs from port 0.\n",
    "This is what the `delta_linear.get_mp_input_sum` does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mp_input_sum(self,time):                                                                                      \n",
    "    \"\"\" The input sum function of the delta_linear unit. \"\"\"                                                              \n",
    "    return  sum( [syn.w * act(time - dely) for syn, act, dely in zip(                                                 \n",
    "                 [self.net.syns[self.ID][i] for i in self.port_idx[0]],                                               \n",
    "                 [self.net.act[self.ID][i] for i in self.port_idx[0]],                                                \n",
    "                 [self.net.delays[self.ID][i] for i in self.port_idx[0]])] ) / self.inp_l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the sum is divided by `inp_l2`. This quantity is the L2 norm of the input vector, and is a synaptic\n",
    "requirement, which means it will be updated at each iteration step, every `min_delay` time units. The requirements of\n",
    "a unit are added in its `__init__` method, at the line where the `syn_needs` set is updated.\n",
    "\n",
    "An alternative to using `port_idx` for defining `get_mp_input_sum` is to use the `get_mp_inputs`, and `get_mp_weights` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from units.units import unit\n",
    "help(unit.get_mp_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(unit.get_mp_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "Write `get_mp_input_sum` using `get_mp_inputs`.  \n",
    "(solution at the bottom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of flat networks things are a bit different. For units with a single input port we obtained the input sum\n",
    "from the `inp_sum` array present in _flattened_ units. This array is updated at each simulation step by the `unit.upd_flat_inp_sum`\n",
    "method, which is invoked by `network.flat_update`. This later method is the one tasked with updating the activities of all units\n",
    "and plants in the network, using them to fill the `network.acts` array.\n",
    "\n",
    "When a flattened unit requires the inputs segregated by port, then `upd_flat_inp_sum` should be replaced by `upd_flat_mp_inp_sum`,\n",
    "which produces the `mp_inp_sum` array. Whereas `inp_sum` is a 1-dimensional array, `mp_inp_sum` is 2-dimensional. \n",
    "`mp_inp_sum[i]` is a 1D array, the equivalent of `inp_sum` if we only consider inputs at port `i`. The `upd_flat_mp_inp_sum` for linear\n",
    "units will be used only if the `needs_mp_inp_sum` attribute of the unit is set to `True`, which is done in the `__init__` method.\n",
    "\n",
    "From this discussion, it is straightforward to understand the `delta_linear.dt_fun` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dt_fun(self, y, s):\n",
    "    \"\"\" Returns the derivative when state is y, at time substep s. \"\"\" \n",
    "    return ( self.gain * self.mp_inp_sum[0][s] / self.inp_l2 + self.bias - y ) / self.tau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point the source code in `delta_linear.__init__`, and `delta_linear.derivatives` should be easy to understand.\n",
    "Please read it now, if you have not done it yet.\n",
    "\n",
    "What is still not clear is how to implement the `error` requirement that will be used by the `delta_synapse` class.\n",
    "Creating a new _requirement_ takes 3 basic steps:\n",
    "1. Register the requirement's name in `draculab.py`.\n",
    "2. Create an initialization method in `requirements.py`.\n",
    "3. Create a function to update the requirement, either in `units.py` or in `custom_units.py`.\n",
    "\n",
    "The first step is similar to how things are done when creating a new unit, plant, or synapse type. There is an _Enum_ subclass called\n",
    "`syn_reqs` in `draculab.py`. All that is required is to add the name of your requirement here, assigning it a unique integer.\n",
    "\n",
    "For the second step, the following \"notes\" (from the source code in `init_pre_syn_update`) are illuminating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "        DEVELOPER'S NOTES: \n",
    "        The names of the possible requirements are in the syn_reqs Enum in draculab.py .\n",
    "        Any new requirement needs to register its name in there.\n",
    "        Implementation of a requirement has 2 main parts:\n",
    "        1) Initialization: where the data structures used by the requirement get their\n",
    "           initial values. This is done here by calling the 'add_<req_name>' functions.\n",
    "           These functions, defined in requirements.py, add the necessary attributes\n",
    "           and initialize them.\n",
    "        2) Update: a method with the name upd_<req name>  that is added to the\n",
    "           'functions' list of the unit. This method usually belongs to the 'unit' \n",
    "           class and is written somewhere after init_pre_syn_update, but in\n",
    "           some cases it is defined only for one of the descendants of 'unit'. It is\n",
    "           good practice to write the location of the update method in the\n",
    "           docstring of the 'add_<req_name>' function.\n",
    "           \n",
    "           The name of the requirement, as it appears in the syn_reqs Enum, must\n",
    "           also be used in the 'add_' and 'upd_' methods. Failing to use this\n",
    "           naming convention will result in failure to add the requirement.\n",
    "\n",
    "           For example, the LPF'd activity with a fast time\n",
    "           constant has the name 'lpf_fast', which is its name in the syn_reqs\n",
    "           Enum. The file requirements.py has the 'add_lpf_fast' method, and\n",
    "           the method that updates 'lpf_fast' is called upd_lpf_fast.\n",
    "\n",
    "        Additionally, some requirements have buffers so they can provide their value\n",
    "        as it was 'n' simulation steps before. The prototypical case is the 'lpf'\n",
    "        variables, which are retrieved by synapses belonging to a target cell, so \n",
    "        the value they get should have a propagation delay. This requires two other\n",
    "        things in the implementation:\n",
    "        1) Initialization of the requirement's buffer is added in unit.init_buffers.\n",
    "           The buffers should have the name <req name>_buff.\n",
    "        2) There are 'getter' methods to retrieve past values of the requirement.\n",
    "           The getter methods are named get_<req_name> .\n",
    "           \n",
    "        Requirements may optionally have a priority number. Requirements with a\n",
    "        lower priority number will be executed first. This is useful when one \n",
    "        requirement uses the value of another for its update. By default all\n",
    "        requirements have priority 3. This can be changed in the 'get_priority'\n",
    "        function of the syn_reqs class.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you can look at the `add_error` and `add_inp_l2` methods in `requirements.py`.\n",
    "They are providing the unit with the new attributes used by the requirement, and checking whether any\n",
    "prerequisite requirements are present. In this case, we want the `mp_inputs` requirement to be present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "What does `mp_inputs` do?  \n",
    "What is the priority number of `mp_inputs`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "Does it make sense to write a version of `get_mp_input_sum` that takes advantage of the `mp_inputs` requirement?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `add_error` method will create `error` and `learning` attributes in the unit.\n",
    "Now we need a method to update those attributes. This method needs to satisfy two requisites: it has to be available to the `delta_linear`\n",
    "class, and its name must be `upd_error`. As mentioned in the notes above, there are naming conventions to follow when creating a\n",
    "requirement. The requirement can have any arbitrary name, but the attribute that implements it must share that name, and the name\n",
    "must be used in the function that initializes it (`add_<req name>`) and in the function that updates it (`upd_<req name>`). There are reasons for these conventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE 4\n",
    "\n",
    "# In unit.init_pre_syn_update we can find the code that adds the \n",
    "# upd_<req name> method to the list of functions that must be executed\n",
    "# at every simulation step to update requirements. This list is\n",
    "# named 'functions'.\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # Prepare all the requirements to be updated by the pre_syn_update function. \n",
    "        for req in self.syn_needs:\n",
    "            if issubclass(type(req), syn_reqs):\n",
    "                eval('add_'+req.name+'(self)')\n",
    "            else:\n",
    "                raise NotImplementedError('Asking for a requirement that is not implemented')\n",
    "        # self.functions must be a list sorted according to priority.\n",
    "        # Thus we turn the set syn_needs into a list sorted by priority\n",
    "        priority = lambda x: x.get_priority()\n",
    "        syn_needs_list = sorted(self.syn_needs, key=priority)\n",
    "        self.functions = [eval('self.upd_'+req.name, {'self':self})\n",
    "                          for req in syn_needs_list]\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "# Can you see how the naming conventions are used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the code of the `delta_synapse` class, there is nothing new about it. The only thing\n",
    "to remark is that it uses the `error` requirement that we created, and that various alternative configurations\n",
    "can be used by commenting and uncommenting source code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "Have a look at the source code of `delta_synapse`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "### The ei_net class\n",
    "Now we will create a delta unit and test it using random input vectors with unit norm.\n",
    "The desired output will be the norm of the projection of the input vector on a given vector $\\bf{v}$.\n",
    "If the learning algorithm works, the weight vector of the delta unit must approach $\\bf{v}$.\n",
    "\n",
    "To create this simulation we use the `ei_net` class from `tools/ei_net.py`.  \n",
    "This class is used to configure  and run simulations involving three different populations, called **e**, **i**, and **x**.  \n",
    "These three populations are meant to contain excitatory, inhibitory, and source units respectively. An instance of the `ei_net` class contains default parameter dictionaries for the three populations, and for their connections. In theory we could just create an instance of `ei_net`, run `ei_net.build`, and then start running simulations with `ei_net.run`. Of course, those simulations would not use the network we want to simulate, so we need to adapt the parameters of `ei_net` for this end.\n",
    "\n",
    "We want the **e** population to contain a single `delta_linear` unit, the **i** population to be empty, and the **x** population to contain __inp_dim__ source units, where __inp_dim__ stands for the dimensionality of the input vectors. Moreover, we want to provide our `delta_linear` unit with the desired output for each input, and with the signal that triggers learning.\n",
    "\n",
    "This part of the tutorial shows how to configure `ei_net` for this end. Given the multiple input dimensionality of the `delta_linear` unit, the `ei_network` class would be a better tool, but for the purpose of this demonstration `ei_net` is appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ei_net in module tools.ei_net:\n",
      "\n",
      "class ei_net(builtins.object)\n",
      " |  ei_net(net_number=None)\n",
      " |  \n",
      " |  This class is used to create, simulate, and visualize a generic network containing 3\n",
      " |  populations: excitatory (e), inhibitory (i), and input (x).  The three populations are \n",
      " |  arranged in a rectangular grid. \n",
      " |  \n",
      " |  After creating an instance of the ei_net class, the parameters of the network can be\n",
      " |  configured by changing the entries of the parameter dictionaries using the ei_net.set_param \n",
      " |  method. Once the parameter dictionaries are as desired the network is created by running\n",
      " |  ei_net.build() . All units and connections are created using the topology module.\n",
      " |  \n",
      " |  An additional 'w_track' population of source units may also be created in order to\n",
      " |  track the temporal evolution of a random sample of synaptic weights.\n",
      " |  Similarly, 'sc_track' or 'thr_track' populations may be created in order to track\n",
      " |  the the values of some scale factors (when using exp_dist_sig units) or thresholds\n",
      " |  (when using exp_dist_sig_thr units).\n",
      " |  \n",
      " |  See tutorial6.ipynb for a usage example.\n",
      " |  \n",
      " |  METHODS OVERVIEW \n",
      " |  __init__ : creates parameter dictionaries with the default values.\n",
      " |  set_param : changes a value in a parameter dictionary.\n",
      " |  build : creates units and connects them.\n",
      " |  run : runs simulations.\n",
      " |  mr_run : runs simulations for units with multiple input ports.\n",
      " |  basic_plot, act_anim, hist_anim, double_anim : result visualization.\n",
      " |  conn_anim : connections visualization.\n",
      " |  act_anim : animation showing the activity of all units through time\n",
      " |  hist_anim : animation showing the histogram of unit activities through time.\n",
      " |  double_anim : combines the visualizations of act_anim and hist_anim.\n",
      " |  annotate : append a line with text in the ei_net.notes string.\n",
      " |  log : save the parameter changes and execution history of the network in a text file.\n",
      " |  save : pickle the object and save it in a file.\n",
      " |  -------- \n",
      " |  make_sin_pulse, input_vector, default_inp_pat, default_inp_fun : auxiliary to 'run'.\n",
      " |  default_mr_inp_pat : auxiliary to mr_run.\n",
      " |  make_inp_fun : auxiliary to default_inp_fun.\n",
      " |  H : the Heaviside step function. Auxiliary to make_inp_fun.\n",
      " |  color_fun : auxiliary to act_anim and double_anim.\n",
      " |  update_(conn|hist|act|double)_anim : auxiliary to (conn|hist|act|double)_anim.\n",
      " |  update_weight_anim : auxiliary to conn_anim.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  H(self, x)\n",
      " |      The Heaviside step function.\n",
      " |  \n",
      " |  __init__(self, net_number=None)\n",
      " |      Create the parameter dictionaries required to build the excitatory-inhibitory network. \n",
      " |      \n",
      " |      Time units are seconds for compatibility with the pendulum model.\n",
      " |      Length units for geometry dictionaries are centimeters.\n",
      " |      Axonal conduction speed is ~1 m/s, synaptic delay about 5 ms for the population response.\n",
      " |      \n",
      " |      The slope, thresh, tau, and init_val parameters of sigmoidal units are by default set \n",
      " |      randomly using the formula:  par = par_min + par_wid * rand ,\n",
      " |      where par is either slope, thresh, tau, or init_val; par_min is the minimum value; \n",
      " |      par_wid is the 'width' of the parameter distribution; and rand is a random value coming \n",
      " |      from the uniform distribution in the [0,1) interval.\n",
      " |      To set a different distribution for these parameters, just set the desired value(s) in\n",
      " |      the 'slope', 'thresh', 'tau', or 'init_val' entry of the (e|i)_pars dictionary.\n",
      " |      \n",
      " |      The argument net_number is an integer to identify the object in multiprocess simulations;\n",
      " |      see ei_runner_mp.ipynb .\n",
      " |  \n",
      " |  act_anim(self, pop, thr, interv=100, slider=False)\n",
      " |      An animation to visualize the activity of a set of units. \n",
      " |      \n",
      " |      pop : an array or list with the IDs of the units to visualize.\n",
      " |      interv : refresh interval of the simulation.\n",
      " |      slider : When set to True the animation is substituted by a slider widget.\n",
      " |      thr : units whose activity surpasses 'thr' will be highligthted.\n",
      " |  \n",
      " |  annotate(self, string, make_history=True)\n",
      " |      Append a string to self.notes and self.history.\n",
      " |  \n",
      " |  basic_plot(self)\n",
      " |  \n",
      " |  build(self)\n",
      " |      Create the draculab network.\n",
      " |  \n",
      " |  color_fun(self, activ)\n",
      " |  \n",
      " |  conn_anim(self, source, sink, interv=100, slider=False, weights=True)\n",
      " |      An animation to visualize the connectivity of populations. \n",
      " |      \n",
      " |      source and sink are lists with the IDs of the units whose connections we'll visualize. \n",
      " |      \n",
      " |      Each frame of the animation shows: for a particular unit in source,\n",
      " |      all the neurons in sink that receive connections from it (left plot), and for \n",
      " |      a particular unit in sink, all the units in source that send it connections\n",
      " |      (right plot).\n",
      " |      \n",
      " |      interv is the refresh interval (in ms) used by FuncAnimation.\n",
      " |      \n",
      " |      If weights=True, then the dots' size and color will reflect the absolute value\n",
      " |      of the connection weight.\n",
      " |      \n",
      " |      Returns:\n",
      " |          animation object from FuncAnimation if slider = False\n",
      " |          widget object from ipywidgets.interact if slider = True\n",
      " |  \n",
      " |  default_inp_fun(self, pre_inp, cur_inp, init_time, pres_time, inp_units, frac=0.2)\n",
      " |      A default set_inp_fun argument for the run() method. \n",
      " |      \n",
      " |      The input function transitions linearly from the previous to the current input\n",
      " |      pattern during the first 'frac' fraction of the presentation.\n",
      " |  \n",
      " |  default_inp_pat(self, pres, rows, columns)\n",
      " |      A default set_inp_pat argument for the run() method. \n",
      " |      \n",
      " |      The returned input patterns are random vectors with some unit norm.\n",
      " |  \n",
      " |  default_mr_inp_pat(self, pres, rows, columns, port)\n",
      " |      A default set_mr_inp_pat argument for the mr_run() method. \n",
      " |      \n",
      " |      The returned input patterns are random vectors with some unit norm. \n",
      " |      Port number is ignored.\n",
      " |  \n",
      " |  double_anim(self, pop, nbins=20, interv=100, slider=False, thr=0.9, pdf=False)\n",
      " |      An animation of both firing rate histograms and unit activities for a given population.\n",
      " |      \n",
      " |      pop : list or array with the IDs of the units to visualize.\n",
      " |      bins = number of bins in the firing rate histogram\n",
      " |      inter = refresh interval in ms\n",
      " |      slider = whether to use an interactive slider instead of an animation\n",
      " |      thr = value at which the dots change color in the unit activity diagram\n",
      " |      pdf = whether to overlay a plot of the exponential pdf the histogram should approach.\n",
      " |           CURRENTLY ONLY SUPPORTED WHEN pop IS THE EXCITATORY POPULATION.\n",
      " |  \n",
      " |  hist_anim(self, pop, nbins=20, interv=100, slider=False, pdf=False)\n",
      " |      An animation to visualize the firing rate histogram of a given population through time. \n",
      " |      \n",
      " |      pop : list or array with the IDs of the units whose histogram we'll visualize.\n",
      " |      interv : refresh interval of the simulation.\n",
      " |      slider : When set to True the animation is substituted by a slider widget.\n",
      " |      nbins : number of bins in the histogram\n",
      " |      pdf: include a plot of the exponential PDF?\n",
      " |           Used when visualizing exp_dist_sig units or exp_rate_dist synapses.\n",
      " |           CURRENTLY ONLY SUPPORTED WHEN pop IS THE EXCITATORY POPULATION.\n",
      " |  \n",
      " |  log(self, name='ei_net_log.txt', params=True)\n",
      " |      Write the history and notes of the ei_net object in a text file.\n",
      " |      \n",
      " |      name : A string with the name and path of the file to be written.\n",
      " |      params : A boolean to indicate including the parameter dictionaries used to build the object.\n",
      " |  \n",
      " |  make_inp_fun(self, prev, cur, init_time, tran_time)\n",
      " |      Creates an input function that from init_time to tran_time goes linearly from prev to cur, \n",
      " |      and remains constant with value cur afterwards.\n",
      " |      \n",
      " |      Auxiliary to default_inp_fun to avoid scoping problems:\n",
      " |      https://eev.ee/blog/2011/04/24/gotcha-python-scoping-closures/\n",
      " |  \n",
      " |  make_sin_pulse(self, t_init, t_end, per, amp)\n",
      " |      This function returns a function implementing a sinusoidal bump(s).\n",
      " |  \n",
      " |  mr_run(self, n_pres, pres_time, set_mr_inp_pat=None, set_inp_fun=None, flat=False)\n",
      " |      Run a simulation, presenting n_pres patterns, each lasting pres_time. \n",
      " |      \n",
      " |      This method is used instead of 'run' when units have multiple input ports (multiple \n",
      " |      receivers or multiple dendritic branches), and the inputs target them in the particular\n",
      " |      arrangement described below.\n",
      " |      All target units should have the same number of ports.\n",
      " |      \n",
      " |      It is assumed that the grid of input units is connected so that units in column 'p' are \n",
      " |      the inputs to port 'p' of the e,i populations. Thus, the number of columns should equal \n",
      " |      the number of distinct ports to be targeted, and the number of rows should equal the number \n",
      " |      of different inputs to provide at each port.\n",
      " |      \n",
      " |      n_pres : number of pattern presentations to simulate.\n",
      " |      pres_time : time that each pattern presentation will last.\n",
      " |      \n",
      " |      At the beginning of each presentation, the method set_mr_inp_pat is called. This creates a new\n",
      " |      pattern to be presented. Then the set_inp_fun method is called, which sets the functions\n",
      " |      of the input units based on the new pattern.\n",
      " |      \n",
      " |      set_mr_inp_pat(pres, rows, columns, port)\n",
      " |          # Given a presentation number, the number of rows and columns in the input layer, and\n",
      " |          # the port where the input should be applied, returns an input pattern, which is a 1-D \n",
      " |          # numpy array with the value that the input function should attain during the presentation.\n",
      " |          # To map a given pair (r,c) to its corresponding entry in the returned vector use:\n",
      " |          # idx = rows*c + r, \n",
      " |          # e.g. input[idx] corresponds to the unit in row r, and column c, with the indexes starting\n",
      " |          # from 0. This is consistent with the way coordinates are assigned in topology.create_group,\n",
      " |          # and the 2-D pattern can be recovered with numpy.reshape(input, (rows, columns)).\n",
      " |          \n",
      " |      set_inp_fun(prev_inp_pat, cur_inp_pat, init_time, pres_time, inp_units))\n",
      " |          # Assigns a Python function to each of the input units.\n",
      " |          # pre_inp_pat : input pattern from the previous presentation (in the format of set_inp_pat).\n",
      " |          # cur_inp_pat : current input pattern.\n",
      " |          # init_time : time when the presentation will start.\n",
      " |          # pres_time : duration of the presentation.\n",
      " |          # inp_units : a list with the input units (e.g. \"x\").\n",
      " |      flat : A  binary value indicating whether to use flat_run instead of run\n",
      " |      \n",
      " |      If the set_inp_pat or set_inp_fun arguments are not provided, the class defaults are used.\n",
      " |      \n",
      " |      The main use of this method has been in testing double sigma units in test10.ipynb.\n",
      " |      \n",
      " |      Updates:\n",
      " |          self.all_times: 1-D numpy array with the times for each data point in all_activs.\n",
      " |          self.all_activs: 2-D numpy array with the activity of all units at each point in all_times.\n",
      " |  \n",
      " |  run(self, n_pres, pres_time, set_inp_pat=None, set_inp_fun=None, flat=False)\n",
      " |      Run a simulation, presenting n_pres patterns, each lasting pres_time. \n",
      " |      \n",
      " |      n_pres : number of pattern presentations to simulate.\n",
      " |      pres_time : time that each pattern presentation will last.\n",
      " |      \n",
      " |      At the beginning of each presentation, the method set_inp_pat is called. This creates a new\n",
      " |      pattern to be presented. Then the set_inp_fun method is called, which sets the functions\n",
      " |      of the input units based on the new pattern.\n",
      " |      \n",
      " |      set_inp_pat(pres, rows, columns)\n",
      " |          # Given a presentation number and the number of rows and columns in the input layer, \n",
      " |          # returns an input pattern, which is a 1-D numpy array with the value that the input \n",
      " |          # function should attain during the presentation.\n",
      " |          # To map a given pair (r,c) to its corresponding entry in the returned vector use:\n",
      " |          # idx = rows*c + r, \n",
      " |          # e.g. input[idx] corresponds to the unit in row r, and column c, with the indexes starting\n",
      " |          # from 0. This is consistent with the way coordinates are assigned in topology.create_group,\n",
      " |          # and the 2-D pattern can be recovered with numpy.reshape(input, (rows, columns)).\n",
      " |          \n",
      " |      set_inp_fun(prev_inp_pat, cur_inp_pat, init_time, pres_time, inp_units))\n",
      " |          # Assigns a Python function to each of the input units.\n",
      " |          # pre_inp_pat : input pattern from the previous presentation (in the format of set_inp_pat).\n",
      " |          # cur_inp_pat : current input pattern.\n",
      " |          # init_time : time when the presentation will start.\n",
      " |          # pres_time : duration of the presentation.\n",
      " |          # inp_units : a list with the input units (e.g. \"x\").\n",
      " |      flat : A  binary value indicating whether to use flat_run instead of run\n",
      " |      \n",
      " |      If the set_inp_pat or set_inp_fun arguments are not provided, the class defaults are used.\n",
      " |      \n",
      " |      This method can be used with multiport units when either all 'x' inputs target the same\n",
      " |      port, or when configuration of the input ports can be done with the 'inp_ports' attribute\n",
      " |      of the xe_syn, and xi_syn parameter dictionaries.\n",
      " |      \n",
      " |      Updates:\n",
      " |          self.all_times: 1-D numpy array with the times for each data point in all_activs.\n",
      " |          self.all_activs: 2-D numpy array with the activity of all units at each point in all_times.\n",
      " |  \n",
      " |  save(self, name='ei_net_pickle.pkl')\n",
      " |      Saving simulation results. \n",
      " |      A draculab network contains lists with functions, so it is not picklable. \n",
      " |      But it can be serialized with dill: https://github.com/uqfoundation/dill \n",
      " |      \n",
      " |      After saving, retrieve object using, for example:\n",
      " |      F = open(\"ei_net_pickle.pkl\", 'rb')\n",
      " |      ei = dill.load(F)\n",
      " |      F.close()\n",
      " |  \n",
      " |  set_param(self, dictionary, entry, value)\n",
      " |      Change a value in a parameter dictionary. \n",
      " |      \n",
      " |      dictionary: a string with the name of one of the parameter dictionaries in __init__\n",
      " |      entry: a string with the name of the entry we'll modify\n",
      " |      value: the value we'll assign (can be any appropriate Python object)\n",
      " |  \n",
      " |  update_act_anim(self, frame)\n",
      " |  \n",
      " |  update_conn_anim(self, frame)\n",
      " |  \n",
      " |  update_double_anim(self, frame)\n",
      " |  \n",
      " |  update_hist_anim(self, frame)\n",
      " |  \n",
      " |  update_weight_anim(self, frame)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tools.ei_net import *\n",
    "help(ei_net)\n",
    "# ei_net contains many methods. \n",
    "# It is not necessary to read all this documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step will be to configure the inputs.  \n",
    "the `ei_net.run` method is used to run simulations where a different input pattern is presented to the **e** and **i** populations\n",
    "every `pres_time` time units. This is reminiscent of how discrete inputs are presented to artificial neural networks, although in this case the inputs are continuous in time.\n",
    "\n",
    "Two arguments tell `ei_net.run` how to present inputs: `set_inp_pat`, and `set_inp_fun`. `set_inp_pat` is a function that receives a presentation number, and returns its corresponding input pattern. The input patterns, however, are discrete, and still need to be converted into functions of time. This is the role of `set_inp_fun`, which sets the functions of the source units in the **x** population.\n",
    "\n",
    "`ei_net.run` works using a _for_ cycle, where at each loop a new pattern is selected using `set_inp_pat`, and this pattern is set into the **x** population using `set_inp_fun`. Then the simulation is advanced `pres_time` time units.\n",
    "\n",
    "`set_inp_fun` can be used to provide continuous transitions between patterns (e.g. the input signals have no \"jumps\"), and if desired, it can also be used to add noise into the patterns. There is a default version of `set_inp_fun` that is used when this argument is not provided to `ei_net.run`. The default version of `set_inp_fun`, uses linear interpolation to transition between different values of the patterns during the first fifth of the presentation. We will use this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# A bit more detail can be seen in the documentation\n",
    "help(ei_net.run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first thing we may do is to create the inputs\n",
    "import numpy as np\n",
    "# Create the input vectors\n",
    "inp_dim = 10  # dimensionality of the input vectors\n",
    "n_inp = 300  # number of input vectors\n",
    "inps = np.zeros((inp_dim, n_inp)) # each column is an input\n",
    "for col in range(n_inp):\n",
    "    vec = np.random.uniform(0., 1., inp_dim)\n",
    "    inps[:, col] = vec / np.linalg.norm(vec)\n",
    "    \n",
    "# Create the desired outputs\n",
    "v = np.zeros(inp_dim)\n",
    "v[0::2] = 1. # every other entry is non-zero\n",
    "v = v / np.linalg.norm(v)\n",
    "des_out = np.zeros(n_inp)\n",
    "for idx in range(n_inp):\n",
    "    des_out[idx] = np.dot(v, inps[:, idx])\n",
    "\n",
    "# Creating the set_inp_pat argument to ei_net.run\n",
    "def inp_pat(pres, rows, columns):\n",
    "    \"\"\" The set_inp_pat argument to ei_net.run .\n",
    "    \n",
    "        The rows and columns arguments are not used for this case.\n",
    "        We use the 'inps' and 'des_out' arrays created above.\n",
    "    \"\"\"\n",
    "    return np.concatenate((inps[:, pres%n_inp], [des_out[pres%n_inp], 1.])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `set_inp_pat` function above it can be seen that in addition to the units with the input pattern, there will be two other input units in **x**, one providing the desired output, and another one the learning trigger signal (always 1).  \n",
    "\n",
    "We must be careful when specifying the input ports to our `delta_linear` unit. The `ei_net` class connects populations using the `topology.topo_connect` method, which specifies input ports using the `inp_ports` entry of its `syn_spec` dictionary. In `ei_net`\n",
    "the `syn_spec` dictionary for the **x** to **e** connection is in the `xe_syn` dictionary.\n",
    "\n",
    "Creation of the **e**, **i**, and **x** populations is done with the `topology.create_group` method, which requires geometry and parameter dictionaries. the `*_geom` and `*_pars` dictionaries (* = e, i, or x) provide the corresponding entries in `ei_net`.\n",
    "\n",
    "Configuration of the parameter dictionaries in `ei_net` is done with the `set_param` method. The advantage of using this method over directly modifying the dictionaries is that the changes done with `set_param` automatically get logged into a `history` list, which records all the modifications done to the standard parameters. Moreover, `set_param` ensures that no parameter modifications are done after the `ei_net.build` method has been called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(ei_net.set_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(ei_net.__init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create an instance of ei_net\n",
    "one_delta = ei_net()\n",
    "\n",
    "## Specify the number of units in e, i, and x\n",
    "# One single unit in e\n",
    "one_delta.set_param('e_geom', 'rows', 1)\n",
    "one_delta.set_param('e_geom', 'columns', 1)\n",
    "one_delta.set_param('e_geom', 'center', [1., 0.])\n",
    "# No units in i\n",
    "one_delta.set_param('i_geom', 'rows', 0)\n",
    "one_delta.set_param('i_geom', 'center', [1., 0.])\n",
    "# inp_dim+2 units in x\n",
    "one_delta.set_param('x_geom', 'rows', inp_dim+2)\n",
    "one_delta.set_param('x_geom', 'columns', 1)\n",
    "\n",
    "## configure the xe connection\n",
    "one_delta.set_param('xe_syn', 'type', synapse_types.delta)\n",
    "one_delta.set_param('xe_syn', 'lrate', 0.4)\n",
    "one_delta.set_param('xe_syn', 'inp_ports', [0]*inp_dim + [1,2])\n",
    "one_delta.set_param('xe_conn', 'mask', {'circular': {'radius':10}})\n",
    "\n",
    "## avoid the ee connection\n",
    "one_delta.set_param('ee_conn', 'allow_autapses', False)\n",
    "\n",
    "## configure the delta unit\n",
    "one_delta.set_param('e_pars', 'type', unit_types.delta_linear)\n",
    "one_delta.set_param('e_pars', 'gain', 1.)\n",
    "one_delta.set_param('e_pars', 'tau_min', 0.02 )\n",
    "one_delta.set_param('e_pars', 'tau_wid', 0.)\n",
    "one_delta.set_param('e_pars', 'tau_e', 1.)\n",
    "one_delta.set_param('e_pars', 'bias_lrate', 0.01)\n",
    "one_delta.set_param('e_pars', 'n_ports', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `center` entry of the `e_geom` dictionary specifies the center of the rectangular grid where the units will be placed.\n",
    "The location of the units affects not only how they are visualized, but also the delay between the connections.\n",
    "\n",
    "Another thing to observe is how the input ports of the \"xe\" connection are specified using a single list with `inp_dim` zeros and a `[1,2]` appended at the end. This is not a good general way to set the input ports, since it is harder to read, and the length of the list must match the number of connections being made, which is tricky when using random connectivity. In general, it is better to use `ei_network` when setting multiport connections, using separate layers for all the unit groups that target a specific port.\n",
    "\n",
    "Next, the ee connection is removed. The `ei_net.__init__` method has dictionaries for the connection and synapse specifications of the ee, ei, ie, ii, xe, and xi connections. By default, if the populations exist, these connections will be made. In our case the **i** population is absent, so ei, ie, ii, and xi will not appear. On the other hand, if we don't specify anything the ee connections will be made, in this case consisting of the delta unit connecting to itself. To avoid this we disallow autapses.\n",
    "\n",
    "Finally, notice that when configuring the delta unit we did not set the value of the `tau` parameter directly, but instead `tau_min` and `tau_wid`. The `tau` parameter of the units created by `ei_net.build` is set stochastically using a uniform distribution ranging from `tau_min` to `tau_min` + `tau_wid`. This approach is used for setting the `slope`, `thresh`, `tau`, and `init_val` parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "What is wrong with the configuration of the xi synapses?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, all the synapses in the the xi connection use `delta` synapses. \n",
    "This is not appropriate for the inputs to ports 1 and 2. Unlike the `inp_ports` entry, we can't use a list to specify the synapse types.\n",
    "A fix to this is to set `lrate`=0 for these two synapses. Unfortunately, `lrate` also can't be set with multiple values on a list.\n",
    "Thus we are pushed to do something \"bad\". We will build the `ei_net` object, and then we will set the learning rates to zero.\n",
    "\n",
    "Changing parameters after building has to be done directly in the synapse object, rather than on the dictionaries of `ei_net`.\n",
    "Such changes are not automatically logged, and can hurt reproducibility of the results, but the user can do as she pleases.\n",
    "\n",
    "This problem is simple to avoid using the `ei_network` class, but this case is more illustrative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/z/projects/draculab/tools/topology.py:434: UserWarning: Zero connections created with topo_connect\n",
      "  warn('Zero connections created with topo_connect', UserWarning)\n",
      "/home/z/projects/draculab/tools/topology.py:284: UserWarning: topo_connect received an empty list as an argument\n",
      "  warn('topo_connect received an empty list as an argument', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "## build the ei_net object\n",
    "one_delta.build()\n",
    "\n",
    "## locate the synapses to ports 1 and 2, and freeze them\n",
    "for syn_list in one_delta.net.syns:\n",
    "    for syn in syn_list:\n",
    "        if syn.port == 1 or syn.port == 2:\n",
    "            syn.w = 1.\n",
    "            syn.lrate = 0.\n",
    "            syn.alpha = 0.\n",
    "            # alpha = min_delay * lrate. \n",
    "            # It is used instead of lrate in the update function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may be a warning caused by `topo_connect` receiving an empty list. This is because the **i** population is empty.\n",
    "\n",
    "After running `build()` the `ei_net` object has a `net` attribute containing the Draculab network.\n",
    "It is important to understand that this is just a regular Draculab network, no different from a network where everything is setup by hand\n",
    "using the regular methods from the `network` and `topology` classes (which `ei_net` uses). It is not necessary to use `ei_net.run` to\n",
    "run simulations with the `net` object that was created, but `ei_net.run` makes it easier to run simulations where particular inputs\n",
    "are presented sequentially. This type of *open_loop* simulations complement the *closed_loop* simulations such as the one in tutorial 4.\n",
    "\n",
    "\n",
    "After running simulations using `ei_net.run` the results will be available in the `all_times` and `all_activs` arrays of `ei_net`.\n",
    "\n",
    "There are several methods that can be used to visualize the results:\n",
    "* basic_plot\n",
    "* conn_anim\n",
    "* act_anim\n",
    "* hist_anim\n",
    "* double_anim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method conn_anim in module tools.ei_net:\n",
      "\n",
      "conn_anim(source, sink, interv=100, slider=False, weights=True) method of tools.ei_net.ei_net instance\n",
      "    An animation to visualize the connectivity of populations. \n",
      "    \n",
      "    source and sink are lists with the IDs of the units whose connections we'll visualize. \n",
      "    \n",
      "    Each frame of the animation shows: for a particular unit in source,\n",
      "    all the neurons in sink that receive connections from it (left plot), and for \n",
      "    a particular unit in sink, all the units in source that send it connections\n",
      "    (right plot).\n",
      "    \n",
      "    interv is the refresh interval (in ms) used by FuncAnimation.\n",
      "    \n",
      "    If weights=True, then the dots' size and color will reflect the absolute value\n",
      "    of the connection weight.\n",
      "    \n",
      "    Returns:\n",
      "        animation object from FuncAnimation if slider = False\n",
      "        widget object from ipywidgets.interact if slider = True\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "*c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n",
      "*c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n",
      "*c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n",
      "*c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.animation.FuncAnimation at 0x7f4f3c0c7bb0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before simulating we can have a quick look at how the connection weights\n",
    "# The input units are on the left, and the delta unit is on the right.\n",
    "# The radius and color of the circles indicate the initial strength of the connection.\n",
    "help(one_delta.conn_anim)\n",
    "one_delta.conn_anim(one_delta.x[0:10], one_delta.e, interv=200, slider=False, weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also visualize the connections \"by hand\"\n",
    "for slist in one_delta.net.syns:\n",
    "    for syn in slist:\n",
    "        pre_type = one_delta.net.units[syn.preID].type.name\n",
    "        post_type = one_delta.net.units[syn.postID].type.name\n",
    "        print(\"%s (%d) --> %s (%d), weight = %f\" % \n",
    "              (pre_type, syn.preID, post_type, syn.postID, syn.w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Presentation 0 took 0.11459159851074219 seconds.\n",
      "Presentation 1 took 0.08243036270141602 seconds.\n",
      "Presentation 2 took 0.08342337608337402 seconds.\n",
      "Presentation 3 took 0.08452081680297852 seconds.\n",
      "Presentation 4 took 0.08100342750549316 seconds.\n",
      "Total execution time is 0.44742560386657715 seconds\n"
     ]
    }
   ],
   "source": [
    "# First a short simulation, to see what's happening\n",
    "n_pres = 5 #n_inp # number of input presentations\n",
    "pres_time = 1. # time duration for each presentation\n",
    "one_delta.run(n_pres, pres_time, set_inp_pat=inp_pat)\n",
    "one_delta.basic_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be observed, `basic_plot` shows not only the activity of the inputs and the delta unit, but also some synaptic weights,\n",
    "the 'learning' and 'error' variables of the delta unit. Behind the scenes `ei_net.build` created some source units and set their functions equal to the value of these variables. The number of source units created to track synaptic weights can be set with in the `w_track` entry of the `n` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Presentation 0 took 0.11416053771972656 seconds.\n",
      "Presentation 1 took 0.08378839492797852 seconds.\n",
      "Presentation 2 took 0.08744430541992188 seconds.\n",
      "Presentation 3 took 0.09218335151672363 seconds.\n",
      "Presentation 4 took 0.08301615715026855 seconds.\n",
      "Presentation 5 took 0.1105034351348877 seconds.\n",
      "Presentation 6 took 0.08269762992858887 seconds.\n",
      "Presentation 7 took 0.08283138275146484 seconds.\n",
      "Presentation 8 took 0.08442950248718262 seconds.\n",
      "Presentation 9 took 0.08680081367492676 seconds.\n",
      "Presentation 10 took 0.08344125747680664 seconds.\n",
      "Presentation 11 took 0.1094198226928711 seconds.\n",
      "Presentation 12 took 0.08282232284545898 seconds.\n",
      "Presentation 13 took 0.0846712589263916 seconds.\n",
      "Presentation 14 took 0.08528685569763184 seconds.\n",
      "Presentation 15 took 0.08633303642272949 seconds.\n",
      "Presentation 16 took 0.08439278602600098 seconds.\n",
      "Presentation 17 took 0.08650708198547363 seconds.\n",
      "Presentation 18 took 0.08420038223266602 seconds.\n",
      "Presentation 19 took 0.0836632251739502 seconds.\n",
      "Presentation 20 took 0.08616828918457031 seconds.\n",
      "Presentation 21 took 0.10958671569824219 seconds.\n",
      "Presentation 22 took 0.0842282772064209 seconds.\n",
      "Presentation 23 took 0.11046147346496582 seconds.\n",
      "Presentation 24 took 0.0842750072479248 seconds.\n",
      "Presentation 25 took 0.10315132141113281 seconds.\n",
      "Presentation 26 took 0.08583736419677734 seconds.\n",
      "Presentation 27 took 0.08186721801757812 seconds.\n",
      "Presentation 28 took 0.0827939510345459 seconds.\n",
      "Presentation 29 took 0.08418512344360352 seconds.\n",
      "Presentation 30 took 0.08510136604309082 seconds.\n",
      "Presentation 31 took 0.08387470245361328 seconds.\n",
      "Presentation 32 took 0.0862126350402832 seconds.\n",
      "Presentation 33 took 0.08233356475830078 seconds.\n",
      "Presentation 34 took 0.10405731201171875 seconds.\n",
      "Presentation 35 took 0.08688068389892578 seconds.\n",
      "Presentation 36 took 0.08373284339904785 seconds.\n",
      "Presentation 37 took 0.08301234245300293 seconds.\n",
      "Presentation 38 took 0.08469033241271973 seconds.\n",
      "Presentation 39 took 0.08365988731384277 seconds.\n",
      "Presentation 40 took 0.08351898193359375 seconds.\n",
      "Presentation 41 took 0.08613348007202148 seconds.\n",
      "Presentation 42 took 0.0848996639251709 seconds.\n",
      "Presentation 43 took 0.08320379257202148 seconds.\n",
      "Presentation 44 took 0.08516311645507812 seconds.\n",
      "Presentation 45 took 0.08402323722839355 seconds.\n",
      "Presentation 46 took 0.08352923393249512 seconds.\n",
      "Presentation 47 took 0.08573555946350098 seconds.\n",
      "Presentation 48 took 0.08261299133300781 seconds.\n",
      "Presentation 49 took 0.08176636695861816 seconds.\n",
      "Presentation 50 took 0.08450746536254883 seconds.\n",
      "Presentation 51 took 0.0835731029510498 seconds.\n",
      "Presentation 52 took 0.0827794075012207 seconds.\n",
      "Presentation 53 took 0.08506631851196289 seconds.\n",
      "Presentation 54 took 0.08337783813476562 seconds.\n",
      "Presentation 55 took 0.08391523361206055 seconds.\n",
      "Presentation 56 took 0.08741617202758789 seconds.\n",
      "Presentation 57 took 0.08657956123352051 seconds.\n",
      "Presentation 58 took 0.08162593841552734 seconds.\n",
      "Presentation 59 took 0.08651208877563477 seconds.\n",
      "Presentation 60 took 0.08307385444641113 seconds.\n",
      "Presentation 61 took 0.08265495300292969 seconds.\n",
      "Presentation 62 took 0.08779764175415039 seconds.\n",
      "Presentation 63 took 0.08181929588317871 seconds.\n",
      "Presentation 64 took 0.08255147933959961 seconds.\n",
      "Presentation 65 took 0.08398795127868652 seconds.\n",
      "Presentation 66 took 0.08207201957702637 seconds.\n",
      "Presentation 67 took 0.08339166641235352 seconds.\n",
      "Presentation 68 took 0.08409786224365234 seconds.\n",
      "Presentation 69 took 0.0827476978302002 seconds.\n",
      "Presentation 70 took 0.10442638397216797 seconds.\n",
      "Presentation 71 took 0.08707952499389648 seconds.\n",
      "Presentation 72 took 0.08295869827270508 seconds.\n",
      "Presentation 73 took 0.08165836334228516 seconds.\n",
      "Presentation 74 took 0.08524274826049805 seconds.\n",
      "Presentation 75 took 0.08405375480651855 seconds.\n",
      "Presentation 76 took 0.08298110961914062 seconds.\n",
      "Presentation 77 took 0.08507490158081055 seconds.\n",
      "Presentation 78 took 0.08231949806213379 seconds.\n",
      "Presentation 79 took 0.08272504806518555 seconds.\n",
      "Presentation 80 took 0.08488702774047852 seconds.\n",
      "Presentation 81 took 0.08463525772094727 seconds.\n",
      "Presentation 82 took 0.08332204818725586 seconds.\n",
      "Presentation 83 took 0.08812522888183594 seconds.\n",
      "Presentation 84 took 0.08727598190307617 seconds.\n",
      "Presentation 85 took 0.08712005615234375 seconds.\n",
      "Presentation 86 took 0.11059284210205078 seconds.\n",
      "Presentation 87 took 0.08728313446044922 seconds.\n",
      "Presentation 88 took 0.08474516868591309 seconds.\n",
      "Presentation 89 took 0.09621047973632812 seconds.\n",
      "Presentation 90 took 0.08223485946655273 seconds.\n",
      "Presentation 91 took 0.0815134048461914 seconds.\n",
      "Presentation 92 took 0.10071372985839844 seconds.\n",
      "Presentation 93 took 0.08290767669677734 seconds.\n",
      "Presentation 94 took 0.10273861885070801 seconds.\n",
      "Presentation 95 took 0.10317683219909668 seconds.\n",
      "Presentation 96 took 0.0999910831451416 seconds.\n",
      "Presentation 97 took 0.08886861801147461 seconds.\n",
      "Presentation 98 took 0.10727715492248535 seconds.\n",
      "Presentation 99 took 0.09827327728271484 seconds.\n",
      "Presentation 100 took 0.09866094589233398 seconds.\n",
      "Presentation 101 took 0.10170674324035645 seconds.\n",
      "Presentation 102 took 0.08764052391052246 seconds.\n",
      "Presentation 103 took 0.08473658561706543 seconds.\n",
      "Presentation 104 took 0.09371805191040039 seconds.\n",
      "Presentation 105 took 0.0822148323059082 seconds.\n",
      "Presentation 106 took 0.09086489677429199 seconds.\n",
      "Presentation 107 took 0.08658719062805176 seconds.\n",
      "Presentation 108 took 0.08136224746704102 seconds.\n",
      "Presentation 109 took 0.08288311958312988 seconds.\n",
      "Presentation 110 took 0.0846402645111084 seconds.\n",
      "Presentation 111 took 0.08124566078186035 seconds.\n",
      "Presentation 112 took 0.08246302604675293 seconds.\n",
      "Presentation 113 took 0.08656835556030273 seconds.\n",
      "Presentation 114 took 0.08323431015014648 seconds.\n",
      "Presentation 115 took 0.0817253589630127 seconds.\n",
      "Presentation 116 took 0.08579778671264648 seconds.\n",
      "Presentation 117 took 0.08164358139038086 seconds.\n",
      "Presentation 118 took 0.08313941955566406 seconds.\n",
      "Presentation 119 took 0.08252239227294922 seconds.\n",
      "Presentation 120 took 0.08224296569824219 seconds.\n",
      "Presentation 121 took 0.08217382431030273 seconds.\n",
      "Presentation 122 took 0.08518815040588379 seconds.\n",
      "Presentation 123 took 0.08205199241638184 seconds.\n",
      "Presentation 124 took 0.08132314682006836 seconds.\n",
      "Presentation 125 took 0.08493566513061523 seconds.\n",
      "Presentation 126 took 0.0816187858581543 seconds.\n",
      "Presentation 127 took 0.08226633071899414 seconds.\n",
      "Presentation 128 took 0.08545565605163574 seconds.\n",
      "Presentation 129 took 0.12806391716003418 seconds.\n",
      "Presentation 130 took 0.08377671241760254 seconds.\n",
      "Presentation 131 took 0.0831151008605957 seconds.\n",
      "Presentation 132 took 0.08163070678710938 seconds.\n",
      "Presentation 133 took 0.08494353294372559 seconds.\n",
      "Presentation 134 took 0.08386015892028809 seconds.\n",
      "Presentation 135 took 0.08160686492919922 seconds.\n",
      "Presentation 136 took 0.12713837623596191 seconds.\n",
      "Presentation 137 took 0.08161425590515137 seconds.\n",
      "Presentation 138 took 0.08250808715820312 seconds.\n",
      "Presentation 139 took 0.08488988876342773 seconds.\n",
      "Presentation 140 took 0.08134031295776367 seconds.\n",
      "Presentation 141 took 0.08325529098510742 seconds.\n",
      "Presentation 142 took 0.08580493927001953 seconds.\n",
      "Presentation 143 took 0.08346819877624512 seconds.\n",
      "Presentation 144 took 0.08140778541564941 seconds.\n",
      "Presentation 145 took 0.08621716499328613 seconds.\n",
      "Presentation 146 took 0.08227252960205078 seconds.\n",
      "Presentation 147 took 0.08240270614624023 seconds.\n",
      "Presentation 148 took 0.08480525016784668 seconds.\n",
      "Presentation 149 took 0.08367800712585449 seconds.\n",
      "Presentation 150 took 0.08183550834655762 seconds.\n",
      "Presentation 151 took 0.08753657341003418 seconds.\n",
      "Presentation 152 took 0.0821082592010498 seconds.\n",
      "Presentation 153 took 0.08224177360534668 seconds.\n",
      "Presentation 154 took 0.0877690315246582 seconds.\n",
      "Presentation 155 took 0.08386063575744629 seconds.\n",
      "Presentation 156 took 0.0814824104309082 seconds.\n",
      "Presentation 157 took 0.08517289161682129 seconds.\n",
      "Presentation 158 took 0.0826117992401123 seconds.\n",
      "Presentation 159 took 0.08376097679138184 seconds.\n",
      "Presentation 160 took 0.0877537727355957 seconds.\n",
      "Presentation 161 took 0.08141684532165527 seconds.\n",
      "Presentation 162 took 0.10719418525695801 seconds.\n",
      "Presentation 163 took 0.08496379852294922 seconds.\n",
      "Presentation 164 took 0.08285188674926758 seconds.\n",
      "Presentation 165 took 0.08275508880615234 seconds.\n",
      "Presentation 166 took 0.08529162406921387 seconds.\n",
      "Presentation 167 took 0.08246326446533203 seconds.\n",
      "Presentation 168 took 0.08165335655212402 seconds.\n",
      "Presentation 169 took 0.10612297058105469 seconds.\n",
      "Presentation 170 took 0.0842437744140625 seconds.\n",
      "Presentation 171 took 0.08197522163391113 seconds.\n",
      "Presentation 172 took 0.08466362953186035 seconds.\n",
      "Presentation 173 took 0.08323264122009277 seconds.\n",
      "Presentation 174 took 0.1087484359741211 seconds.\n",
      "Presentation 175 took 0.08636236190795898 seconds.\n",
      "Presentation 176 took 0.08433818817138672 seconds.\n",
      "Presentation 177 took 0.08228588104248047 seconds.\n",
      "Presentation 178 took 0.0838623046875 seconds.\n",
      "Presentation 179 took 0.08562850952148438 seconds.\n",
      "Presentation 180 took 0.10806465148925781 seconds.\n",
      "Presentation 181 took 0.10596656799316406 seconds.\n",
      "Presentation 182 took 0.09121084213256836 seconds.\n",
      "Presentation 183 took 0.08321070671081543 seconds.\n",
      "Presentation 184 took 0.08783698081970215 seconds.\n",
      "Presentation 185 took 0.08235955238342285 seconds.\n",
      "Presentation 186 took 0.08025169372558594 seconds.\n",
      "Presentation 187 took 0.10378003120422363 seconds.\n",
      "Presentation 188 took 0.08308649063110352 seconds.\n",
      "Presentation 189 took 0.08038043975830078 seconds.\n",
      "Presentation 190 took 0.08418440818786621 seconds.\n",
      "Presentation 191 took 0.08352899551391602 seconds.\n",
      "Presentation 192 took 0.08274102210998535 seconds.\n",
      "Presentation 193 took 0.08407115936279297 seconds.\n",
      "Presentation 194 took 0.08228754997253418 seconds.\n",
      "Presentation 195 took 0.08060431480407715 seconds.\n",
      "Presentation 196 took 0.08387398719787598 seconds.\n",
      "Presentation 197 took 0.0879671573638916 seconds.\n",
      "Presentation 198 took 0.08217954635620117 seconds.\n",
      "Presentation 199 took 0.08528971672058105 seconds.\n",
      "Presentation 200 took 0.10541915893554688 seconds.\n",
      "Presentation 201 took 0.08165955543518066 seconds.\n",
      "Presentation 202 took 0.08417677879333496 seconds.\n",
      "Presentation 203 took 0.08238506317138672 seconds.\n",
      "Presentation 204 took 0.08078980445861816 seconds.\n",
      "Presentation 205 took 0.08397269248962402 seconds.\n",
      "Presentation 206 took 0.08141756057739258 seconds.\n",
      "Presentation 207 took 0.08286166191101074 seconds.\n",
      "Presentation 208 took 0.08414387702941895 seconds.\n",
      "Presentation 209 took 0.08258819580078125 seconds.\n",
      "Presentation 210 took 0.08274650573730469 seconds.\n",
      "Presentation 211 took 0.08322525024414062 seconds.\n",
      "Presentation 212 took 0.08068704605102539 seconds.\n",
      "Presentation 213 took 0.08048295974731445 seconds.\n",
      "Presentation 214 took 0.08291149139404297 seconds.\n",
      "Presentation 215 took 0.08356976509094238 seconds.\n",
      "Presentation 216 took 0.08271193504333496 seconds.\n",
      "Presentation 217 took 0.08657002449035645 seconds.\n",
      "Presentation 218 took 0.0819089412689209 seconds.\n",
      "Presentation 219 took 0.08101725578308105 seconds.\n",
      "Presentation 220 took 0.08480453491210938 seconds.\n",
      "Presentation 221 took 0.08255553245544434 seconds.\n",
      "Presentation 222 took 0.08259844779968262 seconds.\n",
      "Presentation 223 took 0.08424162864685059 seconds.\n",
      "Presentation 224 took 0.08270883560180664 seconds.\n",
      "Presentation 225 took 0.08173680305480957 seconds.\n",
      "Presentation 226 took 0.08492231369018555 seconds.\n",
      "Presentation 227 took 0.08221077919006348 seconds.\n",
      "Presentation 228 took 0.08202028274536133 seconds.\n",
      "Presentation 229 took 0.08347225189208984 seconds.\n",
      "Presentation 230 took 0.08014273643493652 seconds.\n",
      "Presentation 231 took 0.08216094970703125 seconds.\n",
      "Presentation 232 took 0.08349609375 seconds.\n",
      "Presentation 233 took 0.08224701881408691 seconds.\n",
      "Presentation 234 took 0.08202505111694336 seconds.\n",
      "Presentation 235 took 0.08407950401306152 seconds.\n",
      "Presentation 236 took 0.08205389976501465 seconds.\n",
      "Presentation 237 took 0.08125782012939453 seconds.\n",
      "Presentation 238 took 0.08349847793579102 seconds.\n",
      "Presentation 239 took 0.08276724815368652 seconds.\n",
      "Presentation 240 took 0.08105874061584473 seconds.\n",
      "Presentation 241 took 0.0842127799987793 seconds.\n",
      "Presentation 242 took 0.08321833610534668 seconds.\n",
      "Presentation 243 took 0.08108186721801758 seconds.\n",
      "Presentation 244 took 0.0831456184387207 seconds.\n",
      "Presentation 245 took 0.08338761329650879 seconds.\n",
      "Presentation 246 took 0.08091902732849121 seconds.\n",
      "Presentation 247 took 0.10635805130004883 seconds.\n",
      "Presentation 248 took 0.08183073997497559 seconds.\n",
      "Presentation 249 took 0.08258724212646484 seconds.\n",
      "Presentation 250 took 0.0848550796508789 seconds.\n",
      "Presentation 251 took 0.08224105834960938 seconds.\n",
      "Presentation 252 took 0.08237814903259277 seconds.\n",
      "Presentation 253 took 0.08484101295471191 seconds.\n",
      "Presentation 254 took 0.1186678409576416 seconds.\n",
      "Presentation 255 took 0.08076715469360352 seconds.\n",
      "Presentation 256 took 0.08365797996520996 seconds.\n",
      "Presentation 257 took 0.081024169921875 seconds.\n",
      "Presentation 258 took 0.10267376899719238 seconds.\n",
      "Presentation 259 took 0.08644390106201172 seconds.\n",
      "Presentation 260 took 0.08270955085754395 seconds.\n",
      "Presentation 261 took 0.10520100593566895 seconds.\n",
      "Presentation 262 took 0.08536958694458008 seconds.\n",
      "Presentation 263 took 0.08216404914855957 seconds.\n",
      "Presentation 264 took 0.08227705955505371 seconds.\n",
      "Presentation 265 took 0.08415007591247559 seconds.\n",
      "Presentation 266 took 0.08312559127807617 seconds.\n",
      "Presentation 267 took 0.08083891868591309 seconds.\n",
      "Presentation 268 took 0.08496236801147461 seconds.\n",
      "Presentation 269 took 0.08296060562133789 seconds.\n",
      "Presentation 270 took 0.08089590072631836 seconds.\n",
      "Presentation 271 took 0.08276963233947754 seconds.\n",
      "Presentation 272 took 0.08585810661315918 seconds.\n",
      "Presentation 273 took 0.08210587501525879 seconds.\n",
      "Presentation 274 took 0.08560800552368164 seconds.\n",
      "Presentation 275 took 0.08201026916503906 seconds.\n",
      "Presentation 276 took 0.08098053932189941 seconds.\n",
      "Presentation 277 took 0.08449888229370117 seconds.\n",
      "Presentation 278 took 0.08330130577087402 seconds.\n",
      "Presentation 279 took 0.10368895530700684 seconds.\n",
      "Presentation 280 took 0.08454775810241699 seconds.\n",
      "Presentation 281 took 0.08344531059265137 seconds.\n",
      "Presentation 282 took 0.10128498077392578 seconds.\n",
      "Presentation 283 took 0.08520722389221191 seconds.\n",
      "Presentation 284 took 0.08436226844787598 seconds.\n",
      "Presentation 285 took 0.08195137977600098 seconds.\n",
      "Presentation 286 took 0.08339524269104004 seconds.\n",
      "Presentation 287 took 0.08250641822814941 seconds.\n",
      "Presentation 288 took 0.08233428001403809 seconds.\n",
      "Presentation 289 took 0.08592391014099121 seconds.\n",
      "Presentation 290 took 0.08595657348632812 seconds.\n",
      "Presentation 291 took 0.08234739303588867 seconds.\n",
      "Presentation 292 took 0.08290839195251465 seconds.\n",
      "Presentation 293 took 0.08252143859863281 seconds.\n",
      "Presentation 294 took 0.08288383483886719 seconds.\n",
      "Presentation 295 took 0.0826866626739502 seconds.\n",
      "Presentation 296 took 0.08141946792602539 seconds.\n",
      "Presentation 297 took 0.08272862434387207 seconds.\n",
      "Presentation 298 took 0.0838005542755127 seconds.\n",
      "Presentation 299 took 0.08102750778198242 seconds.\n",
      "Presentation 300 took 0.0822601318359375 seconds.\n",
      "Presentation 301 took 0.08411049842834473 seconds.\n",
      "Presentation 302 took 0.08278536796569824 seconds.\n",
      "Presentation 303 took 0.08210110664367676 seconds.\n",
      "Presentation 304 took 0.10833144187927246 seconds.\n",
      "Presentation 305 took 0.08204841613769531 seconds.\n",
      "Presentation 306 took 0.08217573165893555 seconds.\n",
      "Presentation 307 took 0.10831189155578613 seconds.\n",
      "Presentation 308 took 0.08118104934692383 seconds.\n",
      "Presentation 309 took 0.08206558227539062 seconds.\n",
      "Presentation 310 took 0.08386731147766113 seconds.\n",
      "Presentation 311 took 0.08299565315246582 seconds.\n",
      "Presentation 312 took 0.08239412307739258 seconds.\n",
      "Presentation 313 took 0.0851137638092041 seconds.\n",
      "Presentation 314 took 0.08170247077941895 seconds.\n",
      "Presentation 315 took 0.08155322074890137 seconds.\n",
      "Presentation 316 took 0.08620262145996094 seconds.\n",
      "Presentation 317 took 0.0827174186706543 seconds.\n",
      "Presentation 318 took 0.08294105529785156 seconds.\n",
      "Presentation 319 took 0.08330488204956055 seconds.\n",
      "Presentation 320 took 0.08333444595336914 seconds.\n",
      "Presentation 321 took 0.08106112480163574 seconds.\n",
      "Presentation 322 took 0.10696673393249512 seconds.\n",
      "Presentation 323 took 0.08119773864746094 seconds.\n",
      "Presentation 324 took 0.08283758163452148 seconds.\n",
      "Presentation 325 took 0.08423852920532227 seconds.\n",
      "Presentation 326 took 0.08264017105102539 seconds.\n",
      "Presentation 327 took 0.08125758171081543 seconds.\n",
      "Presentation 328 took 0.08351540565490723 seconds.\n",
      "Presentation 329 took 0.08116459846496582 seconds.\n",
      "Presentation 330 took 0.08311009407043457 seconds.\n",
      "Presentation 331 took 0.08525395393371582 seconds.\n",
      "Presentation 332 took 0.08234262466430664 seconds.\n",
      "Presentation 333 took 0.08197808265686035 seconds.\n",
      "Presentation 334 took 0.10934829711914062 seconds.\n",
      "Presentation 335 took 0.08174490928649902 seconds.\n",
      "Presentation 336 took 0.08207535743713379 seconds.\n",
      "Presentation 337 took 0.08469295501708984 seconds.\n",
      "Presentation 338 took 0.08136916160583496 seconds.\n",
      "Presentation 339 took 0.08267450332641602 seconds.\n",
      "Presentation 340 took 0.08453035354614258 seconds.\n",
      "Presentation 341 took 0.08222627639770508 seconds.\n",
      "Presentation 342 took 0.08290982246398926 seconds.\n",
      "Presentation 343 took 0.08477568626403809 seconds.\n",
      "Presentation 344 took 0.08149957656860352 seconds.\n",
      "Presentation 345 took 0.0823814868927002 seconds.\n",
      "Presentation 346 took 0.08768630027770996 seconds.\n",
      "Presentation 347 took 0.10162210464477539 seconds.\n",
      "Presentation 348 took 0.08116602897644043 seconds.\n",
      "Presentation 349 took 0.08329367637634277 seconds.\n",
      "Presentation 350 took 0.08170723915100098 seconds.\n",
      "Presentation 351 took 0.08170366287231445 seconds.\n",
      "Presentation 352 took 0.08399581909179688 seconds.\n",
      "Presentation 353 took 0.08104586601257324 seconds.\n",
      "Presentation 354 took 0.08115005493164062 seconds.\n",
      "Presentation 355 took 0.08349013328552246 seconds.\n",
      "Presentation 356 took 0.08362150192260742 seconds.\n",
      "Presentation 357 took 0.0826561450958252 seconds.\n",
      "Presentation 358 took 0.08347296714782715 seconds.\n",
      "Presentation 359 took 0.08330798149108887 seconds.\n",
      "Presentation 360 took 0.08092474937438965 seconds.\n",
      "Presentation 361 took 0.08480501174926758 seconds.\n",
      "Presentation 362 took 0.0835871696472168 seconds.\n",
      "Presentation 363 took 0.08136558532714844 seconds.\n",
      "Presentation 364 took 0.08428454399108887 seconds.\n",
      "Presentation 365 took 0.08072900772094727 seconds.\n",
      "Presentation 366 took 0.08054971694946289 seconds.\n",
      "Presentation 367 took 0.08453083038330078 seconds.\n",
      "Presentation 368 took 0.08075571060180664 seconds.\n",
      "Presentation 369 took 0.08209753036499023 seconds.\n",
      "Presentation 370 took 0.08486127853393555 seconds.\n",
      "Presentation 371 took 0.08282637596130371 seconds.\n",
      "Presentation 372 took 0.10389161109924316 seconds.\n",
      "Presentation 373 took 0.10424280166625977 seconds.\n",
      "Presentation 374 took 0.08199667930603027 seconds.\n",
      "Presentation 375 took 0.08232927322387695 seconds.\n",
      "Presentation 376 took 0.08427739143371582 seconds.\n",
      "Presentation 377 took 0.08112812042236328 seconds.\n",
      "Presentation 378 took 0.0812063217163086 seconds.\n",
      "Presentation 379 took 0.08364653587341309 seconds.\n",
      "Presentation 380 took 0.08206677436828613 seconds.\n",
      "Presentation 381 took 0.0832374095916748 seconds.\n",
      "Presentation 382 took 0.08461594581604004 seconds.\n",
      "Presentation 383 took 0.08298826217651367 seconds.\n",
      "Presentation 384 took 0.08109831809997559 seconds.\n",
      "Presentation 385 took 0.08384323120117188 seconds.\n",
      "Presentation 386 took 0.08107686042785645 seconds.\n",
      "Presentation 387 took 0.1090555191040039 seconds.\n",
      "Presentation 388 took 0.08506608009338379 seconds.\n",
      "Presentation 389 took 0.08269119262695312 seconds.\n",
      "Presentation 390 took 0.0818784236907959 seconds.\n",
      "Presentation 391 took 0.08295130729675293 seconds.\n",
      "Presentation 392 took 0.0825803279876709 seconds.\n",
      "Presentation 393 took 0.0809178352355957 seconds.\n",
      "Presentation 394 took 0.08481788635253906 seconds.\n",
      "Presentation 395 took 0.08153271675109863 seconds.\n",
      "Presentation 396 took 0.08119392395019531 seconds.\n",
      "Presentation 397 took 0.0822455883026123 seconds.\n",
      "Presentation 398 took 0.08273863792419434 seconds.\n",
      "Presentation 399 took 0.0830385684967041 seconds.\n",
      "Total execution time is 34.37125372886658 seconds\n"
     ]
    }
   ],
   "source": [
    "# Now a longer simulation\n",
    "n_pres = 400 # number of input presentations\n",
    "pres_time = 1. # time duration for each presentation\n",
    "one_delta.run(n_pres, pres_time, set_inp_pat=inp_pat)\n",
    "one_delta.basic_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's see if the learning is working\n",
    "\n",
    "# First, the error should be converging to zero\n",
    "# The ei_net object recorded the errors for basic_plot().\n",
    "# They are in the activity of the unit with 'error_track[0]' ID.\n",
    "err_var = one_delta.all_activs[one_delta.error_track[0]]\n",
    "# Let's plot the first points of the simulation against the last ones\n",
    "err_fig = plt.figure(figsize=(12,10))\n",
    "n_points = 1000  # how many points to plot\n",
    "ts = one_delta.all_times\n",
    "ts1 = ts[0:n_points]\n",
    "err_var1 = err_var[0:n_points]\n",
    "ts2 = ts[-n_points:] - ts[-n_points] + ts1[0]\n",
    "err_var2 = err_var[-n_points:]\n",
    "plt.plot(ts1, err_var1, 'b', label='initial', figure=err_fig)\n",
    "plt.plot(ts2, err_var2, 'r', label='final', figure=err_fig)\n",
    "plt.plot(ts1, np.zeros_like(ts1), 'k--', figure=err_fig)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that after learning the error goes through brief \"jumps\" and then goes back near zero.\n",
    "The \"jumps\" in the error happen when there are transitions between the patterns.  \n",
    "The `delta_linear` unit does not respond instantaneously; it adjusts its output with a `tau` time constant, and during this adjustment\n",
    "its output is not as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second, the input weights of the delta unit should resemble the 'v' vector\n",
    "net = one_delta.net\n",
    "weights = net.units[one_delta.e[0]].get_mp_weights(net.sim_time)\n",
    "weights = weights[0] # only weights at port 0\n",
    "plt.figure()\n",
    "plt.bar(list(range(len(weights))), weights)\n",
    "plt.title('weights vector')\n",
    "plt.figure()\n",
    "plt.bar(list(range(len(v))), v)\n",
    "plt.title('v vector')\n",
    "plt.show()\n",
    "#print(v)\n",
    "#print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If things went well, both vectors should resemble each other.\n",
    "\n",
    "As a final note, **ei_net** is useful mainly because it is the basis of the more general **ei_network** set of tools.\n",
    "\n",
    "**ei_network** takes many objects similar to **ei_net**, rebranding them as _layers_. With **ei_network** we can\n",
    "build networks consisting of many interconnected _layers_. This gives enough flexibility to build almost any network\n",
    "you may want.\n",
    "\n",
    "One drawback of **ei_network** is that each layer is assumed to have 3 populations (inhibitory, excitatory, and inputs). This can lead to empty populations, slighlty more cumbersome coding, and most importantly, to adding extra populations without realizing it. A new tool is being designed to address this."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION TO EXERCISE 1\n",
    "\n",
    "def get_mp_input_sum(self,time):\n",
    "    return np.dot(self.get_mp_inputs(time)[0], self.get_mp_weights(time)[0]) / self.inp_l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOLUTION TO EXERCISE 2\n",
    "The documentation of `mp_inputs` is in the docstring of the `add_mp_inputs` method in `requirements.py`.\n",
    "In general, the documentation to a requirement is kept in the `add_<requirement_name>` method.\n",
    "\n",
    "After importing `draculab`, the priority of `mp_inputs` can be obtained with: `syn_reqs.mp_inputs.get_priority()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOLUTION TO EXERCISE 3\n",
    "It would not be recommended to use the `mp_inputs` array to implement the `get_mp_input_sum` function.  \n",
    "The reason is that `mp_inputs` like all requirements, gets updated **once** per simulation step. On the other hand, the activity of\n",
    "a unit is calculated at least `min_buffer_size` times in one simulation step of `min_delay` length. Using `mp_inputs` would \"freeze\" the inputs during\n",
    "the simulation step, and lead to a decrease in precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
