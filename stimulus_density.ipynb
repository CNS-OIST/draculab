{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Searching for ways to break the uniformity in E-I networks (all-or-none behaviour) I came up with this idea:\n",
    "\n",
    "Let $u$ denote a stimulation level in a neural field. Not the stimulation level at any particular location, just a stimulation level.  \n",
    "If the network has heterogeneous activation then you would expect that there would be a distribution of stimulation levels $\\rho(u)$ that is not concentrated at a single value.  \n",
    "The idea is to use steady-state weights $\\omega$ that are a function of the activation levels, as in the steady-state when using a correlational learning rule.  \n",
    "A self-consistency equation is:\n",
    "\n",
    "$u = \\int_\\Re \\omega(f(u), f(u')) \\rho(u') du'$,\n",
    "\n",
    "where $f = (1 + \\text{e}^{-x})^{-1}$. For example, if you wanted a distribution of stimulations that was constant in the interval $[a, b]$ then you could use \n",
    "\n",
    "$u = \\frac{1}{b-a} \\int_{[a,b]} \\omega(f(u), f(u')) du'$.\n",
    "\n",
    "The solution to this self-consistency equation is not unique. A simple approach is to have $\\omega(x,y) = f^{-1}(x) g(f^{-1}(y))$, where $\\int_a^b g(u)du = 1$. For example, we can have $g(u) = 2u/(b^2-a^2)$.  \n",
    "Considering that $f^{-1}(x) = - \\text{log}\\left(\\frac{1-x}{x}\\right)$, then for the simple case when we want our stimulations evenly distributed in the [0,1] interval, we would have the stationary weights:  \n",
    "$\\omega_s(x,y) = 2 \\ \\text{log}\\left(\\frac{1-x}{x} \\right) \\text{log}\\left(\\frac{1-y}{y} \\right) $.\n",
    "\n",
    "A learning rule to produce such stationary weight distribution could be:  \n",
    "$\\frac{d\\omega}{dt} = \\alpha \\ (\\omega_s - \\omega) = \\alpha \\ \\left( 2 \\ \\text{log}\\left(\\frac{1-x}{x} \\right) \\text{log}\\left(\\frac{1-y}{y} \\right) - \\omega \\right)$,  \n",
    "where $x$ and $y$ are presynaptic and postsynaptic activity respectively, although this rule is symmetric regarding pre and post switches.\n",
    "\n",
    "The rule could be summarized as follows: changes are positive for similar activities, negative for different activities. Changes increase as activities move away from the point $(0.5, 0.5)$.\n",
    "It is somewhat reminiscent of the correlation rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = lambda x: np.log( (1.-x)/x )\n",
    "x = np.linspace(0.001, 0.999, 200)\n",
    "y = np.linspace(0.001, 0.999, 200)\n",
    "x,y = np.meshgrid(x,y)\n",
    "z = 2.*f(x)*f(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " %matplotlib qt5\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "ax = fig.gca(projection='3d')\n",
    "surf = ax.plot_surface(x, y, z, cmap=cm.coolwarm,\n",
    "                       linewidth=0, antialiased=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
