{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v3_nst_afx_hyper2.ipynb\n",
    "\n",
    "Hyperparameter search on the v3_nst_afx model using a genetic algorithm.\n",
    "\n",
    "This second version optimizes a different set of parameters to those in `v3_nst_afx_hyper`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/z/projects/draculab\n"
     ]
    }
   ],
   "source": [
    "%cd ../..\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "from draculab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def net_from_cfg(cfg):\n",
    "    \"\"\" Create a draculab network with the given configuration. \n",
    "\n",
    "        Args:\n",
    "            cfg : a parameter dictionary\n",
    "\n",
    "        Returns:\n",
    "            net, pops_dict\n",
    "            net : A draculab network as in v3_nst_afx, with the given configuration.\n",
    "            pops_dict : a dictionary with the list of ID's for each population in net.\n",
    "            hand_coords : list with the coordinates for all possible targets\n",
    "            m_idxs : which target coordinats will be used for the i-th presentation\n",
    "            t_pres : number of seconds each target is presented\n",
    "    \"\"\"\n",
    "    t_pres = 30. # number of seconds to hold each set of target lengths\n",
    "    rand_w = True # whether to use random weights in M->C, AF->M\n",
    "    rga_diff = True # if True use gated_normal_rga_diff, if False gated_normal_rga\n",
    "    rand_targets = True # whether to train using a large number of random targets\n",
    "    par_heter = 0.001 # range of heterogeneity as a fraction of the original value\n",
    "    \n",
    "    net_params = {'min_delay' : 0.005,\n",
    "              'min_buff_size' : 10 }\n",
    "\n",
    "    P_params = {  'type' : plant_models.bouncy_planar_arm_v3,\n",
    "              'mass1': 1.,\n",
    "              'mass2': 1.,\n",
    "              's_min' : -0.8,\n",
    "              'p1' : (-0.01, 0.04),\n",
    "              'p2' : (0.29, 0.03),\n",
    "              'p3' : (0., 0.05),\n",
    "              'p5' : (0.01, -0.05),\n",
    "              'p10': (0.29, 0.03),\n",
    "              'init_q1': 0.,\n",
    "              'init_q2': np.pi/2.,\n",
    "              'init_q1p': 0.,\n",
    "              'init_q2p': 0.,\n",
    "              'g': 0.0,\n",
    "              'mu1': 3.,\n",
    "              'mu2': 3.,\n",
    "              'l_torque' : 0.001,\n",
    "              'l_visco' : 0.01,\n",
    "              'g_e' : cfg['g_e_factor']*np.array([18., 20., 20., 18., 22., 23.]),\n",
    "              'l0_e' : [1.]*6,\n",
    "              'Ia_gain' : 2.5*np.array([3.,10.,10., 3.,10.,10.]),\n",
    "              'II_gain' : 2.*np.array([3., 8., 8., 3., 8., 8.]),\n",
    "              'Ib_gain' : 1.,\n",
    "              'T_0' : 10.,\n",
    "              'k_pe_e' : 20.,  #8\n",
    "              'k_se_e' : 20., #13\n",
    "              'b_e' : cfg['b_e'],\n",
    "              'g_s' : 0.02,\n",
    "              'k_pe_s' : 2., \n",
    "              'k_se_s' : 2.,\n",
    "              'g_d' : 0.01,\n",
    "              'k_pe_d' : .2, #.1,\n",
    "              'k_se_d' : 1., #2.,\n",
    "              'b_s' : .5,\n",
    "              'b_d' : 2.,#3.,\n",
    "              'l0_s': .7,\n",
    "              'l0_d': .8,\n",
    "              'fs' : 0.1,\n",
    "              'se_II' : 0.5,\n",
    "              'cd' : 0.5,\n",
    "              'cs' : 0.5,\n",
    "              'tau' : 0.1   # ficticious time constant used in create_freqs_steps\n",
    "               }\n",
    "    net = network(net_params)\n",
    "    #P = net.create(1, P_params)\n",
    "    #arm = net.plants[P]\n",
    "\n",
    "    # We organize the spinal connections through 4 types of symmetric relations\n",
    "    # these lists are used to set intraspinal connections and test connection matrices\n",
    "    antagonists = [(0,3), (1,2), (4,5)]\n",
    "    part_antag = [(0,2),(0,5), (3,4), (1,3)]\n",
    "    synergists = [(0,1), (0,4), (2,3), (3,5)]\n",
    "    part_syne = [(1,4), (2,5)]\n",
    "    self_conn = [(x,x) for x in range(6)]\n",
    "\n",
    "    antagonists += [(p[1],p[0]) for p in antagonists]\n",
    "    part_antag += [(p[1],p[0]) for p in part_antag]\n",
    "    synergists += [(p[1],p[0]) for p in synergists]\n",
    "    part_syne += [(p[1],p[0]) for p in part_syne]\n",
    "    all_pairs = [(i,j) for i in range(6) for j in range(6)]\n",
    "    #unrelated = set(all_pairs) - set(antagonists) - set(part_antag) - set(synergists) - set(part_syne) - set(self_conn)\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # UNIT PARAMETER DICTIONARIES\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    randz6 = lambda : (1. + par_heter*(np.random.rand(6)-0.5))\n",
    "    randz12 = lambda : (1. + par_heter*(np.random.rand(12)-0.5))\n",
    "    randz18 = lambda : (1. + par_heter*(np.random.rand(18)-0.5))\n",
    "    randz36 = lambda : (1. + par_heter*(np.random.rand(36)-0.5))\n",
    "\n",
    "    ACT_params = {'type' : unit_types.act,\n",
    "                  'tau_u' : 8.,\n",
    "                  'gamma' : 2.,\n",
    "                  'g' : 2.,\n",
    "                  'theta' : 1.,\n",
    "                  'tau_slow' : 5.,\n",
    "                  'y_min' : 0.2,\n",
    "                  'rst_thr' : 0.1,\n",
    "                  'init_val' : 0. }\n",
    "    spf_sum_min = .4 # value where no corrections are needed anymore\n",
    "    y_min = 1./(1. + np.exp(-ACT_params['g']*(spf_sum_min - ACT_params['theta'])))\n",
    "    ACT_params['y_min'] = y_min\n",
    "\n",
    "    AF_params = {'type' : unit_types.logarithmic,\n",
    "                 'init_val' : [0.1, 0.05, 0.15, 0.1, 0.1, 0.1, # avg afferent values\n",
    "                               0.2, 0.15, 0.3, 0.3, 0.2, 0.25,\n",
    "                               0.2, 0.4, 0.4, 0.2, 0.4, 0.4]*2,\n",
    "                               #0.3, 0.4, 0.5, 0.3, 0.3, 0.5]*2,\n",
    "                 'tau' : 0.02 * randz36(),\n",
    "                 'tau_fast': 0.1,\n",
    "                 'tau_mid' : 1.,\n",
    "                 'tau_slow' : 40.,\n",
    "                 'delay' : 0.1,\n",
    "                 'thresh' : [0.05]*18 + [-0.4]*18 } \n",
    "    AL_params = {'type' : unit_types.sigmoidal,\n",
    "                 'thresh' : cfg['AL_thresh'] * randz6(),\n",
    "                 'slope' : 2. * randz6(),\n",
    "                 'init_val' : 0.1 * randz6(),\n",
    "                 'tau' : 0.02 * randz6() }\n",
    "    CE_params = {'type' : unit_types.gated_rga_inpsel_adapt_sig,\n",
    "                 'thresh' : 0. * randz6(),\n",
    "                 'slope' : 1.5 * randz6(),\n",
    "                 'init_val' : 0.2 * randz6(),\n",
    "                 'tau' : 0.02,\n",
    "                 'tau_fast': 0.1,\n",
    "                 'tau_mid' : 1.,\n",
    "                 'tau_slow' : cfg['C_tau_slow'],\n",
    "                 'custom_inp_del' : 15, # placeholder values\n",
    "                 'custom_inp_del2': 30,\n",
    "                 'integ_amp' : cfg['integ_amp'],\n",
    "                 'integ_decay' : cfg['integ_decay'],\n",
    "                 'adapt_amp' : cfg['adapt_amp'],\n",
    "                 'delay' : 0.2,\n",
    "                 'des_out_w_abs_sum' : 1. }\n",
    "    CI_params = {'type' : unit_types.gated_rga_inpsel_adapt_sig,\n",
    "                 'thresh' : 0.5 * randz6(),\n",
    "                 'slope' : 2. * randz6(),\n",
    "                 'init_val' : 0.2 * randz6(),\n",
    "                 'tau' : 0.1,\n",
    "                 'tau_fast': 0.1,\n",
    "                 'tau_mid' : 1.,\n",
    "                 'tau_slow' : cfg['C_tau_slow'],\n",
    "                 'custom_inp_del' : 15, # placeholder values\n",
    "                 'custom_inp_del2': 30,\n",
    "                 'integ_amp' : cfg['integ_amp'], #.5,\n",
    "                 'integ_decay' : cfg['integ_decay'],\n",
    "                 'adapt_amp' : cfg['adapt_amp'],\n",
    "                 'delay' : 0.2,\n",
    "                 'des_out_w_abs_sum' : 1. }\n",
    "    M_params = {'type' : unit_types.gated_out_norm_am_sig,\n",
    "                'thresh' : 0. * randz12(),\n",
    "                'slope' : 3. * randz12(),\n",
    "                'init_val' : 0.2 * randz12(),\n",
    "                'delay' : 0.2,\n",
    "                'tau_fast': 0.15,\n",
    "                'tau_mid': 1.5,\n",
    "                'tau_slow' : 10.,\n",
    "                'tau' : 0.01 * randz12(),\n",
    "                'p0_inp' : 0.0,\n",
    "                'des_out_w_abs_sum' : 2. }\n",
    "    SF_params = {'type' : unit_types.sigmoidal,\n",
    "                 #'thresh' : np.array([-0.02]*12)\n",
    "                 #'thresh' : np.array([-0.12, -0.13, -0.05, -0.11, -0.03, -0.05, -0.05, 0.05, 0.05, 0.06, -0.03, -0.02]),\n",
    "                 'thresh' : np.array([-0.12, -0.13, -0.05, -0.11, -0.03, -0.05, -0.05, -0.07, 0.05, 0.06, -0.03, -0.02]),\n",
    "                 #'slope' : np.array([np.log(5.)]*12), #np.array([np.log(9.)]*12),\n",
    "                 'slope' : cfg['SF_slope_factor']*np.array([2.75, 1.7, 1.37, 2.75] + [1.37]*2 + [2., 1.37, 1.37]*2),\n",
    "                 'init_val' : 0.2 * randz12(),\n",
    "                 'tau' : 0.03 * randz12() } \n",
    "    SP_params = {'type' : unit_types.source,\n",
    "                 'init_val' : 0.5,\n",
    "                 'tau_fast' : 0.02,\n",
    "                 'tau_mid' : 0.1,\n",
    "                 'function' : lambda t: None }\n",
    "    SP_CHG_params = {'type' : unit_types.sigmoidal,\n",
    "                  'thresh' : 0.25,\n",
    "                  'slope' : 9.,\n",
    "                  'init_val' : 0.1,\n",
    "                  'tau' : 0.01 }\n",
    "    SPF_params = {'type' : unit_types.logarithmic, #sigmoidal,\n",
    "                  'thresh' : -0.1, #0.4 * randz12(),\n",
    "                  'slope' : 6. * randz12(),\n",
    "                  'init_val' : 0.3 * randz12(),\n",
    "                  'tau_fast': 0.005,\n",
    "                  'tau_mid': 0.05,\n",
    "                  'tau_slow' : 5.,\n",
    "                  'tau' : 0.02 * randz12() }\n",
    "    track_params = {'type' : unit_types.source,\n",
    "                    'init_val' : 0.02,\n",
    "                    'function' : lambda t: None }\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # CONNECTION DICTIONARIES\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "    # ACT to CE,CI ------------------------------------------------\n",
    "    ACT__CE_conn = {'rule' : \"all_to_all\",\n",
    "                    'delay' : 0.02 } \n",
    "    ACT__CE_syn = {'type' : synapse_types.static,\n",
    "                   'inp_ports' : 4,\n",
    "                   'init_w' : 1. }\n",
    "    ACT__CI_conn = {'rule' : \"all_to_all\",\n",
    "                    'delay' : 0.02 } \n",
    "    ACT__CI_syn = {'type' : synapse_types.static,\n",
    "                   'inp_ports' : 4,\n",
    "                   'init_w' : 1. }\n",
    "    # AF to CE, CI --------------------------------------------------\n",
    "    AF__CE_conn = {'rule' : 'all_to_all',\n",
    "                   'delay' : 0.02 }\n",
    "    AF__CE_syn = {'type' : synapse_types.gated_inp_sel,\n",
    "                  'aff_port' : 2,\n",
    "                  'inp_ports' : 2,\n",
    "                  'error_port' : 0,\n",
    "                  'normalize' : True,\n",
    "                  'w_sum' : 2.,\n",
    "                  'lrate' : 0., #10.,\n",
    "                  'extra_steps' : 1,\n",
    "                  'init_w' : 0.005 }\n",
    "    AF__CI_conn = {'rule' : 'all_to_all',\n",
    "                   'delay' : 0.02 }\n",
    "    AF__CI_syn = {'type' : synapse_types.gated_inp_sel,\n",
    "                  'aff_port' : 2,\n",
    "                  'inp_ports' : 2,\n",
    "                  'error_port' : 0,\n",
    "                  'normalize' : True,\n",
    "                  'w_sum' : 2.,\n",
    "                  'lrate' : 0., #10.,\n",
    "                  'extra_steps' : 1,\n",
    "                  'init_w' : 0.005 }\n",
    "    # AF to M ------------------------------------------------\n",
    "    ## Creating a test matrix\n",
    "    if not rand_w:\n",
    "        # Initializing manually\n",
    "        AF_M = np.zeros((36, 12)) # rows are source, columns target\n",
    "        for src in range(36):\n",
    "            for trg in range(12):\n",
    "                src_pop = src%6 #src's population\n",
    "                trg_pop = trg%6 #trg's population\n",
    "                if src%18 < 6: # if the afferent is tension, don't reverse signs\n",
    "                    sig = 1\n",
    "                else:\n",
    "                    sig = -1\n",
    "                if src > 17: sig = -sig # if 'negative' afferent reverse sign\n",
    "                for pair in antagonists:\n",
    "                    if pair == (src_pop, trg_pop):\n",
    "                        AF_M[src, trg] = sig*0.2\n",
    "                        break\n",
    "                else: \n",
    "                    for pair in part_antag:\n",
    "                        if pair == (src_pop, trg_pop):\n",
    "                            AF_M[src, trg] = sig*0.1\n",
    "                            break\n",
    "                    else: \n",
    "                        for pair in synergists:\n",
    "                            if pair == (src_pop, trg_pop):\n",
    "                                AF_M[src, trg] = sig*-0.2\n",
    "                                break\n",
    "                        else: \n",
    "                            for pair in synergists:\n",
    "                                if pair == (src_pop, trg_pop):\n",
    "                                    AF_M[src, trg] = sig*-0.2\n",
    "                                    break\n",
    "                            else: \n",
    "                                for pair in part_syne:\n",
    "                                    if pair == (src_pop, trg_pop):\n",
    "                                        AF_M[src, trg] = sig*-0.1\n",
    "                                        break\n",
    "                                else:\n",
    "                                    if src_pop == trg_pop:\n",
    "                                        AF_M[src, trg] = sig*-0.3\n",
    "    else:\n",
    "        #AF_M = 0.2*(np.random.random((12,12)) - 0.5) # random initial connections!!!!!\n",
    "        AF_M = 0.2*(np.random.random((12,36)) - 0.5) # random initial connections!!!!!\n",
    "    AF__M_conn = {'rule' : 'all_to_all',\n",
    "                 'delay' : 0.02 }\n",
    "    AF__M_syn = {'type' : synapse_types.gated_diff_inp_sel,\n",
    "                'aff_port' : 0,\n",
    "                'error_port' : 1,\n",
    "                'normalize' : True,\n",
    "                'w_sum' : 10.,\n",
    "                'inp_ports' : 0, # afferent for out_norm_am_sig\n",
    "                'input_type' : 'pred', # if using inp_corr\n",
    "                'lrate' : 15., #10.\n",
    "                'extra_steps' : None, # placeholder value; filled below,\n",
    "                'init_w' : AF_M.flatten() }\n",
    "    # AF to SF ------------------------------------------------\n",
    "    AF__SF_dubya = np.array([0.57, 0.56, 0.56, 0.57, 0.55, 0.55, 0.57, 0.56, 0.56, 0.57, 0.55, 0.55])\n",
    "    AFe__SF_conn = {'rule' : 'one_to_one',\n",
    "                  'delay' : 0.02 }\n",
    "    AFe__SF_syn = {'type' : synapse_types.static,\n",
    "                 #'init_w' : .5*np.array([ 39.73, 15.03,  9.66, 116.47, 8.63,  63.68, 20.88, 9.69, 5.86, 67.98, 5.44, 57.38]) }\n",
    "                   #'init_w' : np.array([15.07516819, 20.60577773,  6.32821777, 14.08991768,  5.69679834,  8.89424814,\n",
    "                   #                     7.68013805, 13.46076843,  3.87962611,  7.1693345,   3.5817668,   5.8075114 ])}\n",
    "                   'init_w' : AF__SF_dubya}\n",
    "    AFi__SF_conn = {'rule' : 'one_to_one',\n",
    "                  'delay' : 0.02 }\n",
    "    AFi__SF_syn = {'type' : synapse_types.static,\n",
    "                 #'init_w' : .5*np.array([-162.42, -9.56, -15.19, -50.0, -88.47, -9.73, -67.67, -5.85, -9.96, -23.55, -50.63, -5.81])}\n",
    "                  #'init_w' : np.array([-15.36496074,  -5.96985989, -21.78133701, -18.88200345, -11.38248201,\n",
    "                  #            -6.93261446,  -7.53984025,  -3.68499008, -14.59079011,  -9.22649886,  -7.03063956,  -4.19297924])}\n",
    "                  'init_w' : -AF__SF_dubya }\n",
    "    # AL to P ------------------------------------------------\n",
    "    AL__P_conn = {'inp_ports' : list(range(6)),\n",
    "                 'delays': 0.01 }\n",
    "    AL__P_syn = {'type': synapse_types.static,\n",
    "                'init_w' : 1. }\n",
    "    # CE, CI to AL ----------------------------------------------\n",
    "    CE__AL_conn = {'rule' : 'one_to_one',\n",
    "                   'delay' : 0.01 }\n",
    "    CE__AL_syn = {'type' : synapse_types.static,\n",
    "                  'init_w' : [1., 1., 1., 1., 1., 1.] }\n",
    "    CI__AL_conn = {'rule' : 'one_to_one',\n",
    "                   'delay' : 0.01 }\n",
    "    CI__AL_syn = {'type' : synapse_types.static,\n",
    "                  'init_w' : -1. }\n",
    "    # CE,CI to CE,CI  ------------------------------------------------\n",
    "    CE__CI_conn = {'rule' : 'one_to_one',\n",
    "                   'delay' : 0.01 }\n",
    "    CI__CE_conn = {'rule' : 'one_to_one',\n",
    "                   'delay' : 0.01 }\n",
    "    CE__CI_syn = {'type' : synapse_types.static,\n",
    "                  'inp_ports' : 2, #1, # IN AFFERENT PORT!!!!!!!!!!!!!!!!!!!!!! May affect normalization of afferent inputs\n",
    "                  'init_w' : cfg['CE__CI_w'] }\n",
    "    CI__CE_syn = {'type' : synapse_types.static, #static, #corr_inh,\n",
    "                  'inp_ports' : 2, #1, # IN AFFERENT PORT!!!!!!!!!!!!!!!!!!!!!! May affect normalization of afferent inputs\n",
    "                  'lrate' : .0,\n",
    "                  'des_act' : 0.5,\n",
    "                  'init_w' : cfg['CI__CE_w'] }\n",
    "    C__C_conn = {'rule': 'one_to_one',\n",
    "                 'allow_autapses' : False,\n",
    "                 'delay' : 0.015 }\n",
    "    C__C_syn_antag = {'type' : synapse_types.static, #bcm,\n",
    "                      'inp_ports': 2, #1, # IN AFFERENT PORT!!!!!!!!!!!!!!!!!!!!!! May affect normalization of afferent inputs\n",
    "                      'init_w' : cfg['C__C_antag'],\n",
    "                      'lrate' : 1.,\n",
    "                      'des_act' : .5 }\n",
    "    C__C_syn_p_antag = {'type' : synapse_types.static, #bcm,\n",
    "                      'inp_ports': 2, #1, # IN AFFERENT PORT!!!!!!!!!!!!!!!!!!!!!! May affect normalization of afferent inputs\n",
    "                      'init_w' : cfg[\"C__C_p_antag\"], # 8.,\n",
    "                      'lrate' : 1.,\n",
    "                      'des_act' : 0.2 }\n",
    "    C__C_syn_syne = {'type' : synapse_types.static,\n",
    "                     'inp_ports': 1,\n",
    "                     'lrate' : 1.,\n",
    "                     'init_w' : .5 }\n",
    "    C__C_syn_p_syne = {'type' : synapse_types.static,\n",
    "                       'inp_ports': 1,\n",
    "                       'lrate' : 1.,\n",
    "                       'init_w' : 0.2 }\n",
    "    C__C_syn_null_lat = {'type' : synapse_types.static, # connection with static weight zero\n",
    "                       'inp_ports': 1,\n",
    "                       'lrate' : 1.,\n",
    "                       'init_w' : 0. }\n",
    "    C__C_syn_null_aff = {'type' : synapse_types.static, # connection with static weight zero\n",
    "                       'inp_ports': 2, #1, # IN AFFERENT PORT!!!!!!!!!!!!!!!!!!!!!! May affect normalization of afferent inputs\n",
    "                       'lrate' : 1.,\n",
    "                       'init_w' : 0. }\n",
    "\n",
    "    # M to CE,CI ----------------------------------------------\n",
    "    # creating a test matrix\n",
    "    if not rand_w:\n",
    "        # initializing manually\n",
    "        M_CE = np.array(\n",
    "            [[ 0.2,  0.1, -0.1, -0.2,  0.1, -0.1],\n",
    "             [ 0.1,  0.2, -0.2, -0.1,  0.1,  0.0],\n",
    "             [-0.1, -0.2,  0.2,  0.1,  0.0,  0.0],\n",
    "             [-0.2, -0.1,  0.1, -0.2, -0.1,  0.1],\n",
    "             [ 0.1,  0.0,  0.0, -0.1,  0.3, -0.2],\n",
    "             [-0.1,  0.0,  0.0,  0.1, -0.2,  0.3],\n",
    "             [ 0.2,  0.1, -0.1, -0.2,  0.1, -0.1],\n",
    "             [ 0.1,  0.2, -0.2, -0.1,  0.1,  0.0],\n",
    "             [-0.1, -0.2,  0.2,  0.1,  0.0,  0.0],\n",
    "             [-0.2, -0.1,  0.1, -0.2, -0.1,  0.1],\n",
    "             [ 0.1,  0.0,  0.0, -0.1,  0.3, -0.2],\n",
    "             [-0.1,  0.0,  0.0,  0.1, -0.2,  0.3]])\n",
    "        M_CI = -M_CE \n",
    "    else:\n",
    "        M_CE = 0.4*(np.random.random((12,6)) - 0.5) # random initial connections!!!!!\n",
    "        M_CI = 0.4*(np.random.random((12,6)) - 0.5) # random initial connections!!!!!\n",
    "    if rga_diff:\n",
    "        M__C_type = synapse_types.gated_normal_rga_diff\n",
    "    else:\n",
    "        M__C_type = synapse_types.gated_normal_rga\n",
    "    M__CE_conn = {'rule': 'all_to_all',\n",
    "                 'delay': 0.02 }\n",
    "    M__CE_syn = {'type' : M__C_type,\n",
    "                 'inp_ports' : 0,\n",
    "                 'lrate' : 20.,\n",
    "                 'w_sum' : cfg['M__C_w_sum'],\n",
    "                 'sig1' : cfg['sig1'],\n",
    "                 'sig2' : cfg['sig2'],\n",
    "                 'w_thresh' : 0.05,\n",
    "                 'w_decay': 0.005,\n",
    "                 'w_tau' : 60.,\n",
    "                 'init_w' : M_CE.flatten() }\n",
    "    M__CI_conn = {'rule': 'all_to_all',\n",
    "                 'delay': 0.02 }\n",
    "    M__CI_syn = {'type' : M__C_type,\n",
    "                 'inp_ports' : 0,\n",
    "                 'lrate' : 20.,\n",
    "                 'w_sum' : cfg['M__C_w_sum'],\n",
    "                 'sig1' : cfg['sig1'],\n",
    "                 'sig2' : cfg['sig2'],\n",
    "                 'w_thresh' : 0.05,\n",
    "                 'w_tau' : 60.,\n",
    "                 'w_decay': 0.005,\n",
    "                 'init_w' : M_CI.flatten() }\n",
    "    # P to AF  ---------------------------------------------------\n",
    "    idx_aff = np.arange(22,40) # indexes for afferent output in the arm\n",
    "    P__AF_conn = {'port_map' : [[(p,0)] for p in idx_aff],\n",
    "                 'delays' : 0.02 }\n",
    "    Pe__AF_syn = {'type' : synapse_types.static,\n",
    "                  'init_w' : [1.]*18 } \n",
    "    Pi__AF_syn = {'type' : synapse_types.static,\n",
    "                'init_w' :  [-1.]*18 }\n",
    "    # SF, SP to SPF ------------------------------------------------\n",
    "    SFe__SPF_conn = {'rule' : \"one_to_one\",\n",
    "                     'delay' : 0.01 }\n",
    "    SFi__SPF_conn = {'rule' : \"one_to_one\",\n",
    "                     'delay' : 0.02 }\n",
    "    SFe__SPF_syn = {'type' : synapse_types.static,\n",
    "                    'init_w' : cfg['SPF_w'] }\n",
    "    SFi__SPF_syn = {'type' : synapse_types.static,\n",
    "                    'init_w' : -cfg['SPF_w'] }\n",
    "    SPe__SPF_conn = {'rule' : \"one_to_one\",\n",
    "                     'delay' : 0.01 }\n",
    "    SPi__SPF_conn = {'rule' : \"one_to_one\",\n",
    "                     'delay' : 0.02 }\n",
    "    SPe__SPF_syn = {'type' : synapse_types.static,\n",
    "                    'init_w' : cfg['SPF_w'] }\n",
    "    SPi__SPF_syn = {'type' : synapse_types.static,\n",
    "                   'init_w' : -cfg['SPF_w'] }\n",
    "    # SP to SP_CHG ------------------------------------------------\n",
    "    SP__SP_CHG_conn = {'rule' : 'all_to_all',\n",
    "                        'delay' : 0.01}\n",
    "    SP__SP_CHG_syn = {'type' : synapse_types.chg,\n",
    "                      'init_w' : 0.,\n",
    "                      'lrate' : 20. }\n",
    "    # SP_CHG to CE, CI ------------------------------------------------\n",
    "    SP_CHG__CE_conn = {'rule' : \"all_to_all\",\n",
    "                      'delay' : 0.02 }\n",
    "    SP_CHG__CE_syn = {'type' : synapse_types.static,\n",
    "                      'inp_ports' : 3,\n",
    "                      'init_w' : 1. }\n",
    "    SP_CHG__CI_conn = {'rule' : \"all_to_all\",\n",
    "                       'delay' : 0.02 }\n",
    "    SP_CHG__CI_syn = {'type' : synapse_types.static,\n",
    "                      'inp_ports' : 3,\n",
    "                      'init_w' : 1. }\n",
    "    # SP_CHG to ACT ------------------------------------------------\n",
    "    SP_CHG__ACT_conn = {'rule' : \"all_to_all\",\n",
    "                       'delay' : 0.02 }\n",
    "    SP_CHG__ACT_syn = {'type' : synapse_types.static,\n",
    "                      'inp_ports' : 1,\n",
    "                      'init_w' : 1. }\n",
    "    # SP_CHG to M ------------------------------------------------\n",
    "    SP_CHG__M_conn = {'rule' : \"all_to_all\",\n",
    "                      'delay' : 0.02 }\n",
    "    SP_CHG__M_syn = {'type' : synapse_types.static,\n",
    "                      'inp_ports' : 2,\n",
    "                      'init_w' : 1. }\n",
    "    # SPF to ACT ------------------------------------------------\n",
    "    SPF__ACT_conn = {'rule' : \"all_to_all\",\n",
    "                     'delay' : 0.02 }\n",
    "    SPF__ACT_syn = {'type' : synapse_types.static,\n",
    "                    'inp_ports' : 0,\n",
    "                    'init_w' : 1. }\n",
    "    # SPF to M  ------------------------------------------------\n",
    "    SPF__M_conn = {'rule': 'one_to_one',\n",
    "                   'delay': 0.01 }\n",
    "    SPF__M_syn = {'type' : synapse_types.static, #synapse_types.inp_corr,\n",
    "                  'inp_ports' : 1,\n",
    "                  'lrate' : 0.,\n",
    "                  'input_type' : 'error', # if using inp_corr\n",
    "                  'init_w' : 1. }\n",
    "\n",
    "    #*************************************************************\n",
    "    # Setting the right delay for AF-->M\n",
    "    f = 1. # going to estimate the extra delay of error inputs wrt afferent inputs at M\n",
    "    w = 2.*np.pi*f\n",
    "    sf_del = np.arctan(np.mean(SF_params['tau'])*w)/w\n",
    "    spf_del = np.arctan(np.mean(SPF_params['tau'])*w)/w\n",
    "    delay = spf_del + sf_del + AFe__SF_conn['delay'] + SFe__SPF_conn['delay']\n",
    "    steps = int(round(delay/net.min_delay))\n",
    "    AF_params['delay'] = AF_params['delay'] + (\n",
    "                         net_params['min_delay'] * (np.ceil(delay/net_params['min_delay']) + 1))\n",
    "    AF__M_syn['extra_steps'] = steps\n",
    "    #*************************************************************\n",
    "    # utilitiy function for the M-->C delays used in the rga rule\n",
    "    def approx_del(f):\n",
    "        \"\"\" Returns an estimate fo the optimal delay for rga learning.\n",
    "\n",
    "            We assume that the important loop for the learning rule in the C units\n",
    "            is the one going through C-AL-P-AF-M-C.\n",
    "            We also assume the delays to/from CI are the same as the ones for CE.\n",
    "\n",
    "            Args:\n",
    "                f : oscillation frequency of E-I pair in C, in Hertz\n",
    "            Returns:\n",
    "                2-tuple : (time_del, del_steps)\n",
    "                time_del : A float with the time delay.\n",
    "                del_steps : time delay as integer number of min_del steps.\n",
    "        \"\"\"\n",
    "        w = 2.*np.pi*f\n",
    "        al_del = np.arctan(np.mean(AL_params['tau'])*w)/w\n",
    "        p_del = np.arctan(np.mean(P_params['tau'])*w)/w\n",
    "        af_del = np.arctan(np.mean(AF_params['tau'])*w)/w\n",
    "        m_del = np.arctan(np.mean(M_params['tau'])*w)/w\n",
    "        D = [CE__AL_conn['delay'], AL__P_conn['delays'], np.mean(P__AF_conn['delays']),\n",
    "             AF__M_conn['delay'], M__CE_conn['delay'] ]\n",
    "        time_del = al_del + p_del + af_del + m_del + sum(D)\n",
    "        del_steps = int(np.ceil(time_del/net_params['min_delay']))\n",
    "        time_del = del_steps*net_params['min_delay']\n",
    "        del_steps -= 1 # because this is an index, and indexes start at 0\n",
    "        return time_del, del_steps\n",
    "    ############## Approximating the delays for the rga rule #############\n",
    "    ######## Using the utility function (for rga synapses)\n",
    "    # time_del, del_steps = approx_del(0.01) #0.65 was approximate CE/CI frequency observed in simulations\n",
    "    # #time_del, del_steps = (1., 200-1)\n",
    "    # CE_params['delay'] = time_del\n",
    "    # CI_params['delay'] = time_del\n",
    "    # M_params['delay'] = time_del\n",
    "    # CE_params['custom_inp_del'] = del_steps\n",
    "    # CI_params['custom_inp_del'] = del_steps\n",
    "    ######## Using the two custom delays (for rga_diff synapses)\n",
    "    dely1 = round(cfg['dely_low']/net.min_delay)*net.min_delay\n",
    "    dely2 = dely1 + round(cfg['dely_diff']/net.min_delay)*net.min_delay\n",
    "    del_steps1 = int(np.ceil(dely1/net_params['min_delay'])) - 1\n",
    "    del_steps2 = int(np.ceil(dely2/net_params['min_delay'])) - 1\n",
    "    CE_params['delay'] = dely2 + 0.01\n",
    "    CI_params['delay'] = dely2 + 0.01\n",
    "    M_params['delay'] = dely2 + 0.01\n",
    "    CE_params['custom_inp_del'] = del_steps1\n",
    "    CI_params['custom_inp_del'] = del_steps1\n",
    "    CE_params['custom_inp_del2'] = del_steps2\n",
    "    CI_params['custom_inp_del2'] = del_steps2\n",
    "    #*************************************************************\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # CREATING UNITS\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "    P = net.create(1, P_params)\n",
    "    arm = net.plants[P]\n",
    "\n",
    "    ACT = net.create(1, ACT_params)\n",
    "    AF = net.create(36, AF_params)\n",
    "    AL = net.create(6, AL_params)\n",
    "    CE = net.create(6, CE_params)\n",
    "    CI = net.create(6, CI_params)\n",
    "    M = net.create(12, M_params)\n",
    "    SF = net.create(12, SF_params)\n",
    "    SP = net.create(12, SP_params)\n",
    "    SP_CHG = net.create(1, SP_CHG_params)\n",
    "    SPF = net.create(12, SPF_params)\n",
    "\n",
    "    # SET THE PATTERNS IN SP -----------------------------------------------------\n",
    "    # list with hand coordinates [x,y] (meters)\n",
    "    if rand_targets is False:\n",
    "        hand_coords = [[0.3, 0.45], \n",
    "                       [0.35, 0.4],\n",
    "                       [0.4, 0.35],\n",
    "                       [0.35, 0.3],\n",
    "                       [0.3, 0.25],\n",
    "                       [0.25, 0.3],\n",
    "                       [0.2, 0.35],\n",
    "                       [0.25, 0.4]]\n",
    "                       #[-0.1, 0.3],\n",
    "                       #[-0.1, 0.35]] # experimental extra coordinates\n",
    "    else:\n",
    "        # creating a list of random coordinates to use as targets\n",
    "        min_s_ang = -0.1 # minimum shoulder angle\n",
    "        max_s_ang = 0.8  # maximum shoulder angle\n",
    "        min_e_ang = 0.2 # minimum elbow angle\n",
    "        max_e_ang = 2.3 # maximum elbow angle\n",
    "        n_coords = 1000 # number of coordinates to generate\n",
    "        l_arm = net.plants[P].l_arm # upper arm length\n",
    "        l_farm = net.plants[P].l_farm # forearm length\n",
    "        hand_coords = [[0.,0.] for _ in range(n_coords)]\n",
    "        s_angs = (np.random.random(n_coords)+min_s_ang)*(max_s_ang-min_s_ang)\n",
    "        e_angs = (np.random.random(n_coords)+min_e_ang)*(max_e_ang-min_e_ang)\n",
    "        for i in range(n_coords):\n",
    "            hand_coords[i][0] = l_arm*np.cos(s_angs[i]) + l_farm*np.cos(s_angs[i]+e_angs[i]) # x-coordinate\n",
    "            hand_coords[i][1] = l_arm*np.sin(s_angs[i]) + l_farm*np.sin(s_angs[i]+e_angs[i]) # y-coordinate\n",
    "\n",
    "    # list with muscle lengths corresponding to the hand coordinates\n",
    "    m_lengths = []\n",
    "    for coord in hand_coords:\n",
    "        m_lengths.append(arm.coords_to_lengths(coord))\n",
    "    m_lengths = np.array(m_lengths)\n",
    "    #(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)\n",
    "    # We need to translate these lengths to corresponding SF activity levels.\n",
    "    # For that it is necessary to recreate all their transformations\n",
    "    # The first transformation is from length to Ia, II afferent activity.\n",
    "    ### OUT OF THE 36 AFFERENT SIGNALS, WE TAKE THE Ia AND II ###\n",
    "    par = net.plants[P].m_params\n",
    "    # steady state tensions in the static and dynamic bag fibers (no gamma inputs)\n",
    "    Ts_ss = (par['k_se_s']/(par['k_se_s']+par['k_pe_s'])) * (\n",
    "             par['k_pe_s']*(m_lengths - par['l0_s']))\n",
    "    Td_ss = (par['k_se_d']/(par['k_se_d']+par['k_pe_d'])) * (\n",
    "             par['k_pe_d']*(m_lengths - par['l0_d']))\n",
    "    # steady state afferent outputs (no gamma inputs)\n",
    "    Ia_ss = par['fs']*(Ts_ss/par['k_se_s']) + (1.-par['fs'])*(Td_ss/par['k_se_d'])\n",
    "    II_ss = par['se_II']*(Ts_ss/par['k_se_s']) + ((1.-par['se_II'])/par['k_pe_s'])*Ts_ss\n",
    "    Ia_ss *= par['Ia_gain']\n",
    "    II_ss *= par['II_gain']\n",
    "    Ia_II_ss = np.concatenate((Ia_ss, II_ss), axis=1)\n",
    "    # Next transformation is through the chwr_linear afferent units\n",
    "    Pe__AF_ws = np.array(Pe__AF_syn['init_w'][6:18])\n",
    "    Pi__AF_ws = np.array(Pi__AF_syn['init_w'][6:18])\n",
    "    #Ia_II_avgs = np.mean(Ia_II_ss, axis=0)  # when using hundreds of random targets\n",
    "    # target averages\n",
    "    AFe_thr = np.array([net.units[u].thresh for u in AF[6:18]])\n",
    "    AFi_thr = np.array([net.units[u].thresh for u in AF[24:36]])\n",
    "    #AF_Ia = np.maximum((Ia_ss - AF_avgs[0:6])*Pe__AF_Ia_ws - AF_thr[0:6], 0.)\n",
    "    #AF_II = np.maximum((II_ss - AF_avgs[6:12])*Pe__AF_II_ws - AF_thr[6:12], 0.)\n",
    "    AFe_Ia_II = np.log(1. + np.maximum((Ia_II_ss)*Pe__AF_ws - AFe_thr, 0.))\n",
    "    AFi_Ia_II = np.log(1. + np.maximum((Ia_II_ss)*Pi__AF_ws - AFi_thr, 0.))\n",
    "    #(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)\n",
    "    # Next is from AF to SF\n",
    "    SF_arg = AFe__SF_syn['init_w']*AFe_Ia_II + AFi__SF_syn['init_w']*AFi_Ia_II\n",
    "    SF_out = 1./ (1. + np.exp(-SF_params['slope']*(SF_arg - SF_params['thresh'])))\n",
    "    SF_params['init_val'] = SF_out # this might cause a smooth start\n",
    "    # now we set the values in SP\n",
    "    m_idxs = np.random.randint(len(hand_coords), size=1000) # index of all targets\n",
    "        #m_idxs[0] = 0 # for testing\n",
    "    AF_us = [net.units[u] for u in AF]\n",
    "\n",
    "    def SF_sigmo(idx, arg):\n",
    "        \"\"\" The sigmoidal function for SF unit with index SF[idx]. \"\"\"\n",
    "        return 1./ (1. + np.exp(-SF_params['slope'][idx]*(arg - SF_params['thresh'][idx])))\n",
    "\n",
    "    def cur_target(t):\n",
    "        \"\"\" Returns the index of the target at time t. \"\"\"\n",
    "        return m_idxs[int(np.floor(t/t_pres))]\n",
    "\n",
    "    def make_fun(idx):\n",
    "        \"\"\" create a function for the SP unit with index 'idx'. \"\"\"\n",
    "        return lambda t: SF_sigmo(idx, \n",
    "                            AFe__SF_syn['init_w'][idx] * (\n",
    "                            np.log(1. + max(Ia_II_ss[cur_target(t)][idx] * Pe__AF_ws[idx] - \n",
    "                            net.units[AF[6+idx]].thresh, 0.))) +\n",
    "                            AFi__SF_syn['init_w'][idx] * (\n",
    "                            np.log(1. + max(Ia_II_ss[cur_target(t)][idx] * Pi__AF_ws[idx] - \n",
    "                            net.units[AF[24+idx]].thresh, 0.))))\n",
    "        #return lambda t: SF_out[m_idxs[int(np.floor(t/t_pres))]][idx]\n",
    "\n",
    "    for idx, u in enumerate(SP):\n",
    "        net.units[u].set_function(make_fun(idx))\n",
    "\n",
    "    # tracking units\n",
    "    M_CE_track = net.create(len(M), track_params) # to track weights from M to CE\n",
    "    #M_CI_track = net.create(len(M), track_params) # to track weights from M to CI\n",
    "    AF_M0_track = net.create(18, track_params) # to track the weights from AF to M0\n",
    "\n",
    "    # xp_track = net.create(1, track_params) # del_avg_inp_deriv of C0 at port 1\n",
    "    # up_track = net.create(1, track_params) # to track the derivative of CE0\n",
    "    # if rga_diff is True:\n",
    "    #     sp_now_track = net.create(1, track_params)\n",
    "    #     sp_del_track = net.create(1, track_params)\n",
    "    #     spj_now_track = net.create(1, track_params)\n",
    "    #     spj_del_track = net.create(1, track_params)\n",
    "    # else:\n",
    "    #     sp_track = net.create(1, track_params) # avg_inp_deriv_mp for CE0 at port 0\n",
    "    #     spj_track = net.create(1, track_params) # input derivative for MX--CE0\n",
    "\n",
    "    ipx_track = net.create(12, track_params) # x coordinates of insertion points\n",
    "    ipy_track = net.create(12, track_params) # y coordinates of insertion points\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # CONNECTING\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # from M to CE\n",
    "    net.connect(M, CE, M__CE_conn, M__CE_syn)\n",
    "    # from M to CI\n",
    "    net.connect(M, CI, M__CI_conn, M__CI_syn)\n",
    "    # from CE to AL\n",
    "    net.connect(CE, AL, CE__AL_conn, CE__AL_syn)\n",
    "    # from CI to AL\n",
    "    net.connect(CI, AL, CI__AL_conn, CI__AL_syn)\n",
    "    # from AL to P\n",
    "    net.set_plant_inputs(AL, P, AL__P_conn, AL__P_syn)\n",
    "    # from P to AF\n",
    "    net.set_plant_outputs(P, AF[0:18], P__AF_conn, Pe__AF_syn)\n",
    "    net.set_plant_outputs(P, AF[18:36], P__AF_conn, Pi__AF_syn)\n",
    "    # from AF to SF. Only Ia and II are selected\n",
    "    net.connect(AF[6:18], SF, AFe__SF_conn, AFe__SF_syn)\n",
    "    net.connect(AF[24:36], SF, AFi__SF_conn, AFi__SF_syn)\n",
    "    # from AF to M\n",
    "    ## When connecting from all afferents:\n",
    "    net.connect(AF, M, AF__M_conn, AF__M_syn) # should be made before SPF-->M\n",
    "    ## When connecting only from tension afferents\n",
    "    #net.connect(AF[0:6]+AF[18:24], M, AF__M_conn, AF__M_syn) # should be made before SPF-->M\n",
    "    # from AF to CE,CI\n",
    "    #net.connect(AF, CE, AF__CE_conn, AF__CE_syn)\n",
    "    #net.connect(AF, CI, AF__CI_conn, AF__CI_syn)\n",
    "    # from SF, SP to SPF\n",
    "    net.connect(SF, SPF, SFe__SPF_conn, SFe__SPF_syn) # F-P\n",
    "    #net.connect(SF, SPF, SFi__SPF_conn, SFi__SPF_syn)  # P-F\n",
    "    net.connect(SP, SPF, SPi__SPF_conn, SPi__SPF_syn) # F-P\n",
    "    #net.connect(SP, SPF, SPe__SPF_conn, SPe__SPF_syn)  # P-F\n",
    "    # from SPF to M\n",
    "    net.connect(SPF, M, SPF__M_conn, SPF__M_syn) # should be after AF-->M\n",
    "    # from SPF to ACT\n",
    "    net.connect(SPF, ACT, SPF__ACT_conn, SPF__ACT_syn)\n",
    "    # from SP to SP_CHG\n",
    "    net.connect(SP, SP_CHG, SP__SP_CHG_conn, SP__SP_CHG_syn)\n",
    "    # from SP_CHG to CE,CI\n",
    "    net.connect(SP_CHG, CE, SP_CHG__CE_conn, SP_CHG__CE_syn)\n",
    "    net.connect(SP_CHG, CI, SP_CHG__CI_conn, SP_CHG__CI_syn)\n",
    "    # from SP_CHG to M\n",
    "    net.connect(SP_CHG, M, SP_CHG__M_conn, SP_CHG__M_syn)\n",
    "    # from SP_CHG to ACT\n",
    "    net.connect(SP_CHG, ACT, SP_CHG__ACT_conn, SP_CHG__ACT_syn)\n",
    "    # from ACT to CE, CI\n",
    "    net.connect(ACT, CE, ACT__CE_conn, ACT__CE_syn)\n",
    "    net.connect(ACT, CI, ACT__CI_conn, ACT__CI_syn)\n",
    "    # intraspinal connections \n",
    "    # from CE to CI, and CI to CE\n",
    "    #net.connect(CE, CI, CE__CI_conn, CE__CI_syn)\n",
    "    #net.connect(CI, CE, CI__CE_conn, CI__CE_syn)\n",
    "    # agonists and antagonists\n",
    "    for pair in all_pairs:\n",
    "        if pair in synergists:\n",
    "            net.connect([CE[pair[0]]], [CE[pair[1]]], C__C_conn, C__C_syn_syne)\n",
    "            net.connect([CE[pair[0]]], [CI[pair[1]]], C__C_conn, C__C_syn_null_aff)\n",
    "            net.connect([CI[pair[0]]], [CE[pair[1]]], C__C_conn, C__C_syn_null_aff)\n",
    "            net.connect([CI[pair[0]]], [CI[pair[1]]], C__C_conn, C__C_syn_null_lat)\n",
    "        elif pair in part_syne:\n",
    "            net.connect([CE[pair[0]]], [CE[pair[1]]], C__C_conn, C__C_syn_p_syne)\n",
    "            net.connect([CE[pair[0]]], [CI[pair[1]]], C__C_conn, C__C_syn_null_aff)\n",
    "            net.connect([CI[pair[0]]], [CE[pair[1]]], C__C_conn, C__C_syn_null_aff)\n",
    "            net.connect([CI[pair[0]]], [CI[pair[1]]], C__C_conn, C__C_syn_null_lat)\n",
    "        elif pair in antagonists:\n",
    "            net.connect([CE[pair[0]]], [CI[pair[1]]], C__C_conn, C__C_syn_antag)\n",
    "            net.connect([CE[pair[0]]], [CE[pair[1]]], C__C_conn, C__C_syn_null_lat)\n",
    "            net.connect([CI[pair[0]]], [CE[pair[1]]], C__C_conn, C__C_syn_null_aff)\n",
    "            net.connect([CI[pair[0]]], [CI[pair[1]]], C__C_conn, C__C_syn_null_lat)\n",
    "        elif pair in part_antag:\n",
    "            net.connect([CE[pair[0]]], [CI[pair[1]]], C__C_conn, C__C_syn_p_antag)\n",
    "            net.connect([CE[pair[0]]], [CE[pair[1]]], C__C_conn, C__C_syn_null_lat)\n",
    "            net.connect([CI[pair[0]]], [CE[pair[1]]], C__C_conn, C__C_syn_null_aff)\n",
    "            net.connect([CI[pair[0]]], [CI[pair[1]]], C__C_conn, C__C_syn_null_lat)\n",
    "        elif pair in self_conn:\n",
    "            net.connect([CE[pair[0]]], [CI[pair[1]]], CE__CI_conn, CE__CI_syn)\n",
    "            net.connect([CI[pair[0]]], [CE[pair[1]]], CI__CE_conn, CI__CE_syn)\n",
    "        else:\n",
    "            net.connect([CE[pair[0]]], [CE[pair[1]]], C__C_conn, C__C_syn_null_lat)\n",
    "            net.connect([CE[pair[0]]], [CI[pair[1]]], C__C_conn, C__C_syn_null_aff)\n",
    "            net.connect([CI[pair[0]]], [CE[pair[1]]], C__C_conn, C__C_syn_null_aff)\n",
    "            net.connect([CI[pair[0]]], [CI[pair[1]]], C__C_conn, C__C_syn_null_lat)\n",
    "\n",
    "\n",
    "    pops_list = [SF, SP, SPF, AL, AF, SP_CHG, CE, CI, M, ACT, P]\n",
    "    pops_names = ['SF', 'SP', 'SPF', 'AL', 'AF', 'SP_CHG', 'CE', 'CI', 'M', 'ACT', 'P']\n",
    "    pops_dict = {pops_names[idx] : pops_list[idx] for idx in range(len(pops_names))}\n",
    "    return net, pops_dict, hand_coords, m_idxs, t_pres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A method to mutate individual parameters of a configuration\n",
    "focus_params = False # whether to focus mutations on a specific set of parameters\n",
    "\n",
    "# Specify paramters and ranges\n",
    "# defaults are based on incumbent 9 of smac_pars in v3_normal_smac_test.ipynb\n",
    "ranges = {\"dely_low\": {\"low\": 0.05, \"high\": 1., \"default\": 0.65 },\n",
    "          \"dely_diff\": {\"low\": 0.02, \"high\": 1., \"default\": 0.94 },\n",
    "          \"sig1\": {\"low\": 0.02, \"high\": 1., \"default\":0.5},\n",
    "          \"sig2\": {\"low\": 0.02, \"high\": 1., \"default\":0.3},\n",
    "          \"integ_amp\": {\"low\": 0.0, \"high\": 2., \"default\":0.24},\n",
    "          \"integ_decay\": {\"low\": 0.5, \"high\": 2., \"default\":1.7},\n",
    "          # tau_slow is important because it controls how long adaptation lasts\n",
    "          \"C_tau_slow\": {\"low\": 4., \"high\": 50., \"default\":27.},\n",
    "          # parameters from modifications to defaults in v3_normal_smac_test\n",
    "          \"SF_slope_factor\": {\"low\": 1., \"high\": 20., \"default\":8.},\n",
    "          \"M__C_w_sum\": {\"low\": 0.5, \"high\": 4., \"default\":3.},\n",
    "          \"g_e_factor\": {\"low\": 0.5, \"high\": 4., \"default\":2.3},\n",
    "          \"SPF_w\": {\"low\": 0.5, \"high\": 3., \"default\":1.5},\n",
    "          \"adapt_amp\": {\"low\": 0., \"high\": 20., \"default\":5.},\n",
    "          \"b_e\": {\"low\": .5, \"high\": 10., \"default\":3.5},\n",
    "          \"AL_thresh\": {\"low\": 0., \"high\": .8, \"default\":.3},\n",
    "          # parameters that came when playing with n3_nst_afxB\n",
    "          \"C__C_antag\": {\"low\": 0., \"high\": 2., \"default\":0.7}, \n",
    "          \"C__C_p_antag\": {\"low\": -0.5, \"high\": 1., \"default\":0.1},\n",
    "          \"CE__CI_w\": {\"low\": 0., \"high\": 2., \"default\":1.},\n",
    "          \"CI__CE_w\": {\"low\": -.3, \"high\": -.1, \"default\":-2.}\n",
    "         }\n",
    "\n",
    "par_list = [name for name in ranges] # ordered list with names of the parameters\n",
    "# parameters to focus on\n",
    "main_pars = [\"SF_slope_factor\", \"M__C_w_sum\", \"g_e_factor\", \"SPF_w\", \"b_e\", \"AL_thresh\",\n",
    "             \"C__C_antag\", \"C__C_p_antag\", \"CE__CI_w\", \"CI__CE_w\"]\n",
    "# if focus_params == True, ignore the other parameters\n",
    "if focus_params:\n",
    "    par_list = main_pars\n",
    "\n",
    "def mutate(cfg, name_list=par_list):\n",
    "    \"\"\" Mutate a single parameter of the given configuration. \n",
    "    \n",
    "        Args:\n",
    "            name_list: list with names of candidate parameters.\n",
    "    \"\"\"\n",
    "    n = np.random.randint(len(name_list))\n",
    "    par_name = name_list[n]\n",
    "    l = ranges[par_name]['low']\n",
    "    h = ranges[par_name]['high']\n",
    "    cfg[par_name] =  l + (h-l)*np.random.random()\n",
    "    \n",
    "def soft_mutate(cfg, ma, name_list=par_list):\n",
    "    \"\"\" Soft-mutate a single parameter of the given configuration.\n",
    "    \n",
    "        A 'soft mutation' keeps the mutated value close to the original value.\n",
    "        The maximum amplitude of the mutation is given by the 'ma' argument.\n",
    "        \n",
    "        Args:\n",
    "            ma : float in (0,1]. Max. amplitude as fraction of the parameter's range \n",
    "            name_list: list with names of candidate parameters.\n",
    "    \"\"\"\n",
    "    n = np.random.randint(len(name_list))\n",
    "    par_name = name_list[n]\n",
    "    l = ranges[par_name]['low']\n",
    "    h = ranges[par_name]['high']\n",
    "    cfg[par_name] = max(l, min(h, cfg[par_name] + ma * (h-l) * (np.random.random()-0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the initial population: step 1\n",
    "# Based on the SMAC incumbents\n",
    "smac_pop = [\n",
    "{'C_tau_slow':49.900, 'integ_decay':1.332, 'dely_diff':0.448, 'sig2':0.532, 'dely_low':0.769, 'sig1':0.486, 'M__C_lrate':20.256, 'integ_amp':1.868, 'adapt_amp':5.00, 'SF_slope_factor':8.00, 'g_e_factor':2.00, 'AL_thresh':0.30, 'M__C_w_sum':3.00, 'b_e':4.00, 'SPF_w':1.50 },\n",
    "{'C_tau_slow':27.252, 'integ_decay':1.115, 'dely_diff':0.830, 'sig2':0.602, 'dely_low':0.945, 'sig1':0.333, 'M__C_lrate':80.272, 'integ_amp':1.303, 'adapt_amp':5.00, 'SF_slope_factor':8.00, 'g_e_factor':2.00, 'AL_thresh':0.30, 'M__C_w_sum':3.00, 'b_e':4.00, 'SPF_w':1.50 },\n",
    "{'C_tau_slow':4.914, 'integ_decay':1.298, 'dely_diff':0.870, 'sig2':0.674, 'dely_low':0.102, 'sig1':0.656, 'M__C_lrate':18.098, 'integ_amp':0.980, 'adapt_amp':5.00, 'SF_slope_factor':8.00, 'g_e_factor':2.00, 'AL_thresh':0.30, 'M__C_w_sum':3.00, 'b_e':4.00, 'SPF_w':1.50 },\n",
    "{'C_tau_slow':18.108, 'integ_decay':1.897, 'dely_diff':0.916, 'sig2':0.543, 'dely_low':0.180, 'sig1':0.396, 'M__C_lrate':70.582, 'integ_amp':1.600, 'adapt_amp':5.00, 'SF_slope_factor':8.00, 'g_e_factor':2.00, 'AL_thresh':0.30, 'M__C_w_sum':3.00, 'b_e':4.00, 'SPF_w':1.50 },\n",
    "{'C_tau_slow':24.518, 'integ_decay':1.732, 'dely_diff':0.105, 'sig2':0.881, 'dely_low':0.482, 'sig1':0.296, 'M__C_lrate':10.718, 'integ_amp':0.053, 'adapt_amp':5.00, 'SF_slope_factor':8.00, 'g_e_factor':2.00, 'AL_thresh':0.30, 'M__C_w_sum':3.00, 'b_e':4.00, 'SPF_w':1.50 },\n",
    "{'C_tau_slow':25.131, 'integ_decay':1.264, 'dely_diff':0.152, 'sig2':0.522, 'dely_low':0.297, 'sig1':0.167, 'M__C_lrate':8.160, 'integ_amp':0.721, 'adapt_amp':5.00, 'SF_slope_factor':8.00, 'g_e_factor':2.00, 'AL_thresh':0.30, 'M__C_w_sum':3.00, 'b_e':4.00, 'SPF_w':1.50 },\n",
    "{'C_tau_slow':27.709, 'integ_decay':0.990, 'dely_diff':0.074, 'sig2':0.958, 'dely_low':0.385, 'sig1':0.142, 'M__C_lrate':62.358, 'integ_amp':1.229, 'adapt_amp':5.00, 'SF_slope_factor':8.00, 'g_e_factor':2.00, 'AL_thresh':0.30, 'M__C_w_sum':3.00, 'b_e':4.00, 'SPF_w':1.50 },\n",
    "{'C_tau_slow':12.997, 'integ_decay':0.623, 'dely_diff':0.508, 'sig2':0.363, 'dely_low':0.725, 'sig1':0.255, 'M__C_lrate':57.655, 'integ_amp':0.859, 'adapt_amp':5.00, 'SF_slope_factor':8.00, 'g_e_factor':2.00, 'AL_thresh':0.30, 'M__C_w_sum':3.00, 'b_e':4.00, 'SPF_w':1.50 },\n",
    "{'C_tau_slow':28.257, 'integ_decay':1.587, 'dely_diff':0.876, 'sig2':0.034, 'dely_low':0.841, 'sig1':0.272, 'M__C_lrate':19.252, 'integ_amp':1.978, 'adapt_amp':5.00, 'SF_slope_factor':8.00, 'g_e_factor':2.00, 'AL_thresh':0.30, 'M__C_w_sum':3.00, 'b_e':4.00, 'SPF_w':1.50 },\n",
    "{'C_tau_slow':27.010, 'integ_decay':1.676, 'dely_diff':0.938, 'sig2':0.307, 'dely_low':0.652, 'sig1':0.514, 'M__C_lrate':17.694, 'integ_amp':0.242, 'adapt_amp':5.00, 'SF_slope_factor':8.00, 'g_e_factor':2.00, 'AL_thresh':0.30, 'M__C_w_sum':3.00, 'b_e':4.00, 'SPF_w':1.50 },\n",
    "{'C_tau_slow':25.729, 'integ_decay':0.524, 'dely_diff':0.249, 'sig2':0.303, 'dely_low':0.975, 'sig1':0.413, 'M__C_lrate':48.402, 'integ_amp':0.794, 'adapt_amp':5.00, 'SF_slope_factor':8.00, 'g_e_factor':2.00, 'AL_thresh':0.30, 'M__C_w_sum':3.00, 'b_e':4.00, 'SPF_w':1.50 },\n",
    "{'C_tau_slow':39.010, 'integ_decay':1.341, 'dely_diff':0.499, 'sig2':0.684, 'dely_low':0.334, 'sig1':0.955, 'M__C_lrate':33.119, 'integ_amp':0.165, 'adapt_amp':5.00, 'SF_slope_factor':8.00, 'g_e_factor':2.00, 'AL_thresh':0.30, 'M__C_w_sum':3.00, 'b_e':4.00, 'SPF_w':1.50 },\n",
    "{'C_tau_slow':27.252, 'integ_decay':1.177, 'dely_diff':0.979, 'sig2':0.703, 'dely_low':0.983, 'sig1':0.128, 'M__C_lrate':79.764, 'integ_amp':1.266, 'adapt_amp':5.00, 'SF_slope_factor':8.00, 'g_e_factor':2.00, 'AL_thresh':0.30, 'M__C_w_sum':3.00, 'b_e':4.00, 'SPF_w':1.50 },\n",
    "{'C_tau_slow':29.745, 'integ_decay':1.500, 'dely_diff':0.050, 'sig2':0.606, 'dely_low':0.899, 'sig1':0.154, 'M__C_lrate':9.902, 'integ_amp':1.844, 'adapt_amp':5.00, 'SF_slope_factor':8.00, 'g_e_factor':2.00, 'AL_thresh':0.30, 'M__C_w_sum':3.00, 'b_e':4.00, 'SPF_w':1.50 },\n",
    "{'C_tau_slow':35.892, 'integ_decay':1.006, 'dely_diff':0.870, 'sig2':0.430, 'dely_low':0.478, 'sig1':0.452, 'M__C_lrate':33.372, 'integ_amp':1.246, 'adapt_amp':5.00, 'SF_slope_factor':8.00, 'g_e_factor':2.00, 'AL_thresh':0.30, 'M__C_w_sum':3.00, 'b_e':4.00, 'SPF_w':1.50 },\n",
    "{'C_tau_slow':34.454, 'integ_decay':1.519, 'dely_diff':0.182, 'sig2':0.121, 'dely_low':0.862, 'sig1':0.855, 'M__C_lrate':30.181, 'integ_amp':1.193, 'adapt_amp':5.00, 'SF_slope_factor':8.00, 'g_e_factor':2.00, 'AL_thresh':0.30, 'M__C_w_sum':3.00, 'b_e':4.00, 'SPF_w':1.50 },\n",
    "{'C_tau_slow':36.125, 'integ_decay':1.005, 'dely_diff':0.313, 'sig2':0.755, 'dely_low':0.281, 'sig1':0.902, 'M__C_lrate':19.180, 'integ_amp':1.293, 'adapt_amp':5.00, 'SF_slope_factor':8.00, 'g_e_factor':2.00, 'AL_thresh':0.30, 'M__C_w_sum':3.00, 'b_e':4.00, 'SPF_w':1.50 },\n",
    "{'C_tau_slow':27.216, 'integ_decay':1.192, 'dely_diff':0.606, 'sig2':0.647, 'dely_low':0.938, 'sig1':0.067, 'M__C_lrate':24.822, 'integ_amp':1.976, 'adapt_amp':5.00, 'SF_slope_factor':8.00, 'g_e_factor':2.00, 'AL_thresh':0.30, 'M__C_w_sum':3.00, 'b_e':4.00, 'SPF_w':1.50 },\n",
    "{'C_tau_slow':43.652, 'integ_decay':0.807, 'dely_diff':0.984, 'sig2':0.399, 'dely_low':0.080, 'sig1':0.259, 'M__C_lrate':19.631, 'integ_amp':0.164, 'adapt_amp':5.00, 'SF_slope_factor':8.00, 'g_e_factor':2.00, 'AL_thresh':0.30, 'M__C_w_sum':3.00, 'b_e':4.00, 'SPF_w':1.50 },\n",
    "{'C_tau_slow':40.219, 'integ_decay':1.590, 'dely_diff':0.089, 'sig2':0.502, 'dely_low':0.911, 'sig1':0.481, 'M__C_lrate':24.110, 'integ_amp':1.147, 'adapt_amp':5.00, 'SF_slope_factor':8.00, 'g_e_factor':2.00, 'AL_thresh':0.30, 'M__C_w_sum':3.00, 'b_e':4.00, 'SPF_w':1.50 }]\n",
    "\n",
    "\n",
    "# Load the results from generation 26 in v3_nst_afx_hyper\n",
    "fname = 'v3_nst_afx_pop_2020-08-09__20_39_gen26'\n",
    "with (open(fname, \"rb\")) as f:\n",
    "    gen26_pop = pickle.load(f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the initial population: step 2\n",
    "pop_size = 60\n",
    "# option 1: every individual shares the basic elements fo gen26_pop[0]\n",
    "# Useful for focused search\n",
    "#pop = [gen26_pop[0].copy() for _ in range(pop_size)]\n",
    "\n",
    "# option 2: initial population is gen_26_pop with last elements replaced by smac_pop\n",
    "pop = gen26_pop[:pop_size-len(smac_pop)] + smac_pop\n",
    "\n",
    "# create variations of the parameters we want to investigtate\n",
    "\n",
    "# first fill the default values in n_def elements\n",
    "n_def = 4\n",
    "for ind in range(n_def):\n",
    "    for name in main_pars:\n",
    "        pop[ind][name] = ranges[name]['default']\n",
    "        \n",
    "# fill the rest with variations in both directions\n",
    "chg_name = [\"low\", \"default\", \"high\"] # auxiliary list\n",
    "for ind in range(n_def, pop_size):\n",
    "    chg_dirs = [chg_name[i] for i in np.random.randint(3, size=len(par_list))]\n",
    "    for idx, name in enumerate(par_list):\n",
    "        pop[ind][name] = 0.5 * (ranges[name][chg_dirs[idx]] + ranges[name][\"default\"])\n",
    "\n",
    "# reset fitness and number of evaluations\n",
    "for dic in pop:\n",
    "    dic['fitness'] = None # average fitness value\n",
    "    dic['n_evals'] = 0  # number of times fitness has been evaluated"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    \n",
    "# the first variation is the base population where each configuration undergoes `n_muts_1` soft mutations\n",
    "n_muts_1 = 8\n",
    "var_pop_1 = [dic.copy() for dic in base_pop]\n",
    "#var_pop_1 = base_pop.copy()\n",
    "for dic in var_pop_1:\n",
    "    for _ in range(n_muts_1):\n",
    "        soft_mutate(dic, 0.2)\n",
    "        \n",
    "# the second variation is the var_pop_1 with `n_muts_2` mutations on each configuration\n",
    "n_muts_2 = 10\n",
    "var_pop_2 = [dic.copy() for dic in var_pop_1]\n",
    "#var_pop_2 = base_pop.copy()\n",
    "for dic in var_pop_2:\n",
    "    for _ in range(n_muts_2):\n",
    "        mutate(dic)\n",
    "        \n",
    "pop = base_pop + var_pop_1 + var_pop_2 # the initial population\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sig2':0.65, 'adapt_amp':5.00, 'C_tau_slow':23.24, 'g_e_factor':2.30, 'integ_amp':1.94, 'M__C_w_sum':3.00, 'b_e':3.50, 'n_evals':0.00, 'sig1':0.07, 'CI__CE_w':-2.00, 'C__C_p_antag':0.10, 'CE__CI_w':1.00, 'C__C_antag':0.70, 'dely_low':0.94, 'AL_thresh':0.30, 'M__C_lrate':24.82, 'SF_slope_factor':8.00, 'dely_diff':0.61, 'integ_decay':1.68, 'SPF_w':1.50, }\n",
      "\n",
      "{'sig2':0.65, 'adapt_amp':5.00, 'C_tau_slow':23.24, 'g_e_factor':2.30, 'integ_amp':1.94, 'M__C_w_sum':3.00, 'b_e':3.50, 'n_evals':0.00, 'sig1':0.07, 'CI__CE_w':-2.00, 'C__C_p_antag':0.10, 'CE__CI_w':1.00, 'C__C_antag':0.70, 'dely_low':0.94, 'AL_thresh':0.30, 'M__C_lrate':24.82, 'SF_slope_factor':8.00, 'dely_diff':0.61, 'integ_decay':1.68, 'SPF_w':1.50, }\n",
      "\n",
      "{'sig2':0.65, 'adapt_amp':5.00, 'C_tau_slow':23.24, 'g_e_factor':2.30, 'integ_amp':1.94, 'M__C_w_sum':3.00, 'b_e':3.50, 'n_evals':0.00, 'sig1':0.07, 'CI__CE_w':-2.00, 'C__C_p_antag':0.10, 'CE__CI_w':1.00, 'C__C_antag':0.70, 'dely_low':0.94, 'AL_thresh':0.30, 'M__C_lrate':24.82, 'SF_slope_factor':8.00, 'dely_diff':0.61, 'integ_decay':1.68, 'SPF_w':1.50, }\n",
      "\n",
      "{'sig2':0.65, 'adapt_amp':5.00, 'C_tau_slow':23.24, 'g_e_factor':2.30, 'integ_amp':1.94, 'M__C_w_sum':3.00, 'b_e':3.50, 'n_evals':0.00, 'sig1':0.07, 'CI__CE_w':-2.00, 'C__C_p_antag':0.10, 'CE__CI_w':1.00, 'C__C_antag':0.70, 'dely_low':0.94, 'AL_thresh':0.30, 'M__C_lrate':24.82, 'SF_slope_factor':8.00, 'dely_diff':0.61, 'integ_decay':1.68, 'SPF_w':1.50, }\n",
      "\n",
      "{'sig2':0.65, 'adapt_amp':5.00, 'C_tau_slow':23.24, 'g_e_factor':3.15, 'integ_amp':1.94, 'M__C_w_sum':1.75, 'b_e':2.00, 'n_evals':0.00, 'sig1':0.07, 'CI__CE_w':-1.05, 'C__C_p_antag':-0.20, 'CE__CI_w':1.50, 'C__C_antag':0.70, 'dely_low':0.94, 'AL_thresh':0.15, 'M__C_lrate':24.82, 'SF_slope_factor':14.00, 'dely_diff':0.61, 'integ_decay':1.68, 'SPF_w':2.25, }\n",
      "\n",
      "{'sig2':0.65, 'adapt_amp':5.00, 'C_tau_slow':23.24, 'g_e_factor':1.40, 'integ_amp':1.94, 'M__C_w_sum':1.75, 'b_e':3.50, 'n_evals':0.00, 'sig1':0.07, 'CI__CE_w':-1.15, 'C__C_p_antag':0.55, 'CE__CI_w':1.00, 'C__C_antag':1.35, 'dely_low':0.94, 'AL_thresh':0.30, 'M__C_lrate':24.82, 'SF_slope_factor':4.50, 'dely_diff':0.61, 'integ_decay':1.68, 'SPF_w':1.50, }\n",
      "\n",
      "{'sig2':0.65, 'adapt_amp':5.00, 'C_tau_slow':23.24, 'g_e_factor':2.30, 'integ_amp':1.94, 'M__C_w_sum':3.00, 'b_e':2.00, 'n_evals':0.00, 'sig1':0.07, 'CI__CE_w':-1.15, 'C__C_p_antag':0.55, 'CE__CI_w':0.50, 'C__C_antag':1.35, 'dely_low':0.94, 'AL_thresh':0.15, 'M__C_lrate':24.82, 'SF_slope_factor':14.00, 'dely_diff':0.61, 'integ_decay':1.68, 'SPF_w':2.25, }\n",
      "\n",
      "{'sig2':0.65, 'adapt_amp':5.00, 'C_tau_slow':23.24, 'g_e_factor':2.30, 'integ_amp':1.94, 'M__C_w_sum':3.00, 'b_e':3.50, 'n_evals':0.00, 'sig1':0.07, 'CI__CE_w':-1.15, 'C__C_p_antag':0.55, 'CE__CI_w':1.00, 'C__C_antag':0.70, 'dely_low':0.94, 'AL_thresh':0.15, 'M__C_lrate':24.82, 'SF_slope_factor':4.50, 'dely_diff':0.61, 'integ_decay':1.68, 'SPF_w':1.00, }\n",
      "\n",
      "{'sig2':0.65, 'adapt_amp':5.00, 'C_tau_slow':23.24, 'g_e_factor':1.40, 'integ_amp':1.94, 'M__C_w_sum':3.00, 'b_e':6.75, 'n_evals':0.00, 'sig1':0.07, 'CI__CE_w':-2.00, 'C__C_p_antag':0.55, 'CE__CI_w':0.50, 'C__C_antag':0.70, 'dely_low':0.94, 'AL_thresh':0.30, 'M__C_lrate':24.82, 'SF_slope_factor':4.50, 'dely_diff':0.61, 'integ_decay':1.68, 'SPF_w':2.25, }\n",
      "\n",
      "{'sig2':0.65, 'adapt_amp':5.00, 'C_tau_slow':23.24, 'g_e_factor':1.40, 'integ_amp':1.94, 'M__C_w_sum':3.50, 'b_e':6.75, 'n_evals':0.00, 'sig1':0.07, 'CI__CE_w':-1.15, 'C__C_p_antag':0.55, 'CE__CI_w':0.50, 'C__C_antag':0.35, 'dely_low':0.94, 'AL_thresh':0.30, 'M__C_lrate':24.82, 'SF_slope_factor':8.00, 'dely_diff':0.61, 'integ_decay':1.68, 'SPF_w':2.25, }\n",
      "\n",
      "{'sig2':0.65, 'adapt_amp':5.00, 'C_tau_slow':23.24, 'g_e_factor':2.30, 'integ_amp':1.94, 'M__C_w_sum':1.75, 'b_e':3.50, 'n_evals':0.00, 'sig1':0.07, 'CI__CE_w':-1.15, 'C__C_p_antag':0.55, 'CE__CI_w':0.50, 'C__C_antag':0.35, 'dely_low':0.94, 'AL_thresh':0.30, 'M__C_lrate':24.82, 'SF_slope_factor':4.50, 'dely_diff':0.61, 'integ_decay':1.68, 'SPF_w':1.50, }\n",
      "\n",
      "{'sig2':0.65, 'adapt_amp':5.00, 'C_tau_slow':23.24, 'g_e_factor':2.30, 'integ_amp':1.94, 'M__C_w_sum':3.50, 'b_e':6.75, 'n_evals':0.00, 'sig1':0.07, 'CI__CE_w':-1.05, 'C__C_p_antag':0.55, 'CE__CI_w':1.00, 'C__C_antag':0.70, 'dely_low':0.94, 'AL_thresh':0.15, 'M__C_lrate':24.82, 'SF_slope_factor':14.00, 'dely_diff':0.61, 'integ_decay':1.68, 'SPF_w':1.50, }\n",
      "\n",
      "{'sig2':0.65, 'adapt_amp':5.00, 'C_tau_slow':23.24, 'g_e_factor':2.30, 'integ_amp':1.94, 'M__C_w_sum':3.00, 'b_e':6.75, 'n_evals':0.00, 'sig1':0.07, 'CI__CE_w':-2.00, 'C__C_p_antag':0.10, 'CE__CI_w':0.50, 'C__C_antag':0.35, 'dely_low':0.94, 'AL_thresh':0.55, 'M__C_lrate':24.82, 'SF_slope_factor':14.00, 'dely_diff':0.61, 'integ_decay':1.68, 'SPF_w':1.00, }\n",
      "\n",
      "{'sig2':0.65, 'adapt_amp':5.00, 'C_tau_slow':23.24, 'g_e_factor':2.30, 'integ_amp':1.94, 'M__C_w_sum':3.00, 'b_e':6.75, 'n_evals':0.00, 'sig1':0.07, 'CI__CE_w':-2.00, 'C__C_p_antag':0.10, 'CE__CI_w':0.50, 'C__C_antag':0.70, 'dely_low':0.94, 'AL_thresh':0.30, 'M__C_lrate':24.82, 'SF_slope_factor':4.50, 'dely_diff':0.61, 'integ_decay':1.68, 'SPF_w':1.50, }\n",
      "\n",
      "{'sig2':0.65, 'adapt_amp':5.00, 'C_tau_slow':23.24, 'g_e_factor':3.15, 'integ_amp':1.94, 'M__C_w_sum':3.00, 'b_e':6.75, 'n_evals':0.00, 'sig1':0.07, 'CI__CE_w':-1.05, 'C__C_p_antag':0.55, 'CE__CI_w':1.00, 'C__C_antag':0.70, 'dely_low':0.94, 'AL_thresh':0.15, 'M__C_lrate':24.82, 'SF_slope_factor':4.50, 'dely_diff':0.61, 'integ_decay':1.68, 'SPF_w':1.00, }\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print used configuration\n",
    "for dic in pop[0:15]:\n",
    "    print('{',end='')\n",
    "    for name in dic.keys():\n",
    "        if name != 'fitness' or dic['fitness'] != None:\n",
    "            print(\"\\'%s\\':%.2f, \" % (name, dic[name]), end='')\n",
    "    print('}\\n')\n",
    "\n",
    "#pop[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Load the results from a previous run\n",
    "fname = 'v3_nst_afx_pop_'\n",
    "with (open(fname, \"rb\")) as f:\n",
    "    gen26_pop = pickle.load(f)\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to produce offspring by crossing individuals\n",
    "par_names = list(pop[0].keys()) # list with all parameter names\n",
    "\n",
    "def create_offspring(cfg1, cfg2, par_list=par_names):\n",
    "    \"\"\" Given 2 configurations, return 2 offspring from random swapping.\n",
    "    \n",
    "        To produce offspring, first we choose one split point in the\n",
    "        dictionary. The first offspring has the values of cfg1 up to that\n",
    "        point, and cfg2 afterwards. The second offspring has the cfg2 values\n",
    "        up to the split point, and cfg1 afterwards. Since the dictionaries are\n",
    "        not ordered, we use a parameter list to set the split point.\n",
    "    \n",
    "        Args:\n",
    "            cfg1, cfg2: parameter dictionaries\n",
    "            par_list: list with the keys in cfg1, cfg2\n",
    "        Returns:\n",
    "            cfg3, cfg4: dictionaries from swapping values in cfg1, cfg2\n",
    "    \"\"\"\n",
    "    if focus_params:\n",
    "        par_list = main_pars\n",
    "    sp = np.random.randint(len(par_list))# split point as an index to par_list\n",
    "    cfg3 = cfg1.copy()\n",
    "    cfg4 = cfg2.copy()\n",
    "    for i in range(sp, len(par_list)):\n",
    "        cfg3[par_list[i]] = cfg2[par_list[i]]\n",
    "        cfg4[par_list[i]] = cfg1[par_list[i]]\n",
    "    return cfg3, cfg4\n",
    "\n",
    "# visualize\n",
    "# cfg3, cfg4 = create_offspring(pop[10], pop[11])\n",
    "# for dic in (pop[10], pop[11], cfg3, cfg4):\n",
    "#     print('{',end='')\n",
    "#     for name in par_names:\n",
    "#         if name != \"fitness\":\n",
    "#             print(\"\\'%s\\':%.2f, \" % (name, dic[name]), end='')\n",
    "#     print('}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# A function that evaluates the fitness of a given configuration\n",
    "def eval_config(cfg):\n",
    "    \"\"\" Returns the error for a network with a given configuration.\n",
    "\n",
    "        Args:\n",
    "            cfg : a configuration dictionary.\n",
    "        Returns:\n",
    "            error : A float calculated from the sum of activities in the SPF layer.\n",
    "    \"\"\"\n",
    "    if cfg['n_evals'] > 8: # if the fitness has been evaluated \"enough\" times. See cell below...\n",
    "        return cfg['fitness']\n",
    "    \n",
    "    # obtain a network with the given configuration\n",
    "    net, pops_dict, hand_coords, m_idxs, t_pres = net_from_cfg(cfg)\n",
    "\n",
    "    # run the network\n",
    "    run_time = 900.\n",
    "    #start_time = time.time()\n",
    "    times, data, plant_data  = net.flat_run(run_time)\n",
    "    #print('Execution time is %s seconds' % (time.time() - start_time))\n",
    "\n",
    "    # calculate average error in last ~300 seconds of reaching\n",
    "    P = pops_dict['P']\n",
    "    arm_activs = plant_data[P]\n",
    "    plant = net.plants[P]\n",
    "    # modified copy-paste of plt.upd_ip_impl\n",
    "    q1 = arm_activs[:,0]\n",
    "    q2 = arm_activs[:,2]\n",
    "    q12 = q1+q2\n",
    "    c_elbow = np.array((plant.l_arm*np.cos(q1), plant.l_arm*np.sin(q1)))\n",
    "    c_hand = np.array((c_elbow[0] + plant.l_farm*np.cos(q12),\n",
    "                    c_elbow[1] + plant.l_farm*np.sin(q12))).transpose()\n",
    "    coord_idxs = np.floor(times/t_pres).astype(int)\n",
    "    des_coords = np.array(hand_coords)[m_idxs[coord_idxs],:] # desired coordinates at each moment in time\n",
    "\n",
    "    error_time = run_time - round(run_time/3.)\n",
    "    error_idx = int(round(error_time/net.min_delay))\n",
    "    hand_error = np.linalg.norm(c_hand-des_coords, axis=1)\n",
    "    hand_error_integ = hand_error[error_idx:].sum()\n",
    "    avg_hand_error = hand_error_integ / (hand_error.size - error_idx)\n",
    "\n",
    "    #return hand_error_integ\n",
    "    return avg_hand_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/z/projects/draculab/network.py:916: UserWarning: Integration method odeint substituted by Forward Euler in some units\n",
      "  ' substituted by Forward Euler in some units', UserWarning)\n",
      "/home/z/projects/draculab/network.py:916: UserWarning: Integration method odeint substituted by Forward Euler in some units\n",
      "  ' substituted by Forward Euler in some units', UserWarning)\n",
      "/home/z/projects/draculab/network.py:916: UserWarning: Integration method odeint substituted by Forward Euler in some units\n",
      "  ' substituted by Forward Euler in some units', UserWarning)\n",
      "/home/z/projects/draculab/network.py:916: UserWarning: Integration method odeint substituted by Forward Euler in some units\n",
      "  ' substituted by Forward Euler in some units', UserWarning)\n",
      "/home/z/projects/draculab/network.py:916: UserWarning: Integration method odeint substituted by Forward Euler in some units\n",
      "  ' substituted by Forward Euler in some units', UserWarning)\n",
      "/home/z/projects/draculab/network.py:916: UserWarning: Integration method odeint substituted by Forward Euler in some units\n",
      "  ' substituted by Forward Euler in some units', UserWarning)\n",
      "/home/z/projects/draculab/network.py:916: UserWarning: Integration method odeint substituted by Forward Euler in some units\n",
      "  ' substituted by Forward Euler in some units', UserWarning)\n",
      "/home/z/projects/draculab/network.py:916: UserWarning: Integration method odeint substituted by Forward Euler in some units\n",
      "  ' substituted by Forward Euler in some units', UserWarning)\n",
      "/home/z/projects/draculab/network.py:916: UserWarning: Integration method odeint substituted by Forward Euler in some units\n",
      "  ' substituted by Forward Euler in some units', UserWarning)\n",
      "/home/z/projects/draculab/network.py:916: UserWarning: Integration method odeint substituted by Forward Euler in some units\n",
      "  ' substituted by Forward Euler in some units', UserWarning)\n",
      "/home/z/projects/draculab/network.py:916: UserWarning: Integration method odeint substituted by Forward Euler in some units\n",
      "  ' substituted by Forward Euler in some units', UserWarning)\n",
      "/home/z/projects/draculab/network.py:916: UserWarning: Integration method odeint substituted by Forward Euler in some units\n",
      "  ' substituted by Forward Euler in some units', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "###### The genetic algorithm ######\n",
    "####################################\n",
    "\n",
    "# pop = pop[0:6] # limit pop size for debugging\n",
    "n_mates = 12 # number of individuals to mate at each generation (even number)\n",
    "max_gens = 30 # maximum number of generations\n",
    "n_soft_mut = 6 # numbef of individuals to soft-mutate per generation\n",
    "r_soft_mut = 0.2 # relative amplitude of soft mutations\n",
    "n_mut = 6 # number of individuals to mutate per generation\n",
    "n_procs = 12 # number of processes to use for fitness evaluation\n",
    "n_save = 4 # number of individuals to protect from replacement and mutation\n",
    "# setting name for file where parameters will be stored\n",
    "fname = \"v3_nst_afx_pop\"\n",
    "fname += \"_\" + datetime.now().strftime('%Y-%m-%d__%H_%M')\n",
    "\n",
    "for gen in range(max_gens):\n",
    "    start_time = time.time()\n",
    "    # 1) Evaluate fitness\n",
    "    # 1.1) Do the evaluation\n",
    "    ######### Single process version\n",
    "    #fits = list(map(eval_config, pop))\n",
    "    ######## parallel version\n",
    "    with Pool(n_procs) as p:\n",
    "        fits = list(p.map(eval_config, pop))\n",
    "        p.close()\n",
    "        p.join()\n",
    "    print(fits)\n",
    "    # 1.2) update the average fitness values\n",
    "    for idx, cfg in enumerate(pop):\n",
    "        nr = cfg['n_evals'] # n_evals is not updated yet...\n",
    "        #print(nr)\n",
    "        #print(fits[idx])\n",
    "        #print(cfg['fitness'])\n",
    "        if nr > 0:\n",
    "            if nr < 9: # horrible arbitrary number that might be different in eval_config :( \n",
    "                cfg['fitness'] = (cfg['fitness']*nr + fits[idx])/(nr+1)\n",
    "        else:\n",
    "            cfg['fitness'] = fits[idx]\n",
    "        cfg['n_evals'] = cfg['n_evals'] + 1\n",
    "    # 2) Sort according to fitness. Lowest error first.\n",
    "    pop = sorted(pop, key=lambda d: d['fitness'])\n",
    "    # 2.1) Save current generation\n",
    "    with open(fname, 'wb') as f:\n",
    "        pickle.dump(pop, f)\n",
    "        f.close()\n",
    "    # 2.2) A quick message\n",
    "    print(\"Generation %d evaluated. Best fitness: %.3f\"%(gen,pop[0]['fitness']))\n",
    "    print(\"Mean fitness = %.3f\"%(np.mean(np.array(fits))))\n",
    "    # 2.3) If best fitness good enough, break\n",
    "    if pop[0]['fitness'] < 0.02:\n",
    "        print(\"Good enough parameters found. Stopping search.\")\n",
    "        break\n",
    "    # 3) mate and replace\n",
    "    # 3.1) Select individuals to be replaced with probability proportional to error\n",
    "    fits.sort() # sort the fitnesses (now in the same order as pop)\n",
    "    fits = np.array(fits)\n",
    "    fits = fits/fits.sum() # normalize fitnesses so they add to 1\n",
    "    cumsum_fits = fits[:] # cumsum_fits[i] = sum(fits[:i])\n",
    "    for i in range(1,len(cumsum_fits)):\n",
    "        cumsum_fits[i] = cumsum_fits[i-1] + cumsum_fits[i]\n",
    "    repl_list = [] # list with indexes of individuals to be replaced\n",
    "    while len(repl_list) < n_mates:\n",
    "        min_r = cumsum_fits[n_save] # don't replace the first n_save individuals\n",
    "        r = min_r + (1.-min_r) * np.random.random()\n",
    "        candidate = n_save\n",
    "        for i in range(n_save, len(fits)):\n",
    "            if cumsum_fits[i] > r:\n",
    "                break\n",
    "            candidate += 1\n",
    "        if candidate in repl_list:\n",
    "            continue\n",
    "        else:\n",
    "            repl_list.append(candidate)\n",
    "    print(\"to replace: \", end='')\n",
    "    print(repl_list)\n",
    "    # 3.2) Arrange individuals in random pairs\n",
    "    perm = np.random.permutation(n_mates) # this will do \n",
    "    # 3.3) mate\n",
    "    new_pops = []\n",
    "    for i in range(int(np.floor(n_mates/2))):\n",
    "        off1, off2 = create_offspring(pop[perm[2*i]], pop[perm[2*i+1]])\n",
    "        new_pops.append(off1)\n",
    "        new_pops.append(off2)\n",
    "    # 3.4) replace\n",
    "    for i, cfg in enumerate(new_pops):\n",
    "        pop[repl_list[i]] = cfg\n",
    "    # 4) mutate\n",
    "    # 4.1) soft mutations\n",
    "    for _ in range(n_soft_mut):\n",
    "        idx = np.random.randint(len(pop))\n",
    "        if idx < n_save:\n",
    "            copy = pop[idx].copy()\n",
    "            soft_mutate(copy, r_soft_mut)\n",
    "            pop[-idx-1] = copy\n",
    "            pop[-idx-1]['fitness'] = None\n",
    "            pop[-idx-1]['n_evals'] = 0\n",
    "        else:\n",
    "            soft_mutate(pop[idx], r_soft_mut)\n",
    "            pop[idx]['fitness'] = None\n",
    "            pop[idx]['n_evals'] = 0\n",
    "    # 4.2) mutations\n",
    "    # 4.2.1) select individuals to mutate\n",
    "    # sq_fits = fits*fits\n",
    "    cumsum_sq_fits = fits * fits # cumsum_sq_fits[i] = sum(sq_fits[:i])\n",
    "    cumsum_sq_fits = cumsum_sq_fits / cumsum_sq_fits.sum()\n",
    "    for i in range(1,len(cumsum_sq_fits)):\n",
    "        cumsum_sq_fits[i] = cumsum_sq_fits[i-1] + cumsum_sq_fits[i]\n",
    "    mut_list = [] # list with indexes of individuals to be mutate\n",
    "    while len(mut_list) < n_mut:\n",
    "        r = np.random.random()\n",
    "        candidate = 0\n",
    "        for i in range(len(fits)):\n",
    "            if cumsum_sq_fits[i] > r:\n",
    "                break\n",
    "            candidate += 1\n",
    "        if candidate in mut_list:\n",
    "            continue\n",
    "        else:\n",
    "            mut_list.append(candidate)\n",
    "    print(\"to mutate: \", end='')\n",
    "    print(mut_list)\n",
    "    for idx in mut_list:\n",
    "        if idx < n_save:\n",
    "            copy = pop[idx].copy()\n",
    "            mutate(copy)\n",
    "            pop[-idx-1] = copy\n",
    "            pop[-idx-1]['fitness'] = None\n",
    "            pop[-idx-1]['n_evals'] = 0\n",
    "        else:\n",
    "            mutate(pop[idx])\n",
    "            pop[idx]['fitness'] = None\n",
    "            pop[idx]['n_evals'] = 0\n",
    "            \n",
    "    print('generation %d finished in %s seconds' % (gen, time.time() - start_time))\n",
    "#     while len(mut_list) < n_mut:\n",
    "#         min_r = cumsum_sq_fits[n_save] # don't mutate the first n_save individuals\n",
    "#         r = min_r + (1.-min_r) * np.random.random()\n",
    "#         candidate = n_save\n",
    "#         for i in range(n_save, len(fits)):\n",
    "#             if cumsum_sq_fits[i] > r:\n",
    "#                 break\n",
    "#             candidate += 1\n",
    "#         if candidate in mut_list:\n",
    "#             continue\n",
    "#         else:\n",
    "#             mut_list.append(candidate)\n",
    "#     print(\"to mutate: \", end='')\n",
    "#     print(mut_list)\n",
    "#     for _ in range(n_mut):\n",
    "#         idx = np.random.randint(len(pop))\n",
    "#         mutate(pop[idx])\n",
    "#         pop[idx]['fitness'] = None\n",
    "#         pop[idx]['n_evals'] = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SF_slope_factor':14.00, 'g_e_factor':3.15, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.05, 'SPF_w':1.50, 'CE__CI_w':0.50, 'C__C_p_antag':0.10, 'dely_diff':0.61, 'C__C_antag':0.35, 'b_e':2.00, 'sig1':0.07, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'integ_decay':1.68, 'M__C_w_sum':1.75, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.55, 'CI__CE_w':-1.05, }\n",
      "\n",
      "{'SF_slope_factor':8.00, 'g_e_factor':3.15, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.05, 'SPF_w':1.50, 'CE__CI_w':1.50, 'C__C_p_antag':0.55, 'dely_diff':0.61, 'C__C_antag':0.35, 'b_e':6.75, 'sig1':0.07, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'integ_decay':1.68, 'M__C_w_sum':1.75, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.15, 'CI__CE_w':-2.00, }\n",
      "\n",
      "{'SF_slope_factor':4.50, 'g_e_factor':1.40, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.06, 'SPF_w':2.25, 'CE__CI_w':1.00, 'C__C_p_antag':0.10, 'dely_diff':0.61, 'C__C_antag':0.35, 'b_e':2.00, 'sig1':0.07, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'integ_decay':1.68, 'M__C_w_sum':3.50, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.15, 'CI__CE_w':-1.15, }\n",
      "\n",
      "{'SF_slope_factor':8.00, 'g_e_factor':3.15, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.06, 'SPF_w':1.00, 'CE__CI_w':1.50, 'C__C_p_antag':-0.20, 'dely_diff':0.61, 'C__C_antag':0.70, 'b_e':2.00, 'sig1':0.07, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'integ_decay':1.68, 'M__C_w_sum':3.00, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.30, 'CI__CE_w':-2.00, }\n",
      "\n",
      "{'SF_slope_factor':8.00, 'g_e_factor':1.40, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.06, 'SPF_w':2.25, 'CE__CI_w':0.50, 'C__C_p_antag':-0.20, 'dely_diff':0.61, 'C__C_antag':0.70, 'b_e':2.00, 'sig1':0.07, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'integ_decay':1.68, 'M__C_w_sum':3.00, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.30, 'CI__CE_w':-1.15, }\n",
      "\n",
      "{'SF_slope_factor':8.00, 'g_e_factor':1.40, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.07, 'SPF_w':1.00, 'CE__CI_w':1.50, 'C__C_p_antag':0.55, 'dely_diff':0.61, 'C__C_antag':0.70, 'b_e':2.00, 'sig1':0.07, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'integ_decay':1.68, 'M__C_w_sum':3.50, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.55, 'CI__CE_w':-2.00, }\n",
      "\n",
      "{'SF_slope_factor':14.00, 'g_e_factor':2.30, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.07, 'SPF_w':1.00, 'CE__CI_w':1.50, 'C__C_p_antag':-0.20, 'dely_diff':0.61, 'C__C_antag':1.35, 'b_e':3.50, 'sig1':0.07, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'integ_decay':1.68, 'M__C_w_sum':1.75, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.15, 'CI__CE_w':-1.05, }\n",
      "\n",
      "{'SF_slope_factor':14.00, 'g_e_factor':3.15, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.05, 'SPF_w':1.50, 'CE__CI_w':1.50, 'C__C_p_antag':0.55, 'dely_diff':0.61, 'C__C_antag':0.70, 'b_e':2.00, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'sig1':0.07, 'integ_decay':1.68, 'M__C_w_sum':1.75, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.55, 'CI__CE_w':-2.00, }\n",
      "\n",
      "{'SF_slope_factor':4.50, 'g_e_factor':3.15, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.08, 'SPF_w':1.00, 'CE__CI_w':0.50, 'C__C_p_antag':-0.20, 'dely_diff':0.61, 'C__C_antag':0.35, 'b_e':3.50, 'sig1':0.07, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'integ_decay':1.68, 'M__C_w_sum':3.00, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.15, 'CI__CE_w':-2.00, }\n",
      "\n",
      "{'SF_slope_factor':4.50, 'g_e_factor':1.40, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.08, 'SPF_w':2.25, 'CE__CI_w':0.50, 'C__C_p_antag':0.55, 'dely_diff':0.61, 'C__C_antag':0.70, 'b_e':3.50, 'sig1':0.07, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'integ_decay':1.68, 'M__C_w_sum':3.00, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.55, 'CI__CE_w':-1.15, }\n",
      "\n",
      "{'SF_slope_factor':14.00, 'g_e_factor':1.40, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.08, 'SPF_w':1.00, 'CE__CI_w':0.50, 'C__C_p_antag':0.55, 'dely_diff':0.61, 'C__C_antag':1.35, 'b_e':6.75, 'sig1':0.07, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'integ_decay':1.68, 'M__C_w_sum':3.50, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.55, 'CI__CE_w':-1.15, }\n",
      "\n",
      "{'SF_slope_factor':4.50, 'g_e_factor':1.40, 'n_evals':0.00, 'integ_amp':1.94, 'SPF_w':1.00, 'CE__CI_w':1.00, 'C__C_p_antag':0.10, 'dely_diff':0.61, 'C__C_antag':0.35, 'b_e':2.00, 'sig1':0.07, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'integ_decay':1.68, 'M__C_w_sum':3.00, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.22, 'CI__CE_w':-1.05, }\n",
      "\n",
      "{'SF_slope_factor':14.00, 'g_e_factor':2.30, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.08, 'SPF_w':1.00, 'CE__CI_w':0.50, 'C__C_p_antag':0.10, 'dely_diff':0.61, 'C__C_antag':1.35, 'b_e':2.00, 'sig1':0.07, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'integ_decay':1.68, 'M__C_w_sum':3.00, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.55, 'CI__CE_w':-1.15, }\n",
      "\n",
      "{'SF_slope_factor':4.50, 'g_e_factor':2.30, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.09, 'SPF_w':1.50, 'CE__CI_w':0.50, 'C__C_p_antag':-0.20, 'dely_diff':0.61, 'C__C_antag':1.35, 'b_e':2.00, 'sig1':0.07, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'integ_decay':1.68, 'M__C_w_sum':1.75, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.15, 'CI__CE_w':-1.15, }\n",
      "\n",
      "{'SF_slope_factor':8.00, 'g_e_factor':2.30, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.09, 'SPF_w':1.50, 'CE__CI_w':1.00, 'C__C_p_antag':0.55, 'dely_diff':0.61, 'C__C_antag':1.35, 'b_e':2.00, 'sig1':0.07, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'integ_decay':1.68, 'M__C_w_sum':1.75, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.55, 'CI__CE_w':-2.00, }\n",
      "\n",
      "{'SF_slope_factor':14.00, 'g_e_factor':2.30, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.09, 'SPF_w':1.00, 'CE__CI_w':1.50, 'C__C_p_antag':0.10, 'dely_diff':0.61, 'C__C_antag':1.35, 'b_e':2.00, 'sig1':0.07, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'integ_decay':1.68, 'M__C_w_sum':3.50, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.55, 'CI__CE_w':-1.15, }\n",
      "\n",
      "{'SF_slope_factor':8.00, 'g_e_factor':1.40, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.09, 'SPF_w':2.25, 'CE__CI_w':0.50, 'C__C_p_antag':-0.20, 'dely_diff':0.61, 'C__C_antag':1.35, 'b_e':6.75, 'sig1':0.07, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'integ_decay':1.68, 'M__C_w_sum':1.75, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.15, 'CI__CE_w':-2.00, }\n",
      "\n",
      "{'SF_slope_factor':4.50, 'g_e_factor':1.40, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.09, 'SPF_w':2.25, 'CE__CI_w':1.00, 'C__C_p_antag':-0.20, 'dely_diff':0.61, 'C__C_antag':0.70, 'b_e':2.00, 'sig1':0.07, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'integ_decay':1.68, 'M__C_w_sum':3.00, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.55, 'CI__CE_w':-1.15, }\n",
      "\n",
      "{'SF_slope_factor':8.00, 'g_e_factor':2.30, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.09, 'SPF_w':1.50, 'CE__CI_w':1.00, 'C__C_p_antag':0.10, 'dely_diff':0.61, 'C__C_antag':0.70, 'b_e':3.50, 'sig1':0.07, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'integ_decay':1.68, 'M__C_w_sum':3.00, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.30, 'CI__CE_w':-2.00, }\n",
      "\n",
      "{'SF_slope_factor':8.00, 'g_e_factor':2.30, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.09, 'SPF_w':1.50, 'CE__CI_w':1.00, 'C__C_p_antag':0.10, 'dely_diff':0.61, 'C__C_antag':0.70, 'b_e':3.50, 'sig1':0.07, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'integ_decay':1.68, 'M__C_w_sum':3.00, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.30, 'CI__CE_w':-2.00, }\n",
      "\n",
      "{'SF_slope_factor':14.00, 'g_e_factor':1.40, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.09, 'SPF_w':1.50, 'CE__CI_w':1.00, 'C__C_p_antag':-0.20, 'dely_diff':0.61, 'C__C_antag':0.35, 'b_e':3.50, 'sig1':0.07, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'integ_decay':1.68, 'M__C_w_sum':1.75, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.30, 'CI__CE_w':-1.15, }\n",
      "\n",
      "{'SF_slope_factor':14.00, 'g_e_factor':1.40, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.09, 'SPF_w':1.50, 'CE__CI_w':1.00, 'C__C_p_antag':0.10, 'dely_diff':0.61, 'C__C_antag':0.70, 'b_e':6.75, 'sig1':0.07, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'integ_decay':1.68, 'M__C_w_sum':3.50, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.55, 'CI__CE_w':-1.15, }\n",
      "\n",
      "{'SF_slope_factor':14.00, 'g_e_factor':1.40, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.09, 'SPF_w':1.00, 'CE__CI_w':1.00, 'C__C_p_antag':0.10, 'dely_diff':0.61, 'C__C_antag':0.35, 'b_e':6.75, 'sig1':0.07, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'integ_decay':1.68, 'M__C_w_sum':3.50, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.55, 'CI__CE_w':-1.15, }\n",
      "\n",
      "{'SF_slope_factor':4.50, 'g_e_factor':2.30, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.10, 'SPF_w':1.00, 'CE__CI_w':1.00, 'C__C_p_antag':0.10, 'dely_diff':0.61, 'C__C_antag':0.70, 'b_e':6.75, 'sig1':0.07, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'integ_decay':1.68, 'M__C_w_sum':3.00, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.55, 'CI__CE_w':-1.15, }\n",
      "\n",
      "{'SF_slope_factor':4.50, 'g_e_factor':3.15, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.10, 'SPF_w':1.50, 'CE__CI_w':0.50, 'C__C_p_antag':0.55, 'dely_diff':0.61, 'C__C_antag':0.70, 'b_e':6.75, 'sig1':0.07, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'integ_decay':1.68, 'M__C_w_sum':1.75, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.55, 'CI__CE_w':-2.00, }\n",
      "\n",
      "{'SF_slope_factor':4.50, 'g_e_factor':1.40, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.10, 'SPF_w':2.25, 'CE__CI_w':1.00, 'C__C_p_antag':0.10, 'dely_diff':0.61, 'C__C_antag':0.70, 'b_e':6.75, 'sig1':0.07, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'integ_decay':1.68, 'M__C_w_sum':3.00, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.55, 'CI__CE_w':-1.05, }\n",
      "\n",
      "{'SF_slope_factor':4.50, 'g_e_factor':3.15, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.10, 'SPF_w':1.00, 'CE__CI_w':0.50, 'C__C_p_antag':-0.20, 'dely_diff':0.61, 'C__C_antag':1.35, 'b_e':6.75, 'sig1':0.07, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'integ_decay':1.68, 'M__C_w_sum':1.75, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.30, 'CI__CE_w':-2.00, }\n",
      "\n",
      "{'SF_slope_factor':14.00, 'g_e_factor':1.40, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.08, 'SPF_w':1.00, 'CE__CI_w':0.50, 'C__C_p_antag':0.55, 'dely_diff':0.61, 'C__C_antag':1.35, 'b_e':6.75, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'sig1':0.07, 'integ_decay':1.68, 'M__C_w_sum':3.50, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.55, 'CI__CE_w':-1.15, }\n",
      "\n",
      "{'SF_slope_factor':8.00, 'g_e_factor':2.30, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.10, 'SPF_w':1.00, 'CE__CI_w':1.50, 'C__C_p_antag':0.10, 'dely_diff':0.61, 'C__C_antag':0.70, 'b_e':2.00, 'sig1':0.07, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'integ_decay':1.68, 'M__C_w_sum':1.75, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.55, 'CI__CE_w':-2.00, }\n",
      "\n",
      "{'SF_slope_factor':4.50, 'g_e_factor':1.40, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.11, 'SPF_w':1.00, 'CE__CI_w':0.50, 'C__C_p_antag':0.55, 'dely_diff':0.61, 'C__C_antag':0.35, 'b_e':2.00, 'sig1':0.07, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'integ_decay':1.68, 'M__C_w_sum':1.75, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.30, 'CI__CE_w':-1.05, }\n",
      "\n",
      "{'SF_slope_factor':4.50, 'g_e_factor':3.15, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.11, 'SPF_w':1.50, 'CE__CI_w':0.50, 'C__C_p_antag':-0.20, 'dely_diff':0.61, 'C__C_antag':1.35, 'b_e':3.50, 'sig1':0.07, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'integ_decay':1.68, 'M__C_w_sum':3.50, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.15, 'CI__CE_w':-2.00, }\n",
      "\n",
      "{'SF_slope_factor':4.50, 'g_e_factor':3.15, 'n_evals':0.00, 'integ_amp':1.94, 'SPF_w':1.00, 'CE__CI_w':1.00, 'C__C_p_antag':-0.20, 'dely_diff':0.61, 'C__C_antag':1.35, 'b_e':6.75, 'sig1':0.07, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'integ_decay':1.68, 'M__C_w_sum':1.75, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.37, 'CI__CE_w':-1.15, }\n",
      "\n",
      "{'SF_slope_factor':8.00, 'g_e_factor':1.40, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.06, 'SPF_w':2.25, 'CE__CI_w':1.50, 'C__C_p_antag':-0.20, 'dely_diff':0.61, 'C__C_antag':1.35, 'b_e':2.00, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'sig1':0.07, 'integ_decay':1.68, 'M__C_w_sum':3.00, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.30, 'CI__CE_w':-1.05, }\n",
      "\n",
      "{'SF_slope_factor':4.50, 'g_e_factor':2.30, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.12, 'SPF_w':1.00, 'CE__CI_w':1.00, 'C__C_p_antag':0.55, 'dely_diff':0.61, 'C__C_antag':1.35, 'b_e':6.75, 'sig1':0.07, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'integ_decay':1.68, 'M__C_w_sum':3.50, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.30, 'CI__CE_w':-2.00, }\n",
      "\n",
      "{'SF_slope_factor':14.00, 'g_e_factor':2.30, 'n_evals':0.00, 'integ_amp':1.94, 'SPF_w':1.50, 'CE__CI_w':0.50, 'C__C_p_antag':-0.33, 'dely_diff':0.61, 'C__C_antag':0.35, 'b_e':3.50, 'sig1':0.07, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'integ_decay':1.68, 'M__C_w_sum':3.50, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.55, 'CI__CE_w':-2.00, }\n",
      "\n",
      "{'SF_slope_factor':8.00, 'g_e_factor':2.30, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.13, 'SPF_w':1.50, 'CE__CI_w':1.00, 'C__C_p_antag':0.10, 'dely_diff':0.61, 'C__C_antag':0.70, 'b_e':3.50, 'sig1':0.07, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'integ_decay':1.68, 'M__C_w_sum':3.00, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.30, 'CI__CE_w':-2.00, }\n",
      "\n",
      "{'SF_slope_factor':8.00, 'g_e_factor':2.30, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.13, 'SPF_w':1.50, 'CE__CI_w':1.00, 'C__C_p_antag':0.10, 'dely_diff':0.61, 'C__C_antag':0.70, 'b_e':3.50, 'sig1':0.07, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'integ_decay':1.68, 'M__C_w_sum':3.00, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.30, 'CI__CE_w':-2.00, }\n",
      "\n",
      "{'SF_slope_factor':14.00, 'g_e_factor':2.30, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.07, 'SPF_w':1.00, 'CE__CI_w':0.50, 'C__C_p_antag':-0.20, 'dely_diff':0.61, 'C__C_antag':0.70, 'b_e':3.50, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'sig1':0.07, 'integ_decay':1.68, 'M__C_w_sum':1.75, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.15, 'CI__CE_w':-1.15, }\n",
      "\n",
      "{'SF_slope_factor':4.50, 'g_e_factor':2.30, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.14, 'SPF_w':1.50, 'CE__CI_w':1.50, 'C__C_p_antag':-0.20, 'dely_diff':0.61, 'C__C_antag':0.70, 'b_e':6.75, 'sig1':0.07, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'integ_decay':1.68, 'M__C_w_sum':3.50, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.30, 'CI__CE_w':-1.05, }\n",
      "\n",
      "{'SF_slope_factor':4.50, 'g_e_factor':1.40, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.08, 'SPF_w':1.00, 'CE__CI_w':0.50, 'C__C_p_antag':-0.20, 'dely_diff':0.61, 'C__C_antag':0.35, 'b_e':2.00, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'sig1':0.07, 'integ_decay':1.68, 'M__C_w_sum':3.00, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.15, 'CI__CE_w':-2.00, }\n",
      "\n",
      "{'SF_slope_factor':4.50, 'g_e_factor':1.40, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.14, 'SPF_w':1.00, 'CE__CI_w':1.50, 'C__C_p_antag':-0.20, 'dely_diff':0.61, 'C__C_antag':0.70, 'b_e':3.50, 'sig1':0.07, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'integ_decay':1.68, 'M__C_w_sum':1.75, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.15, 'CI__CE_w':-1.15, }\n",
      "\n",
      "{'SF_slope_factor':14.00, 'g_e_factor':1.40, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.15, 'SPF_w':2.25, 'CE__CI_w':0.50, 'C__C_p_antag':-0.20, 'dely_diff':0.61, 'C__C_antag':1.35, 'b_e':3.50, 'sig1':0.07, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'integ_decay':1.68, 'M__C_w_sum':3.00, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.30, 'CI__CE_w':-2.00, }\n",
      "\n",
      "{'SF_slope_factor':8.00, 'g_e_factor':3.15, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.05, 'SPF_w':1.50, 'CE__CI_w':1.50, 'C__C_p_antag':-0.20, 'dely_diff':0.61, 'C__C_antag':0.70, 'b_e':6.75, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'sig1':0.07, 'integ_decay':1.68, 'M__C_w_sum':1.75, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.30, 'CI__CE_w':-2.00, }\n",
      "\n",
      "{'SF_slope_factor':4.50, 'g_e_factor':1.40, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.16, 'SPF_w':1.50, 'CE__CI_w':1.00, 'C__C_p_antag':0.55, 'dely_diff':0.61, 'C__C_antag':0.35, 'b_e':2.00, 'sig1':0.07, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'integ_decay':1.68, 'M__C_w_sum':3.50, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.30, 'CI__CE_w':-1.15, }\n",
      "\n",
      "{'SF_slope_factor':8.00, 'g_e_factor':2.30, 'n_evals':0.00, 'integ_amp':1.94, 'SPF_w':1.00, 'CE__CI_w':1.00, 'C__C_p_antag':0.55, 'dely_diff':0.61, 'C__C_antag':0.70, 'b_e':3.50, 'sig1':0.07, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'integ_decay':1.68, 'M__C_w_sum':2.00, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.55, 'CI__CE_w':-1.05, }\n",
      "\n",
      "{'SF_slope_factor':4.50, 'g_e_factor':1.40, 'n_evals':0.00, 'integ_amp':1.94, 'SPF_w':2.25, 'CE__CI_w':0.50, 'C__C_p_antag':0.55, 'dely_diff':0.61, 'C__C_antag':0.70, 'b_e':3.50, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'sig1':0.07, 'integ_decay':1.68, 'M__C_w_sum':3.00, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.49, 'CI__CE_w':-1.15, }\n",
      "\n",
      "{'SF_slope_factor':14.00, 'g_e_factor':2.30, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.17, 'SPF_w':1.50, 'CE__CI_w':1.50, 'C__C_p_antag':0.10, 'dely_diff':0.61, 'C__C_antag':0.35, 'b_e':3.50, 'sig1':0.07, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'integ_decay':1.68, 'M__C_w_sum':1.75, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.55, 'CI__CE_w':-2.00, }\n",
      "\n",
      "{'SF_slope_factor':14.00, 'g_e_factor':2.30, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.19, 'SPF_w':1.50, 'CE__CI_w':1.50, 'C__C_p_antag':-0.20, 'dely_diff':0.61, 'C__C_antag':1.35, 'b_e':6.75, 'sig1':0.07, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'integ_decay':1.68, 'M__C_w_sum':3.00, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.30, 'CI__CE_w':-1.15, }\n",
      "\n",
      "{'SF_slope_factor':8.00, 'g_e_factor':3.15, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.06, 'SPF_w':1.00, 'CE__CI_w':1.50, 'C__C_p_antag':0.55, 'dely_diff':0.61, 'C__C_antag':0.35, 'b_e':2.00, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'sig1':0.07, 'integ_decay':1.68, 'M__C_w_sum':3.00, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.15, 'CI__CE_w':-2.00, }\n",
      "\n",
      "{'SF_slope_factor':14.00, 'g_e_factor':2.30, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.20, 'SPF_w':2.25, 'CE__CI_w':1.50, 'C__C_p_antag':0.55, 'dely_diff':0.61, 'C__C_antag':1.35, 'b_e':6.75, 'sig1':0.07, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'integ_decay':1.68, 'M__C_w_sum':3.50, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.15, 'CI__CE_w':-1.15, }\n",
      "\n",
      "{'SF_slope_factor':14.00, 'g_e_factor':3.15, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.23, 'SPF_w':1.50, 'CE__CI_w':1.00, 'C__C_p_antag':0.55, 'dely_diff':0.61, 'C__C_antag':0.35, 'b_e':2.00, 'sig1':0.07, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'integ_decay':1.68, 'M__C_w_sum':3.50, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.30, 'CI__CE_w':-1.05, }\n",
      "\n",
      "{'SF_slope_factor':8.00, 'g_e_factor':1.40, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.24, 'SPF_w':2.25, 'CE__CI_w':0.50, 'C__C_p_antag':-0.20, 'dely_diff':0.61, 'C__C_antag':1.35, 'b_e':2.00, 'sig1':0.07, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'integ_decay':1.68, 'M__C_w_sum':1.75, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.55, 'CI__CE_w':-1.15, }\n",
      "\n",
      "{'SF_slope_factor':14.00, 'g_e_factor':2.30, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.24, 'SPF_w':2.25, 'CE__CI_w':0.50, 'C__C_p_antag':0.55, 'dely_diff':0.61, 'C__C_antag':1.35, 'b_e':6.75, 'sig1':0.07, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'integ_decay':1.68, 'M__C_w_sum':1.75, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.15, 'CI__CE_w':-1.15, }\n",
      "\n",
      "{'SF_slope_factor':14.00, 'g_e_factor':2.30, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.28, 'SPF_w':2.25, 'CE__CI_w':1.50, 'C__C_p_antag':0.55, 'dely_diff':0.61, 'C__C_antag':1.35, 'b_e':6.75, 'sig1':0.07, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'integ_decay':1.68, 'M__C_w_sum':1.75, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.30, 'CI__CE_w':-1.15, }\n",
      "\n",
      "{'SF_slope_factor':14.00, 'g_e_factor':2.30, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.30, 'SPF_w':1.50, 'CE__CI_w':1.00, 'C__C_p_antag':-0.20, 'dely_diff':0.61, 'C__C_antag':0.70, 'b_e':6.75, 'sig1':0.07, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'integ_decay':1.68, 'M__C_w_sum':1.75, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.15, 'CI__CE_w':-1.05, }\n",
      "\n",
      "{'SF_slope_factor':4.50, 'g_e_factor':1.40, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.06, 'SPF_w':2.25, 'CE__CI_w':1.00, 'C__C_p_antag':0.10, 'dely_diff':0.61, 'C__C_antag':0.35, 'b_e':2.00, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'sig1':0.07, 'integ_decay':1.68, 'M__C_w_sum':3.50, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.15, 'CI__CE_w':-1.15, }\n",
      "\n",
      "{'SF_slope_factor':8.00, 'g_e_factor':1.40, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.07, 'SPF_w':1.00, 'CE__CI_w':0.50, 'C__C_p_antag':0.10, 'dely_diff':0.61, 'C__C_antag':0.35, 'b_e':2.00, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'sig1':0.07, 'integ_decay':1.68, 'M__C_w_sum':3.50, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.55, 'CI__CE_w':-1.05, }\n",
      "\n",
      "{'SF_slope_factor':5.95, 'g_e_factor':1.40, 'n_evals':0.00, 'integ_amp':1.94, 'SPF_w':2.25, 'CE__CI_w':1.00, 'C__C_p_antag':0.10, 'dely_diff':0.61, 'C__C_antag':0.35, 'b_e':2.00, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'sig1':0.07, 'integ_decay':1.68, 'M__C_w_sum':3.50, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.15, 'CI__CE_w':-1.15, }\n",
      "\n",
      "{'SF_slope_factor':14.00, 'g_e_factor':3.15, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.48, 'SPF_w':2.25, 'CE__CI_w':0.50, 'C__C_p_antag':0.10, 'dely_diff':0.61, 'C__C_antag':1.35, 'b_e':2.00, 'sig1':0.07, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'integ_decay':1.68, 'M__C_w_sum':1.75, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.15, 'CI__CE_w':-1.15, }\n",
      "\n",
      "{'SF_slope_factor':4.50, 'g_e_factor':3.15, 'n_evals':1.00, 'integ_amp':1.94, 'fitness':0.08, 'SPF_w':1.00, 'CE__CI_w':1.00, 'C__C_p_antag':0.10, 'dely_diff':0.61, 'C__C_antag':0.35, 'b_e':3.50, 'dely_low':0.94, 'C_tau_slow':23.24, 'adapt_amp':5.00, 'sig1':0.07, 'integ_decay':1.68, 'M__C_w_sum':3.00, 'sig2':0.65, 'M__C_lrate':24.82, 'AL_thresh':0.15, 'CI__CE_w':-1.05, }\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AL_thresh': 0.55,\n",
       " 'CE__CI_w': 0.5,\n",
       " 'CI__CE_w': -1.05,\n",
       " 'C__C_antag': 0.35,\n",
       " 'C__C_p_antag': 0.1,\n",
       " 'C_tau_slow': 23.242490852056637,\n",
       " 'M__C_lrate': 24.822,\n",
       " 'M__C_w_sum': 1.75,\n",
       " 'SF_slope_factor': 14.0,\n",
       " 'SPF_w': 1.5,\n",
       " 'adapt_amp': 5.0,\n",
       " 'b_e': 2.0,\n",
       " 'dely_diff': 0.606,\n",
       " 'dely_low': 0.938,\n",
       " 'fitness': 0.053472247097058286,\n",
       " 'g_e_factor': 3.15,\n",
       " 'integ_amp': 1.9404408352576688,\n",
       " 'integ_decay': 1.676,\n",
       " 'n_evals': 1,\n",
       " 'sig1': 0.067,\n",
       " 'sig2': 0.647}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print final population\n",
    "for dic in pop:\n",
    "    print('{',end='')\n",
    "    for name in dic.keys():\n",
    "        if name != 'fitness' or dic['fitness'] != None:\n",
    "            print(\"\\'%s\\':%.2f, \" % (name, dic[name]), end='')\n",
    "    print('}\\n')\n",
    "\n",
    "pop[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final population\n",
    "fname = \"v3_nst_afx_pop\"\n",
    "fname += \"_\" + datetime.now().strftime('%Y-%m-%d__%H_%M')\n",
    "with open(fname, 'wb') as f:\n",
    "    pickle.dump(pop, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a saved population\n",
    "import pickle\n",
    "fname = 'v3_nst_afx_pop'\n",
    "with (open(fname, \"rb\")) as f:\n",
    "    results = pickle.load(f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#########################################\n",
    "####### Create initial population #######\n",
    "#########################################\n",
    "\n",
    "# SMAC results\n",
    "smac_pars = [\n",
    "{'C_tau_slow': 49.900, 'M__C_lrate': 20.256325499385603, 'dely_diff': 0.4477162684973992, 'dely_low': 0.7690321580597904, 'integ_amp': 1.8680498901623337, 'integ_decay': 1.331543939419169, 'sig1': 0.48604095779065143, 'sig2': 0.5320816125480841},\n",
    "{'C_tau_slow': 27.252, 'M__C_lrate': 80.27152103642422, 'dely_diff': 0.8302689896646621, 'dely_low': 0.9448004009625702, 'integ_amp': 1.3027796535384801, 'integ_decay': 1.1151473648799848, 'sig1': 0.3333822457927232, 'sig2': 0.602142622846035},\n",
    "{'C_tau_slow': 4.9141, 'M__C_lrate': 18.098479993035365, 'dely_diff': 0.8695623842178384, 'dely_low': 0.10163432855643398, 'integ_amp': 0.9804760861765895, 'integ_decay': 1.2978170473471935, 'sig1': 0.656398764586348, 'sig2': 0.6742611615552242},\n",
    "{'C_tau_slow': 18.108, 'M__C_lrate': 70.58210808493664, 'dely_diff': 0.9156992538198638, 'dely_low': 0.18042294270342568, 'integ_amp': 1.59953879143932, 'integ_decay': 1.897236067590745, 'sig1': 0.3957607764124875, 'sig2': 0.5429333172434175},\n",
    "{'C_tau_slow': 24.518, 'M__C_lrate': 10.7176426472955, 'dely_diff': 0.10514977247320526, 'dely_low': 0.4816425668212565, 'integ_amp': 0.052640553537827595, 'integ_decay': 1.7320132135471584, 'sig1': 0.295530218281941, 'sig2': 0.880618543454016},\n",
    "{'C_tau_slow': 25.131, 'M__C_lrate': 8.160115362344975, 'dely_diff': 0.15209745789146595, 'dely_low': 0.2971574565762005, 'integ_amp': 0.7212105628587586, 'integ_decay': 1.2643001767159114, 'sig1': 0.1669654700375103, 'sig2': 0.5220741046559334},\n",
    "{'C_tau_slow': 27.709, 'M__C_lrate': 62.35766801787423, 'dely_diff': 0.07370635778545681, 'dely_low': 0.38524713527134113, 'integ_amp': 1.2291955633815432, 'integ_decay': 0.9896030918849588, 'sig1': 0.14235717383922966, 'sig2': 0.9582724349305285},\n",
    "{'C_tau_slow': 12.997, 'M__C_lrate': 57.65450266617434, 'dely_diff': 0.5083698033447426, 'dely_low': 0.724537611535787, 'integ_amp': 0.8589370531537637, 'integ_decay': 0.6225080770661751, 'sig1': 0.25523782305135023, 'sig2': 0.36341948478792335},\n",
    "{'C_tau_slow': 28.257, 'M__C_lrate': 19.252231491689262, 'dely_diff': 0.8763317882227273, 'dely_low': 0.841057789838837, 'integ_amp': 1.9778746085958865, 'integ_decay': 1.5874651978467111, 'sig1': 0.2718302954029159, 'sig2': 0.03423522965364858},\n",
    "{'C_tau_slow': 27.009797921470376, 'M__C_lrate': 17.694235082185887, 'dely_diff': 0.9384702718097053, 'dely_low': 0.6517772404779072, 'integ_amp': 0.2424992022256729, 'integ_decay': 1.6757876363132431, 'sig1': 0.5141582780361669, 'sig2': 0.3071316800951735},\n",
    "{'C_tau_slow': 25.729475034960863, 'M__C_lrate': 48.401533341300016, 'dely_diff': 0.24880132489065293, 'dely_low': 0.9746745035966982, 'integ_amp': 0.794323985976406, 'integ_decay': 0.5237268865028553, 'sig1': 0.41274555308325395, 'sig2': 0.3028571295297247},\n",
    "{'C_tau_slow': 39.010267662409674, 'M__C_lrate': 33.119435678535304, 'dely_diff': 0.4986448750167204, 'dely_low': 0.33383698843467075, 'integ_amp': 0.16451224022611344, 'integ_decay': 1.3405396186429095, 'sig1': 0.9552533588686478, 'sig2': 0.6841484570576724},\n",
    "{'C_tau_slow': 27.252121017797577, 'M__C_lrate': 79.76447366382578, 'dely_diff': 0.9793711907204399, 'dely_low': 0.9830480060207323, 'integ_amp': 1.2659311648927787, 'integ_decay': 1.1769262305731851, 'sig1': 0.12834583948739434, 'sig2': 0.7027399937002696},\n",
    "{'C_tau_slow': 29.74481631589189, 'M__C_lrate': 9.901672707786418, 'dely_diff': 0.049978459103808534, 'dely_low': 0.8986963379124208, 'integ_amp': 1.8438486164295065, 'integ_decay': 1.4995234577998642, 'sig1': 0.15367679188724145, 'sig2': 0.6063690021347338},\n",
    "{'C_tau_slow': 35.89214723944998, 'M__C_lrate': 33.37190084703984, 'dely_diff': 0.8695479326390626, 'dely_low': 0.47774803863208554, 'integ_amp': 1.2463743741545108, 'integ_decay': 1.0057687470085104, 'sig1': 0.4519939350991195, 'sig2': 0.43024783779731374},\n",
    "{'C_tau_slow': 34.4537421036612, 'M__C_lrate': 30.181240076812124, 'dely_diff': 0.18244166780521462, 'dely_low': 0.8616640027270661, 'integ_amp': 1.1931544367082136, 'integ_decay': 1.5187889664847, 'sig1': 0.8553123793461737, 'sig2': 0.12069845945582337},\n",
    "{'C_tau_slow': 36.125431675125455, 'M__C_lrate': 19.179672286682916, 'dely_diff': 0.3129705870870137, 'dely_low': 0.2809586827400535, 'integ_amp': 1.293055288788612, 'integ_decay': 1.0052978776894752, 'sig1': 0.902479273391096, 'sig2': 0.755171077907023},\n",
    "{'C_tau_slow': 27.216028584333692, 'M__C_lrate': 24.821548897212057, 'dely_diff': 0.6056479425068247, 'dely_low': 0.9383520595323166, 'integ_amp': 1.9763366064562846, 'integ_decay': 1.1923753169724318, 'sig1': 0.06679587483694582, 'sig2': 0.6470469584127114},\n",
    "{'C_tau_slow': 43.65235157707517, 'M__C_lrate': 19.630877556410248, 'dely_diff': 0.9839068656993065, 'dely_low': 0.07985377947673952, 'integ_amp': 0.16376085261522544, 'integ_decay': 0.8070659249467205, 'sig1': 0.2586941180276561, 'sig2': 0.3986169740263769},\n",
    "{'C_tau_slow': 40.21935826195853, 'M__C_lrate': 24.109667476701, 'dely_diff': 0.08852751974460826, 'dely_low': 0.9107932744583151, 'integ_amp': 1.1474968260770186, 'integ_decay': 1.5899088641287276, 'sig1': 0.48106556698704567, 'sig2': 0.501550946801979}\n",
    "]\n",
    "\n",
    "# print SMAC parameter dictionaries appended with the new parameters\n",
    "for dic in smac_pars:\n",
    "    print('{',end='')\n",
    "    for par in dic:\n",
    "        print(\"\\'%s\\':%.3f, \" % (par, dic[par]), end='')\n",
    "    for par in ranges:\n",
    "        if not par in dic.keys():\n",
    "            print(\"\\'%s\\':%.2f, \" % (par, ranges[par]['default']),end='')\n",
    "    print('}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a configuation\n",
    "cfg_id = 0 # index in the population for the configuration\n",
    "net, pops_dict, hand_coords, m_idxs, t_pres = net_from_cfg(pop[cfg_id])\n",
    "pops_names = ['SF', 'SP', 'SPF', 'AL', 'AF', 'SP_CHG', 'CE', 'CI', 'M', 'ACT', 'P']\n",
    "for name in pops_names:\n",
    "    exec(\"%s = %s\"% (name, str(pops_dict[name])))\n",
    "\n",
    "start_time = time.time()\n",
    "times, data, plant_data  = net.flat_run(300.)\n",
    "#times, data, plant_data  = net.run(40.)\n",
    "print('Execution time is %s seconds' % (time.time() - start_time))\n",
    "data = np.array(data)\n",
    "#Execution time is 8.687349319458008 seconds  << before sc_inp_sum_mp, flat_run(5.)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# reducing the scope of the plots\n",
    "data_back = data\n",
    "times_back = times\n",
    "plant_data_back = [np.array([])]\n",
    "plant_data_back[0] = plant_data[0]\n",
    "\n",
    "first_idx=100*200\n",
    "second_idx=115*200\n",
    "times = times[first_idx:second_idx]\n",
    "data = data[:, first_idx:second_idx]\n",
    "plant_data[0] = plant_data[0][first_idx:second_idx,:]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# recover the data\n",
    "data = data_back\n",
    "plant_data[0] = plant_data_back[0]\n",
    "times = times_back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same history as `v3_nst_afx`\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arm_activs = plant_data[P]\n",
    "# SPF\n",
    "fs = (25,6)\n",
    "SPF_fig = plt.figure(figsize=fs)\n",
    "SPF_data = np.array(data[SPF])\n",
    "plt.plot(times, SPF_data.transpose())\n",
    "SPF_legends = ['SPF'+str(i) for i in range(len(SPF))]\n",
    "plt.legend(SPF_legends)\n",
    "plt.title('SPF')\n",
    "print(SPF_data[:,-1])\n",
    "\n",
    "# M\n",
    "M_fig = plt.figure(figsize=fs)\n",
    "M_data = np.array(data[M])\n",
    "plt.plot(times, M_data.transpose())\n",
    "M_legends = ['M'+str(i) for i in range(len(M))]\n",
    "plt.legend(M_legends)\n",
    "plt.title('M')\n",
    "print(M_data[:,-1])\n",
    "\n",
    "# C\n",
    "C_fig = plt.figure(figsize=fs)\n",
    "CE_data = np.array(data[CE])\n",
    "CI_data = np.array(data[CI])\n",
    "plt.plot(times, CE_data.transpose(), linewidth=2)\n",
    "plt.plot(times, CI_data.transpose(), '--')\n",
    "C_legends = ['CE'+str(i) for i in range(len(CE))]\n",
    "C_legends += ['CI'+str(i) for i in range(len(CI))]\n",
    "plt.legend(C_legends)\n",
    "plt.title('C')\n",
    "\n",
    "# M--CE0 weights\n",
    "W_fig1 = plt.figure(figsize=fs)\n",
    "w_track_data = np.array(data[M_CE_track])\n",
    "plt.plot(times, w_track_data.transpose())\n",
    "M_CE0_legends = ['M'+str(i)+'--CE0' for i in range(len(M_CE_track))]\n",
    "plt.legend(M_CE0_legends)\n",
    "plt.title('M--CE0 weights')\n",
    "\n",
    "# AF--M0 weights\n",
    "W_fig2 = plt.figure(figsize=fs)\n",
    "w_track_data2 = np.array(data[AF_M0_track[0:18]])\n",
    "plt.plot(times, w_track_data2.transpose())\n",
    "AF_M0_legends = ['AF'+str(i)+'--M0' for i in range(len(AF_M0_track[:18]))]\n",
    "plt.legend(AF_M0_legends)\n",
    "plt.title('AF--M0 weights exc')\n",
    "\"\"\"\n",
    "W_fig3 = plt.figure(figsize=fs)\n",
    "w_track_data3 = np.array(data[AF_M0_track[18:]])\n",
    "plt.plot(times, w_track_data3.transpose())\n",
    "AF_M0_legends2 = ['AF'+str(i)+'--M0' for i in range(len(AF_M0_track[18:]))]\n",
    "plt.legend(AF_M0_legends2)\n",
    "plt.title('AF--M0 weights inh')\n",
    "\"\"\"\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P\n",
    "P_fig = plt.figure(figsize=fs)\n",
    "P_state = plant_data[P][:,0:4:2]\n",
    "plt.plot(times, P_state)\n",
    "#plt.legend(['sh ang', 'sh ang vel', 'elb ang', 'elb ang vel'])\n",
    "plt.legend(['sh ang', 'elb ang',])\n",
    "plt.title('double pendulum state variables')\n",
    "print(\"shoulder angle: %f, shoulder vel: %f\" % (P_state[-1,0],P_state[-1,1]))\n",
    "\n",
    "# AF plus\n",
    "AFe_fig = plt.figure(figsize=fs)\n",
    "AFe_data = np.array(data[AF[0:18]])\n",
    "plt.plot(times, AFe_data.transpose())\n",
    "AF_legends = ['Ib' + str(i) for i in range(6)] + \\\n",
    "             ['Ia' + str(i) for i in range(6)] + \\\n",
    "             ['II' + str(i) for i in range(6)]\n",
    "plt.legend(AF_legends)\n",
    "plt.title('AF excited')\n",
    "print('AFe_data:')\n",
    "print(AFe_data[:,-1])\n",
    "\n",
    "# AF minus\n",
    "AFi_fig = plt.figure(figsize=fs)\n",
    "AFi_data = np.array(data[AF[18:36]])\n",
    "plt.plot(times, AFi_data.transpose())\n",
    "AF_legends = ['Ib' + str(i) for i in range(6)] + \\\n",
    "             ['Ia' + str(i) for i in range(6)] + \\\n",
    "             ['II' + str(i) for i in range(6)]\n",
    "plt.legend(AF_legends)\n",
    "plt.title('AF inhibited')\n",
    "print('AFi_data:')\n",
    "print(AFi_data[:,-1])\n",
    "\n",
    "fs = (30,10)\n",
    "# SF, SP\n",
    "SF_fig, axs = plt.subplots(2, 2, figsize=(fs[0], 2.2*fs[1]))\n",
    "SF_data = np.array(data[SF])\n",
    "SP_data = np.array(data[SP])\n",
    "for row in range(2):\n",
    "    for col in range(2):\n",
    "        ax = axs[row][col]\n",
    "        base = 3*col + 6*row\n",
    "        ax.plot(times, SF_data[base:base+3, :].transpose(), linewidth=2)\n",
    "        ax.plot(times, SP_data[base:base+3, :].transpose(), '--')\n",
    "        ax.set_title('SF, SP, units %d to %d' % (base, base+3))\n",
    "        SF_legends = ['SF '+ str(base+i) for i in range(3)]\n",
    "        SP_legends = ['SP '+ str(base+i) for i in range(3)]\n",
    "        ax.legend(SF_legends + SP_legends)\n",
    "\n",
    "plt.show()\n",
    "print('SF = ')\n",
    "print(SF_data[:,-1])\n",
    "print('SP = ')\n",
    "print(SP_data[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(x,window_len=11,window='hanning'):\n",
    "    \"\"\"smooth the data using a window with requested size.\n",
    "    \n",
    "    This method is based on the convolution of a scaled window with the signal.\n",
    "    The signal is prepared by introducing reflected copies of the signal \n",
    "    (with the window size) in both ends so that transient parts are minimized\n",
    "    in the begining and end part of the output signal.\n",
    "    \n",
    "    input:\n",
    "        x: the input signal \n",
    "        window_len: the dimension of the smoothing window; should be an odd integer\n",
    "        window: the type of window from 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'\n",
    "            flat window will produce a moving average smoothing.\n",
    "\n",
    "    output:\n",
    "        the smoothed signal\n",
    "        \n",
    "    example:\n",
    "\n",
    "    t=linspace(-2,2,0.1)\n",
    "    x=sin(t)+randn(len(t))*0.1\n",
    "    y=smooth(x)\n",
    "    \n",
    "    see also: \n",
    "    \n",
    "    numpy.hanning, numpy.hamming, numpy.bartlett, numpy.blackman, numpy.convolve\n",
    "    scipy.signal.lfilter\n",
    " \n",
    "    TODO: the window parameter could be the window itself if an array instead of a string\n",
    "    NOTE: length(output) != length(input), to correct this: return y[(window_len/2-1):-(window_len/2)] instead of just y.\n",
    "    \"\"\"\n",
    "\n",
    "    if x.ndim != 1:\n",
    "        raise ValueError(\"smooth only accepts 1 dimension arrays.\")\n",
    "    if x.size < window_len:\n",
    "        raise ValueError(\"Input vector needs to be bigger than window size.\")\n",
    "    if window_len<3:\n",
    "        return x\n",
    "    if not window in ['flat', 'hanning', 'hamming', 'bartlett', 'blackman']:\n",
    "        raise ValueError(\"Window is on of 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'\")\n",
    "    s=np.r_[x[window_len-1:0:-1],x,x[-2:-window_len-1:-1]]\n",
    "    #print(len(s))\n",
    "    if window == 'flat': #moving average\n",
    "        w=np.ones(window_len,'d')\n",
    "    else:\n",
    "        w=eval('np.'+window+'(window_len)')\n",
    "\n",
    "    y=np.convolve(w/w.sum(),s,mode='valid')\n",
    "    return y\n",
    "\n",
    "chg_fig = plt.figure(figsize=fs)\n",
    "chg_data = np.array(data[SP_CHG])[0]\n",
    "plt.plot(times, chg_data)\n",
    "plt.title('SP_CHG')\n",
    "\n",
    "act_fig = plt.figure(figsize=fs)\n",
    "act_data = np.array(data[ACT])[0]\n",
    "plt.plot(times, act_data)\n",
    "plt.plot(times, 0.8*np.ones_like(times), 'k--')\n",
    "plt.title('ACT')\n",
    "\n",
    "plant = net.plants[P]\n",
    "# modified copy-paste of plt.upd_ip_impl\n",
    "q1 = arm_activs[:,0]\n",
    "q2 = arm_activs[:,2]\n",
    "q12 = q1+q2\n",
    "c_elbow = np.array((plant.l_arm*np.cos(q1), plant.l_arm*np.sin(q1)))\n",
    "c_hand = np.array((c_elbow[0] + plant.l_farm*np.cos(q12),\n",
    "                   c_elbow[1] + plant.l_farm*np.sin(q12))).transpose()\n",
    "coord_idxs = np.floor(times/t_pres).astype(int)\n",
    "des_coords = np.array(hand_coords)[m_idxs[coord_idxs],:] # desired coordinates at each moment in time\n",
    "coords_fig = plt.figure(figsize=fs)\n",
    "plt.plot(times, c_hand)\n",
    "plt.plot(times, des_coords)\n",
    "plt.title('desired vs. actual hand coordinates')\n",
    "plt.legend(['X', 'Y', 'des_X', 'des_Y'])\n",
    "\n",
    "err_fig = plt.figure(figsize=fs)\n",
    "w_len = 8001\n",
    "hand_error = np.linalg.norm(c_hand-des_coords, axis=1)\n",
    "smooth_hand_error = smooth(hand_error, window_len=w_len)[int(np.floor(w_len/2)):-int(np.floor(w_len/2))]\n",
    "plt.plot(times, smooth_hand_error)\n",
    "plt.title('distance error')\n",
    "avg_error = hand_error.sum()/hand_error.size\n",
    "print(\"average error: %f\" % (avg_error))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all factors in the MX--C0 synaptic plasticity\n",
    "\n",
    "print('X=%d' % (X))\n",
    "#fs = (30,8)\n",
    "plastic_fig = plt.figure(figsize=fs)\n",
    "xp_data = np.array(data[xp_track[0]])\n",
    "up_data = np.array(data[up_track[0]])\n",
    "sp_now_data = np.array(data[sp_now_track[0]])\n",
    "sp_del_data = np.array(data[sp_del_track[0]])\n",
    "spj_now_data = np.array(data[spj_now_track[0]])\n",
    "spj_del_data = np.array(data[spj_del_track[0]])\n",
    "plt.plot(times, xp_data)\n",
    "plt.plot(times, up_data)\n",
    "plt.plot(times, sp_now_data)\n",
    "plt.plot(times, sp_del_data)\n",
    "plt.plot(times, spj_now_data)\n",
    "plt.plot(times, spj_del_data)\n",
    "plt.legend(['xp', 'up', 'sp_now', 'sp_del', 'spj_now', 'spj_del'])\n",
    "\n",
    "plastic_fig2 = plt.figure(figsize=fs)\n",
    "f1 = up_data - xp_data\n",
    "C2 = f1 * (sp_now_data - spj_now_data)\n",
    "C1 = f1 * (sp_del_data - spj_del_data)\n",
    "rule = 200. * (C2 - C1)\n",
    "plt.plot(times, f1)\n",
    "plt.plot(times, 100*C1)\n",
    "plt.plot(times, 100*C2)\n",
    "plt.plot(times, rule, linewidth=5)\n",
    "plt.plot(times, np.zeros(len(times)), 'k', linewidth=1)\n",
    "plt.legend(['up - xp', 'C1', 'C2', 'prod'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpha units\n",
    "fs2 =(25,5)\n",
    "AL_fig = plt.figure(figsize=fs2)\n",
    "AL_data = np.array(data[AL])\n",
    "plt.plot(times, AL_data.transpose())\n",
    "AL_legends = ['AL'+str(i) for i in range(len(AL))]\n",
    "plt.legend(AL_legends)\n",
    "plt.title('AL')\n",
    "print('AL_data:')\n",
    "print(AL_data[:,-1])\n",
    "\n",
    "# plotting muscle outputs\n",
    "#fs = (20,5)\n",
    "legs = ['Ib', 'Ia', 'II']\n",
    "\n",
    "for i in range(6):\n",
    "    next_fig = plt.figure(figsize=fs2)\n",
    "    Ib = arm_activs[:,22+i]\n",
    "    Ia = arm_activs[:,28+i]\n",
    "    II = arm_activs[:,34+i]\n",
    "    plt.plot(times, Ib, times, Ia, times, II)\n",
    "    #plt.plot(times, Ib)\n",
    "    plt.legend(legs)\n",
    "    plt.title('m' + str(i))\n",
    "    print('Ib avg for muscle '+ str(i) + '= ' + str(np.mean(Ib)))\n",
    "    print('Ia avg for muscle '+ str(i) + '= ' + str(np.mean(Ia)))\n",
    "    print('II avg for muscle '+ str(i) + '= ' + str(np.mean(II)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANALYSIS OF CORRELATIONS BETWEEN ARM AFFERENTS\n",
    "\n",
    "# obtaining arm afferents and their derivatives\n",
    "PAF = arm_activs[:10000, 22:40]\n",
    "PAF_diff = np.gradient(PAF, axis=0) / net.min_delay\n",
    "fig, axs = plt.subplots(2, 1, figsize=(20, 8))\n",
    "axs[0].plot(PAF)\n",
    "axs[0].set_title('arm afferents')\n",
    "axs[1].plot(PAF_diff)\n",
    "axs[1].set_title('derivatives for arm afferents')\n",
    "axs[1].plot(np.zeros(PAF_diff.shape[0]), 'k')\n",
    "\n",
    "# comparing some signals\n",
    "fig, axs = plt.subplots(2, 1, figsize=(20, 8))\n",
    "axs[0].plot(PAF[:,6], 'b', PAF[:,14], 'r')\n",
    "axs[1].plot(PAF[:,6], 'b', PAF[:,10], 'r')\n",
    "\n",
    "# plotting the correlation of one signal against all others\n",
    "corr_fig, corr_axs = plt.subplots(6, 3, figsize=(20,30))\n",
    "for col in range(3):\n",
    "    for row in range(6):\n",
    "        #corr = np.correlate(PAF_diff[:,6], PAF_diff[:, 6*col+row], mode='same')\n",
    "        corr = np.correlate(PAF[:,6], PAF[:, 6*col+row], mode='same')\n",
    "        corr_axs[row,col].plot(corr)\n",
    "        corr_axs[row,col].set_ylim([0,280])\n",
    "        corr_axs[row,col].set_title('PAF ' + str(6*col+row))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANALYSIS OF CORRELATIONS BETWEEN AFFERENT UNITS \n",
    "##############################\n",
    "### NON-NORMALIZED VERSION ###\n",
    "##############################\n",
    "# obtaining arm afferents and their derivatives\n",
    "AFD = data[AF,:20000].transpose()\n",
    "AFD_diff = np.gradient(AFD, axis=0) / net.min_delay\n",
    "fig, axs = plt.subplots(2, 1, figsize=(20, 8))\n",
    "axs[0].plot(AFD)\n",
    "axs[0].set_title('arm afferents')\n",
    "axs[1].plot(AFD_diff)\n",
    "axs[1].set_title('derivatives for arm afferents')\n",
    "axs[1].plot(np.zeros(AFD_diff.shape[0]), 'k')\n",
    "\n",
    "# comparing some signals\n",
    "fig, axs = plt.subplots(2, 1, figsize=(20, 8))\n",
    "axs[0].plot(AFD[:,6], 'b', AFD[:,14], 'r')\n",
    "axs[1].plot(AFD_diff[:,6], 'b', AFD_diff[:,14], 'r')\n",
    "\n",
    "# plotting the correlation of one signal against all others\n",
    "AF__M0_w = [syn.w for syn in net.syns[M[0]] if syn.preID in AF]\n",
    "corr_fig, corr_axs = plt.subplots(6, 3, figsize=(20,30))\n",
    "base_idx = 6 # index for positive Ia afferent for muscle 0\n",
    "mid = int(round(AFD.shape[0]/2.)) # middle index for the signal (where 0 is)\n",
    "span = min(2000, mid)   # span to plot for the correlation\n",
    "t = np.linspace(-span*net.min_delay, span*net.min_delay, 2*span)\n",
    "max_c = 4\n",
    "min_c = -4\n",
    "des_val_idx = mid + AF__M_syn['extra_steps']\n",
    "for col in range(3):\n",
    "    for row in range(6):\n",
    "        cur_idx = 6*col+row\n",
    "        if cur_idx == base_idx: lw = 4\n",
    "        else: lw = 1\n",
    "        corr = np.correlate(AFD_diff[:,base_idx], AFD[:,cur_idx], mode='same') # what M0 should be obtaining?\n",
    "        corr_axs[row,col].plot(t, corr[mid-span:mid+span], linewidth=lw)\n",
    "        corr_axs[row,col].set_ylim([min_c,max_c])\n",
    "        corr_axs[row,col].plot(t, 1.*AF__M0_w[cur_idx]*np.ones(len(t)), linewidth=3)\n",
    "        corr_axs[row,col].plot(t, corr[des_val_idx]*np.ones(len(t)), 'g--', linewidth=1)\n",
    "        x = t[int(len(t)/2)+AF__M_syn['extra_steps']]\n",
    "        corr_axs[row,col].plot([x, x], [min_c, max_c], 'g--', linewidth=1)\n",
    "        corr_axs[row,col].plot(t, np.zeros(len(t)), 'k--', linewidth=1)\n",
    "        corr_axs[row,col].set_title('AF ' + str(cur_idx))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting AF_diff VS AF_diff instead of AF_diff VS AF, to see if \n",
    "# the correlations are more informative\n",
    "corr_fig2, corr_axs2 = plt.subplots(6, 3, figsize=(20,30))\n",
    "#max_c = .6\n",
    "#min_c = -.6\n",
    "hspan = int(round(span/2.))\n",
    "hlen = int(round(len(AFD_diff[:,0])))\n",
    "AFD_diff_red = AFD_diff[hlen-hspan:hlen+hspan, :]\n",
    "for col in range(3):\n",
    "    for row in range(6):\n",
    "        cur_idx = 6*col+row\n",
    "        if cur_idx == base_idx: lw = 4\n",
    "        else: lw = 1\n",
    "        corr = np.correlate(AFD_diff[:,base_idx], AFD_diff_red[:,cur_idx], mode='valid')\n",
    "        corr_axs2[row,col].plot(t, corr[mid-span:mid+span], linewidth=lw)\n",
    "        #corr_axs2[row,col].set_ylim([min_c,max_c])\n",
    "        #corr_axs2[row,col].plot(t, 1.*AF__M0_w[cur_idx]*np.ones(len(t)), linewidth=3)\n",
    "        corr_axs2[row,col].plot(t, corr[des_val_idx]*np.ones(len(t)), 'g--', linewidth=1)\n",
    "        x = t[int(len(t)/2)+AF__M_syn['extra_steps']]\n",
    "        #corr_axs2[row,col].plot([x, x], [min_c, max_c], 'g--', linewidth=1)\n",
    "        corr_axs2[row,col].plot(t, np.zeros(len(t)), 'k--', linewidth=1)\n",
    "        corr_axs2[row,col].set_title('AF ' + str(cur_idx))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(x,window_len=11,window='hanning'):\n",
    "    \"\"\"smooth the data using a window with requested size.\n",
    "    \n",
    "    This method is based on the convolution of a scaled window with the signal.\n",
    "    The signal is prepared by introducing reflected copies of the signal \n",
    "    (with the window size) in both ends so that transient parts are minimized\n",
    "    in the begining and end part of the output signal.\n",
    "    \n",
    "    input:\n",
    "        x: the input signal \n",
    "        window_len: the dimension of the smoothing window; should be an odd integer\n",
    "        window: the type of window from 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'\n",
    "            flat window will produce a moving average smoothing.\n",
    "\n",
    "    output:\n",
    "        the smoothed signal\n",
    "        \n",
    "    example:\n",
    "\n",
    "    t=linspace(-2,2,0.1)\n",
    "    x=sin(t)+randn(len(t))*0.1\n",
    "    y=smooth(x)\n",
    "    \n",
    "    see also: \n",
    "    \n",
    "    numpy.hanning, numpy.hamming, numpy.bartlett, numpy.blackman, numpy.convolve\n",
    "    scipy.signal.lfilter\n",
    " \n",
    "    TODO: the window parameter could be the window itself if an array instead of a string\n",
    "    NOTE: length(output) != length(input), to correct this: return y[(window_len/2-1):-(window_len/2)] instead of just y.\n",
    "    \"\"\"\n",
    "\n",
    "    if x.ndim != 1:\n",
    "        raise ValueError(\"smooth only accepts 1 dimension arrays.\")\n",
    "    if x.size < window_len:\n",
    "        raise ValueError(\"Input vector needs to be bigger than window size.\")\n",
    "    if window_len<3:\n",
    "        return x\n",
    "    if not window in ['flat', 'hanning', 'hamming', 'bartlett', 'blackman']:\n",
    "        raise ValueError(\"Window is on of 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'\")\n",
    "    s=np.r_[x[window_len-1:0:-1],x,x[-2:-window_len-1:-1]]\n",
    "    #print(len(s))\n",
    "    if window == 'flat': #moving average\n",
    "        w=np.ones(window_len,'d')\n",
    "    else:\n",
    "        w=eval('np.'+window+'(window_len)')\n",
    "\n",
    "    y=np.convolve(w/w.sum(),s,mode='valid')\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data[M[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to interpret the plots below\n",
    "**For the first two plot grids**:  \n",
    "The value at `t=0` is the that comes from using the current `custom_inp_del(2)` values in the rga_diff rule.\n",
    "Negative `t` values correspond to delay in the M signal, which is what makes most sense in the physical implementation. The value at `t=-x` can be interpreted as the direction of change that the rga_diff learning rule would imply if both `custom_inp_del` and `custom_inp_del2` were increased by `x` time units.\n",
    "\n",
    "\n",
    "**For the third plot grid**:  \n",
    "The value at `t=0` is the value of the M-C correlation when C is delayed by `custom_inp_del2`. Values to the left imply increasing the delay in C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANALYSIS OF CORRELATIONS BETWEEN MOTOR UNITS AND C UNITS\n",
    "# Modified for the rga_diff rule\n",
    "\n",
    "# obtaining M,C unit activities and their derivatives\n",
    "full_size = len(data[M[0]])\n",
    "D_size = 100000 # number of points we will take from the data signals\n",
    "corr_size = 1000 # number of points in the correlation vector\n",
    "MD = data[M, -D_size:].transpose() # take the data from the latest points\n",
    "w_l = 31 # window length for smoothing\n",
    "for idx in range(len(M)): # smoothing MD\n",
    "    MD[:,idx] = smooth(MD[:,idx], window_len=w_l)[int(np.floor(w_l/2)):-int(np.floor(w_l/2))]\n",
    "MD_diff = np.gradient(MD, axis=0) / net.min_delay\n",
    "MD_diff_roll = np.roll(MD_diff, -CE_params['custom_inp_del']+CE_params['custom_inp_del2'], axis=0) # advancing MD_diff\n",
    "# Obtaining means over 2 seconds\n",
    "mean_win = int(2./net_params['min_delay']) # number of points in the window to extract the means\n",
    "MD_diff_means = np.zeros_like(MD_diff)\n",
    "MD_diff_roll_means = np.zeros_like(MD_diff_roll)\n",
    "for idx in range(mean_win,len(MD_diff[:,0])-1):\n",
    "        MD_diff_means[idx,:] = np.sum(MD_diff_roll[idx-mean_win:idx,:], axis=0)/mean_win\n",
    "        MD_diff_roll_means[idx,:] = np.sum(MD_diff_roll[idx-mean_win:idx,:], axis=0)/mean_win\n",
    "MD_diff_center = MD_diff_means - MD_diff # The NEGATIVE of the centered derivative\n",
    "MD_diff_roll_center = MD_diff_roll - MD_diff_roll_means\n",
    "MD_diff_center_red = MD_diff_center[:-corr_size,:]\n",
    "MD_diff_roll_center_red = MD_diff_roll_center[:-corr_size,:] # when it's second in np.correlate\n",
    "\n",
    "CD = data[CE+CI, -D_size:].transpose()\n",
    "CD_diff = np.gradient(CD, axis=0) / net.min_delay\n",
    "CD_diff_roll = np.roll(CD_diff, CE_params['custom_inp_del2'], axis=0)\n",
    "CD_diff_roll_means = np.zeros_like(CD_diff_roll)\n",
    "for idx in range(6):\n",
    "    CD_diff_roll_means[:, idx] = np.mean(np.delete(CD_diff_roll[:,0:6], idx, axis=1), axis=1)\n",
    "for idx in range(6,12):\n",
    "    CD_diff_roll_means[:, idx] = np.mean(np.delete(CD_diff_roll[:,6:12], idx-6, axis=1), axis=1)\n",
    "CD_diff_roll_center = CD_diff_roll - CD_diff_roll_means\n",
    "#CD_diff_roll_center_red = CD_diff_roll_center[:-corr_size,:]\n",
    "hlf_cspan = int(round(corr_size/2.))\n",
    "CD_diff_roll_center_red = CD_diff_roll_center[hlf_cspan:-hlf_cspan,:]\n",
    "\n",
    "# plots for the signals used in this the analysis\n",
    "plt_len=4000  # number of points to plot\n",
    "fig, axs = plt.subplots(3, 1, figsize=(30, 28))\n",
    "axs[0].plot(MD[0:plt_len,0])\n",
    "axs[0].set_title('motor unit activities')\n",
    "\n",
    "axs[1].plot(MD_diff_center[0:plt_len,0])\n",
    "axs[1].plot(MD_diff_roll_center[0:plt_len,0])\n",
    "axs[1].plot(np.zeros(plt_len), 'k')\n",
    "axs[1].set_title('centered derivatives for motor unit activities')\n",
    "#axs[1].plot(np.zeros(MD_diff_center.shape[0]), 'k')\n",
    "axs[1].set_ylim([-.1, .1])\n",
    "\n",
    "axs[2].plot(CD_diff_roll_center[0:plt_len,0])\n",
    "axs[2].plot(np.zeros(plt_len), 'k')\n",
    "#axs[2].set_ylim([-2., 2.])\n",
    "axs[2].set_title('centered derivatives for C units')\n",
    "#axs[2].plot(np.zeros(CD_diff_center.shape[0]), 'k')\n",
    "\n",
    "# plotting the expected weights for pairs of M and CE/CI signals\n",
    "M__CE_w = [[syn.w for syn in net.syns[c] if syn.preID in M] for c in CE]\n",
    "corr_figE, corr_axsE = plt.subplots(12, 6, figsize=(40,50))\n",
    "\n",
    "M__CI_w = [[syn.w for syn in net.syns[c] if syn.preID in M] for c in CI]\n",
    "corr_figI, corr_axsI = plt.subplots(12, 6, figsize=(40,50))\n",
    "\n",
    "# plotting the differential correlations for pairs of M and CE/CI signals\n",
    "corr_figX, corr_axsX = plt.subplots(6, 6, figsize=(40,20))\n",
    "\n",
    "base_idx = 0 # index for positive Ia afferent for muscle 0\n",
    "plt_span = min(300, corr_size)   # span to plot for the correlation\n",
    "#t = np.linspace(-plt_span*net.min_delay, plt_span*net.min_delay, 2*plt_span)\n",
    "                \n",
    "hlf_pspn = int(round(plt_span/2.))\n",
    "t = np.linspace(-hlf_pspn*net.min_delay, hlf_pspn*net.min_delay, plt_span)\n",
    "#pos_t = np.linspace(0., plt_span*net.min_delay, plt_span)\n",
    "#neg_t = np.linspace(-(plt_span+1)*net.min_delay, -net.min_delay, plt_span)\n",
    "#all_t = np.concatenate((neg_t,pos_t))\n",
    "eqs = np.zeros((12,12)) # matrix indicating whether weight and correlation have same sign\n",
    "corr_w = np.zeros((12,12)) # what the weight should be, according to the correlation\n",
    "des_val_idx = hlf_cspan # index in the correlation vector where the weight should be...\n",
    "                        # depends on CD_diff_diff_roll_center_red was reduced\n",
    "\n",
    "for col in range(12):  # ranges through C units\n",
    "    if col < 6:\n",
    "        axes = corr_axsE\n",
    "        weights = M__CE_w\n",
    "        color='b'\n",
    "    else:\n",
    "        axes = corr_axsI\n",
    "        weights = M__CI_w\n",
    "        color='r'\n",
    "    mcol = col%6\n",
    "    for row in range(12):  # ranges through M units\n",
    "        cur_idx = 12*mcol+row\n",
    "        if cur_idx == base_idx: lw = 4\n",
    "        else: lw = 1\n",
    "        # This corresponds to what should be learned with two different single delays\n",
    "        # In 'corr' MD is not delayed, and CD is delayed by dely2 (CE_params[custom_inp_del2])\n",
    "        corr = np.correlate(MD_diff_center[:,row], CD_diff_roll_center_red[:,col], mode='valid')\n",
    "        #corr = np.correlate(CD_diff_roll_center[:,col], MD_diff_center[:,row], mode='valid')\n",
    "        # In corr MD is additionally delayed by dely2-dely1\n",
    "        corr2 = np.correlate(MD_diff_roll_center[:,row], CD_diff_roll_center_red[:,col], mode='valid')\n",
    "        # this corresponds to what should be learned using the \"diff\" learning rule\n",
    "        corr_diff = corr - corr2\n",
    "        \n",
    "        # plot value of rga_diff learning rule\n",
    "        axes[row,mcol].plot(t, corr_diff[hlf_cspan-hlf_pspn:hlf_cspan+hlf_pspn], linewidth=lw, color=color)\n",
    "        \n",
    "        corr_w[row,col] = corr_diff[des_val_idx]\n",
    "        acw = abs(corr_w[row,col])\n",
    "        t_des = t[hlf_pspn]\n",
    "        axes[row,mcol].plot(t, max(min(10.*weights[mcol][row],acw),-acw)*np.ones(len(t)), linewidth=3)\n",
    "        axes[row,mcol].plot(t, corr_diff[des_val_idx]*np.ones(len(t)), 'g--', linewidth=1)\n",
    "        axes[row,mcol].plot([t_des, t_des], [-corr_w[row,col], corr_w[row,col]], 'g--', linewidth=1)\n",
    "        axes[row,mcol].plot(t, np.zeros(len(t)), 'k--', linewidth=1)\n",
    "        axes[row,mcol].set_title('M' + str(row) + ', C' + str(col))\n",
    "        \n",
    "        if col<6 and row<6:  # plot differential correlation\n",
    "            # Legacy code:\n",
    "            #---------------------------------------\n",
    "            # These are further values of corr when CD rather than MD is further delayed\n",
    "            # Because the first array is shorter, the arguments will be reversed (see np.convolve documentation);\n",
    "            # however, the sign of the indexes is not inverted. This means:\n",
    "            # correlate(MD_diff_center_red,CD_diff_roll_center) == correlate(CD_diff_roll_center,MD_diff_center_red)[::-1]\n",
    "            #rev_corr = np.correlate(MD_diff_center_red[:,row], CD_diff_roll_center[:,col], mode='valid')\n",
    "            #corr_axsX[row,col].plot(all_t, np.concatenate((rev_corr[-plt_span:],corr[0:plt_span])), linewidth=3, color='b')\n",
    "            #---------------------------------------\n",
    "            corr_axsX[row,col].plot(t, corr[hlf_cspan-hlf_pspn:hlf_cspan+hlf_pspn], linewidth=3, color='b')\n",
    "            corr_axsX[row,mcol].set_title('M' + str(row) + ', C' + str(col))\n",
    "            corr_axsX[row,mcol].plot(t, np.zeros(len(t)), 'k--', linewidth=1)\n",
    "            c0 = abs(corr[des_val_idx])\n",
    "            corr_axsX[row,mcol].plot([t_des, t_des], [-c0, c0], 'g--', linewidth=1)\n",
    "        \n",
    "        # Finding if weights are following the correlations\n",
    "        scat_s = 50\n",
    "        scat_c = 'r'\n",
    "        if np.sign(weights[mcol][row]) == np.sign(corr_diff[des_val_idx]):\n",
    "            eqs[row,col] = 1.\n",
    "            scat_s = 90\n",
    "            scat_c = 'g'\n",
    "        axes[row,mcol].scatter([t_des], [corr_diff[des_val_idx]], s=scat_s, c=scat_c)\n",
    "plt.show()\n",
    "\n",
    "# Regarding whether correlations and weights have the same sign\n",
    "print(eqs)\n",
    "print(np.sum(eqs, axis=0))\n",
    "print(np.sum(eqs, axis=1))\n",
    "print(sum(eqs.flatten())/(eqs.shape[0]*eqs.shape[1]))\n",
    "# Regarding desired weights according to correlations\n",
    "# normalizing columns of corr_w (weights on the same C unit)\n",
    "n_corr_w = corr_w / np.linalg.norm(corr_w, axis=0)\n",
    "import re\n",
    "print('normalized M__CE correlation weight matrix:')\n",
    "print(re.sub('\\[,', '[', re.sub('\\ +', ', ', str(n_corr_w[:,:6]))))\n",
    "print('normalized M__CI correlation weight matrix:')\n",
    "print(re.sub('\\[,', '[', re.sub('\\ +', ', ', str(n_corr_w[:,6:]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing a nice print of the AF__M weights in order to initialize them\n",
    "AF__M_w = np.zeros((len(M), len(AF)))\n",
    "for idx, mid in enumerate(M):\n",
    "    AF__M_w[idx, :] = np.array([syn.w for syn in net.syns[mid] if syn.preID in AF])\n",
    "      \n",
    "import re\n",
    "\n",
    "stryn = re.sub('\\[,', '[', re.sub('\\ +', ', ', str(AF__M_w)))\n",
    "for c,s in enumerate(re.split(', ', stryn)):\n",
    "    print(s, end=',')\n",
    "    if (c+1)%36 == 0: print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking some connections\n",
    "print(\"Connections to motor units\")\n",
    "for idx, syn in enumerate(net.syns[M[0]]):\n",
    "    pre_id = syn.preID\n",
    "    pre_type = net.units[pre_id].type\n",
    "    if pre_type is plant_models.planar_arm_v3:\n",
    "        pre_pop = 'P'\n",
    "    elif pre_id in AF:\n",
    "        pre_pop = 'AF'\n",
    "    elif pre_id in SPF:\n",
    "        pre_pop = 'SPF'\n",
    "    else:\n",
    "        pre_pop = 'erroneous'\n",
    "    if pre_pop == 'P':\n",
    "        plant_out = str(syn.plant_out)\n",
    "    else:\n",
    "        plant_out = 'None'\n",
    "    print('%d) %s (%d) --> M, w=%f, port=%d, plant_out=%s'%(idx, pre_pop, pre_id, syn.w, syn.port, plant_out))\n",
    "print(\"-------------------------\\n\")\n",
    "    \n",
    "print(\"Connections to afferent units\")\n",
    "for idx, syn in enumerate(net.syns[AF[0]]):\n",
    "    pre_id = syn.preID\n",
    "    pre_type = net.units[pre_id].type\n",
    "    if pre_type is plant_models.planar_arm_v3:\n",
    "        pre_pop = 'P'\n",
    "    elif pre_id in AF:\n",
    "        pre_pop = 'A'\n",
    "    elif pre_id in SPF:\n",
    "        pre_pop = 'SPF'\n",
    "    else:\n",
    "        pre_pop = 'other'\n",
    "    if pre_pop == 'P':\n",
    "        plant_out = str(syn.plant_out)\n",
    "    else:\n",
    "        plant_out = 'None'\n",
    "    print('%d) %s (%d) --> A, w=%f, port=%d, plant_out=%s'%(idx, pre_pop, pre_id, syn.w, syn.port, plant_out))\n",
    "print(\"-------------------------\\n\")\n",
    "    \n",
    "print(\"Connections to spinal units\")\n",
    "for idx, syn in enumerate(net.syns[CE[0]]):\n",
    "    pre_id = syn.preID\n",
    "    pre_type = net.units[pre_id].type\n",
    "    if pre_type is plant_models.planar_arm_v3:\n",
    "        pre_pop = 'P'\n",
    "    elif pre_id in AF:\n",
    "        pre_pop = 'AF'\n",
    "    elif pre_id in SPF:\n",
    "        pre_pop = 'SPF'\n",
    "    elif pre_id in M:\n",
    "        pre_pop = 'M'\n",
    "    elif pre_id in CE:\n",
    "        pre_pop = 'CE'\n",
    "    elif pre_id in CI:\n",
    "        pre_pop = 'CI'\n",
    "    else:\n",
    "        pre_pop = 'other'\n",
    "    if pre_pop == 'P':\n",
    "        plant_out = str(syn.plant_out)\n",
    "    else:\n",
    "        plant_out = 'None'\n",
    "    print('%d) %s (%d) --> CE, w=%f, port=%d, plant_out=%s'%(idx, pre_pop, pre_id, syn.w, syn.port, plant_out))\n",
    "print(\"-------------------------\\n\")\n",
    "\n",
    "for idx, syn in enumerate(net.syns[CI[0]]):\n",
    "    pre_id = syn.preID\n",
    "    pre_type = net.units[pre_id].type\n",
    "    if pre_type is plant_models.planar_arm_v3:\n",
    "        pre_pop = 'P'\n",
    "    elif pre_id in AF:\n",
    "        pre_pop = 'AF'\n",
    "    elif pre_id in SPF:\n",
    "        pre_pop = 'SPF'\n",
    "    elif pre_id in M:\n",
    "        pre_pop = 'M'\n",
    "    elif pre_id in CE:\n",
    "        pre_pop = 'CE'\n",
    "    elif pre_id in CI:\n",
    "        pre_pop = 'CI'\n",
    "    else:\n",
    "        pre_pop = 'other'\n",
    "    if pre_pop == 'P':\n",
    "        plant_out = str(syn.plant_out)\n",
    "    else:\n",
    "        plant_out = 'None'\n",
    "    print('%d) %s (%d) --> CI, w=%f, port=%d, plant_out=%s'%(idx, pre_pop, pre_id, syn.w, syn.port, plant_out))\n",
    "print(\"-------------------------\\n\")\n",
    "\n",
    "\n",
    "print(\"Connections to plant\")\n",
    "for idx, syn in enumerate(net.plants[P].inp_syns[0]):\n",
    "    pre_id = syn.preID\n",
    "    pre_type = net.units[pre_id].type\n",
    "    if pre_id in AL:\n",
    "        pre_pop = 'AL'\n",
    "    else:\n",
    "        pre_pop = 'erroneous'\n",
    "    print('%d) %s (%d, %s) --> P, w=%f'%(idx, pre_pop, pre_id, pre_type, syn.w))\n",
    "print(\"-------------------------\\n\")\n",
    "\n",
    "print(\"Connections to SF units\")\n",
    "for idx, syn in enumerate(net.syns[SF[0]]):\n",
    "    pre_id = syn.preID\n",
    "    pre_type = net.units[pre_id].type\n",
    "    if pre_id == P:\n",
    "        pre_pop = 'P'\n",
    "    elif pre_id in AF:\n",
    "        pre_pop = 'AF'\n",
    "    elif pre_id in SPF:\n",
    "        pre_pop = 'SPF'\n",
    "    elif pre_id in M:\n",
    "        pre_pop = 'M'\n",
    "    else:\n",
    "        pre_pop = 'erroneous'\n",
    "    if pre_pop == 'P':\n",
    "        plant_out = str(syn.plant_out)\n",
    "    else:\n",
    "        plant_out = 'None'\n",
    "    print('%d) %s (%d) --> SF, w=%f, port=%d, plant_out=%s'%(idx, pre_pop, pre_id, syn.w, syn.port, plant_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arm_activs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Animation of the arm and muscles\n",
    "%matplotlib widget\n",
    "from numpy import cos, sin\n",
    "from matplotlib.animation import FuncAnimation\n",
    "start_time = 0.\n",
    "start_idx = int(start_time/net.min_delay)\n",
    "\n",
    "# angles of shoulder and elbow\n",
    "theta_s = arm_activs[start_idx:,0]\n",
    "theta_e = arm_activs[start_idx:,2]\n",
    "phi = theta_s + theta_e # elbow angle wrt x axis\n",
    "# data from tracking units\n",
    "#acts = np.array(data[1])\n",
    "ipx = data[ipx_track,start_idx:]\n",
    "ipy = data[ipy_track,start_idx:]\n",
    "ten = arm_activs[start_idx:, np.array(range(4,10))].transpose()\n",
    "# coordinates of hand and elbow\n",
    "l1 = net.plants[P].l_arm\n",
    "l2 = net.plants[P].l_farm\n",
    "xe = cos(theta_s)*l1\n",
    "ye = sin(theta_s)*l1\n",
    "xh = xe + cos(phi)*l2\n",
    "yh = ye + sin(phi)*l2\n",
    "# creating the figure and axis\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.axis('equal')\n",
    "ax = plt.gca()\n",
    "lim = l1 + l2\n",
    "ax.set_xlim([-lim, lim])\n",
    "ax.set_ylim([-lim, lim])\n",
    "ax.grid()\n",
    "# creating lines and points\n",
    "line, = ax.plot([], [], 'o-b', lw=2)\n",
    "points, = ax.plot([], [], '+k')\n",
    "target, = ax.plot([], [], 'gD')\n",
    "# preparing a colormap for the tensions\n",
    "ten_max = max(ten.max(), 1e-5)\n",
    "ten_min = min(ten.min(), -1e-5)\n",
    "for row_idx, row in enumerate(ten):\n",
    "    for ent_idx, entry in enumerate(row):\n",
    "        if entry > 0:\n",
    "            ten[row_idx, ent_idx] = entry/ten_max\n",
    "        else:\n",
    "            ten[row_idx, ent_idx] = entry/abs(ten_min)\n",
    "#ten = (ten / 2.) + 0.5 # we'll have only positive tensions\n",
    "mus_lines = []\n",
    "#cmap=plt.get_cmap('Reds')\n",
    "#cmap=plt.get_cmap('coolwarm')\n",
    "cmap=plt.get_cmap('bwr')\n",
    "for i in range(6):\n",
    "    mus_lines.append(ax.plot([], [], color=cmap(0.5))[0])\n",
    "# stuff used to plot the target\n",
    "#strt_idx = int(np.round(times[0]/t_pres)) # initial index in m_idxs\n",
    "strt_idx = int(np.round((times[0]+start_time)/t_pres)) # initial index in m_idxs\n",
    "\n",
    "def init():\n",
    "    line.set_data([], [])\n",
    "    points.set_data([], [])\n",
    "    for i in range(6):\n",
    "        mus_lines = []\n",
    "        mus_lines.append(ax.plot([], [], color=cmap(0.5))[0])\n",
    "    return line, points, mus_lines\n",
    "\n",
    "def update(frame):\n",
    "    coord_x = [0, xe[frame], xh[frame]]\n",
    "    coord_y = [0, ye[frame], yh[frame]]\n",
    "    ip_x = ipx[:,frame]\n",
    "    ip_y = ipy[:,frame]\n",
    "    tens = ten[:, frame]\n",
    "    line.set_data(coord_x, coord_y)\n",
    "    points.set_data(ip_x, ip_y)\n",
    "    for i, ml in enumerate(mus_lines):\n",
    "        idx = 2*i\n",
    "        ml.set_data(ip_x[idx:idx+2], ip_y[idx:idx+2])\n",
    "        ml.set_color(cmap(tens[i]))\n",
    "    \n",
    "    cur_time = (frame+start_idx)*net.min_delay\n",
    "    fig.suptitle('time: ' + '{:f}'.format(cur_time))\n",
    "    # plotting target\n",
    "    cur_idx = int(cur_time/t_pres) + strt_idx\n",
    "    x_coord, y_coord = hand_coords[m_idxs[cur_idx]]\n",
    "    target.set_data([x_coord], [y_coord])\n",
    "    \n",
    "    return line, points, mus_lines #muscle1\n",
    "\n",
    "ani = FuncAnimation(fig, update, frames=np.arange(0, len(theta_s), 20), init_func=init, blit=True, interval=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell used to adjust parameters affecting the range of SF values\n",
    "# corresponding to the various targets when using AFi__SF connections\n",
    "\n",
    "# In the first cell we calculated the AF output for the 8 targets.\n",
    "# It is in the AF(e/i)_Ia_II arrays, which are 8x12\n",
    "# We get the input to SF from AFe_Ia_II and AFi_Ia_II\n",
    "#AF_Ia_II = AFe__SF_syn['init_w'*AFe_Ia_II + Pi__AF_ws*AFi_Ia_II\n",
    "# We reduce AF_Ia_II in three ways: maximum, minimum, and mean for all targets\n",
    "AFe_max = np.max(AFe_Ia_II, axis=0)\n",
    "AFe_min = np.min(AFe_Ia_II, axis=0)\n",
    "AFe_mean = np.mean(AFe_Ia_II, axis=0)\n",
    "AFe_std = np.std(AFe_Ia_II, axis=0)\n",
    "AFe_spread = AFe_max - AFe_min\n",
    "\n",
    "AFi_max = np.max(AFi_Ia_II, axis=0)\n",
    "AFi_min = np.min(AFi_Ia_II, axis=0)\n",
    "AFi_mean = np.mean(AFi_Ia_II, axis=0)\n",
    "AFi_std = np.std(AFi_Ia_II, axis=0)\n",
    "AFi_spread = AFi_max - AFi_min\n",
    "\n",
    "print(\"AFe_max\")\n",
    "print(AFe_max)\n",
    "print(\"AFe_min\")\n",
    "print(AFe_min)\n",
    "print(\"AFe_mean\")\n",
    "print(AFe_mean)\n",
    "print(\"AFe_spread\")\n",
    "print(AFe_spread, end=\"\\n\\n\")\n",
    "\n",
    "print(\"AFi_max\")\n",
    "print(AFi_max)\n",
    "print(\"AFi_min\")\n",
    "print(AFi_min)\n",
    "print(\"AFi_mean\")\n",
    "print(AFi_mean)\n",
    "print(\"AFi_spread\")\n",
    "print(AFi_spread, end=\"\\n\\n\")\n",
    "\n",
    "\n",
    "# We adjust the SF parameters so the minimum output is around 0.2, and the maximum is\n",
    "# around 0.9 . This can be roughly achieved if:\n",
    "# 1) AF__SF weights are inversely proportional to the max\n",
    "# 2) SF thresholds are set to zero\n",
    "# 3) The SF slope is such that SF_out(1) = 0.9, or SF_out(-1) = 0.1\n",
    "#    Doing the algebra, this leads to: slope = log(9)\n",
    "print(\"--------------------------------------------------------------\")\n",
    "print(\"Recommended AFe__SF weights: \", end=\"\\n\")\n",
    "print(1./AFe_max, end=\"\\n\\n\")\n",
    "print(\"Recommended AFi__SF weights: \", end=\"\\n\")\n",
    "print(-1./AFi_max, end=\"\\n\\n\")\n",
    "print(\"Recommended SF thresholds: \")\n",
    "print(0., end=\"\\n\\n\")\n",
    "print(\"Recommended SF slopes: \")\n",
    "print(np.log(9.), end=\"\\n\\n\")\n",
    "\n",
    "# A different approach is to have the mean afferent value produce an SF output around 0.5,\n",
    "# and values one standard deviation away produce values of 0.8 or 0.2, respectively.\n",
    "# This can be achieved by using the summed AFe+AFi signal to produce AF_sum, and: \n",
    "# 1) AFe__SF weights are set as w=1/AFsum_std, AFi__SF as w=-1/AFsum_std. \n",
    "# 2) Thresholds are set to the mean of AFsum\n",
    "# 3) Slopes are set so SF_out(1) = 0.8. This works out to:\n",
    "#    slope = (log(0.8)-log(0.2))/(1 - (mean/std)) = log(4)/(1 - thr)\n",
    "# This assumes (wrongly) that the std of the combined signal is like the std of the\n",
    "# individual signals, but let's test it.\n",
    "\n",
    "AFsum = AFe_Ia_II/AFe_std - AFi_Ia_II/AFi_std\n",
    "AFsum_mean = np.mean(AFsum, axis=0)\n",
    "AFsum_std = np.std(AFsum, axis=0)\n",
    "\n",
    "print(\"--------------------------------------------------------------\")\n",
    "print(\"Recommended AFe__SF weights: \", end=\"\\n\")\n",
    "AFe_w = 1./AFsum_std\n",
    "for w in AFe_w:\n",
    "    print(\"{:1.2f}\".format(w) + ',', end=' ')\n",
    "print('\\b\\b]', end=\"\\n\\n\")\n",
    "print(\"Recommended AFi__SF weights: \", end=\"\\n\")\n",
    "AFi_w = -1./AFsum_std\n",
    "for w in AFi_w:\n",
    "    print(\"{:1.2f}\".format(w) + ',', end=' ')\n",
    "print('\\b\\b]', end=\"\\n\\n\")\n",
    "print(\"Recommended SF thresholds: \")\n",
    "thr = AFsum_mean/AFsum_std\n",
    "for t in thr:\n",
    "    print(\"{:1.2f}\".format(t) + ',', end=' ')\n",
    "print('\\b\\b]', end=\"\\n\\n\")\n",
    "print(\"Recommended SF slopes: \")\n",
    "slopes = np.log(4.) / (1. - thr)\n",
    "for s in slopes:\n",
    "    print(\"{:1.2f}\".format(s) + ',', end=' ')\n",
    "print('\\b\\b]', end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell used to adjust parameters affecting the range of SF values\n",
    "# corresponding to the various targets\n",
    "\n",
    "min_len = np.min(m_lengths, axis=0) # m_lengths, etc. calclulated in the first cell\n",
    "max_len = np.max(m_lengths, axis=0)\n",
    "mean_lens = np.mean(m_lengths, axis=0)\n",
    "#print('length for muscle %d: min=%f, max=%f, mean=%f' % (muscl, min_len, max_len, mean_len))\n",
    "print('length spreads: ')\n",
    "print(max_len - min_len)\n",
    "\n",
    "# These lengths are used to obtain steady state tensions assuming zero velocity and\n",
    "# no gamma inputs. The steady state tensions are then used to produce steady state\n",
    "# Ia and II outputs Ia_ss and II_ss.\n",
    "\n",
    "# Using Ia_ss and II_ss the steady state output of the AF population is obtained.\n",
    "# This output comes from Ia and II minus their average values. We assume that these\n",
    "# values change slowly relative to the change in the target, or otherwise they would\n",
    "# approach the steady state of Ia and II for the present target. Moreover, we assume\n",
    "# that SP knows these averages.\n",
    "\n",
    "# We recommend weigths so the AF output for the mean target length is around 0.5\n",
    "par = net.plants[P].m_params\n",
    "Ts_ss_ml = (par['k_se_s']/(par['k_se_s']+par['k_pe_s'])) * (\n",
    "            par['k_pe_s']*(mean_lens - par['l0_s']))\n",
    "Td_ss_ml = (par['k_se_d']/(par['k_se_d']+par['k_pe_d'])) * (\n",
    "            par['k_pe_d']*(mean_lens - par['l0_d']))\n",
    "Ia_ss_ml = par['fs']*(Ts_ss_ml/par['k_se_s']) + (1.-par['fs'])*(Td_ss_ml/par['k_se_d'])\n",
    "II_ss_ml = par['se_II']*(Ts_ss_ml/par['k_se_s']) + ((1.-par['se_II'])/par['k_pe_s'])*Ts_ss_ml\n",
    "Ia_ss_ml *= par['Ia_gain']\n",
    "II_ss_ml *= par['II_gain']\n",
    "AF_ml = np.concatenate((Ia_ss_ml, II_ss_ml), axis=0)\n",
    "# Next transformation is through the chwr_linear afferent units\n",
    "#AF_Ia = np.maximum(Ia_ss*Pe__AF_Ia_ws - AF_avgs[0:6] - AF_thr[0:6], 0.)\n",
    "#AF_II = np.maximum(II_ss*Pe__AF_II_ws - AF_avgs[6:12] - AF_thr[6:12], 0.)\n",
    "# 0.5 = (Ia_ss_ml - AF_avgs[muscl])*w_Ia - AF_thr[muscl]\n",
    "# 0.5 = (II_ss_ml - AF_avgs[6+muscl])*w_II - AF_thr[6+muscl]\n",
    "w = (0.5 + AF_thr)/(AF_ml - AF_avgs)\n",
    "print(\"Recommended P__AF_syn['init_w']:\")\n",
    "print(w)\n",
    "\n",
    "# Next we want the  mean_aff value to produce sp output of roughly 0.5,\n",
    "# and we want an sp spread of roughly 0.6 .\n",
    "# This implies SF_params['thresh'] ~= 0.5*AF__SF_syn['init_w'], \n",
    "# and a SF_params['slope'] = s such that\n",
    "# sig(max_aff) - sig(min_aff) = 0.6, so\n",
    "# 1/(1+exp(-s*(max_aff-thr))) - 1/(1+exp(-s(min_aff-thr))) = 0.6\n",
    "\n",
    "# For the second criterion min_aff is always 0, but mean_aff changes\n",
    "# SF_params['thresh'] = AF__SF_syn['init_w']*mean_aff\n",
    "# 1/(1+exp(-slope*(max_aff-thr))) - 1/(1+exp(slope*thr)) = 0.6\n",
    "\n",
    "#sp_vals = sf_preds[:,muscl]\n",
    "#min_sp = min(sp_vals)\n",
    "#max_sp = max(sp_vals)\n",
    "#mean_sp = np.mean(sp_vals)\n",
    "#print('SP for muscle %d: min=%f, max=%f, mean=%f' % (muscl, min_sp, max_sp, mean_sp))\n",
    "#print('sp spread = %f' %(max_sp - min_sp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
