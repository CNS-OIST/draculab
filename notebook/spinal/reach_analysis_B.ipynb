{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reach_analysis_B.ipynb\n",
    "\n",
    "Parallel version of reach_analysis_A.\n",
    "\n",
    "Learn to reach, perform radial reaches, and do the analyses that concern the characteristics of the reach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/z/projects/draculab\n"
     ]
    }
   ],
   "source": [
    "%cd /home/z/projects/draculab/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from draculab import *\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "#from tools.visualization import plotter\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/z/projects/draculab/notebook/spinal\n",
      "/home/z/projects/draculab\n"
     ]
    }
   ],
   "source": [
    "%cd /home/z/projects/draculab/notebook/spinal/\n",
    "from v3ft3p2ph2_net import *\n",
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load parameter configurations\n",
    "import pickle\n",
    "fname = '/home/z/Dropbox (OIST)/saves/gene_2021-06-17'\n",
    "with (open(fname, \"rb\")) as f:\n",
    "    pop_gene17 = pickle.load(f)\n",
    "    f.close()\n",
    "# fname = '/home/z/Dropbox (OIST)/saves/pso_2021-06-17'\n",
    "# with (open(fname, \"rb\")) as f:\n",
    "#     pop_pso17 = pickle.load(f)\n",
    "#     f.close()\n",
    "fname = '/home/z/Dropbox (OIST)/saves/gene_2021-06-24'\n",
    "with (open(fname, \"rb\")) as f:\n",
    "    pop_gene24 = pickle.load(f)\n",
    "    f.close()\n",
    "fname = '/home/z/Dropbox (OIST)/saves/pso_2021-06-24'\n",
    "with (open(fname, \"rb\")) as f:\n",
    "    pop_pso24 = pickle.load(f)\n",
    "    f.close()\n",
    "fname = '/home/z/Dropbox (OIST)/saves/pso_2021-06-28'\n",
    "with (open(fname, \"rb\")) as f:\n",
    "    pop_pso28 = pickle.load(f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configurations for the simulations\n",
    "#np.random.seed(123456) # always the same random values\n",
    "np.random.seed() # arbitrary random seed\n",
    "\n",
    "cfg_list = [pop_gene17[0], pop_gene17[1],\n",
    "            pop_gene24[0], pop_gene24[1],\n",
    "            pop_pso24[0], pop_pso28[0]]\n",
    "\n",
    "basic_dict = {'n_reaches_l':16,\n",
    "              'use_syne':False,\n",
    "              'set_C_delay':False,\n",
    "              'rand_targets':True,\n",
    "              'C_noise':True,\n",
    "              'M__C_rand_w':True,\n",
    "              'permute_targets':True,\n",
    "              'new_noise':0.,\n",
    "              'new_gain':1.,\n",
    "              't_pres':5.,\n",
    "              'n_rounds':6,\n",
    "              'r':0.1,\n",
    "              't_smp':1.}\n",
    "\n",
    "new_gain_vals = [1., 1.2, 1.5]\n",
    "new_noise_vals = [0., .1, .3]\n",
    "r_vals = [0.1, 0.12]\n",
    "n_configs1 = len(new_gain_vals)*len(new_noise_vals)*len(r_vals)\n",
    "varis = []\n",
    "\n",
    "for new_gain in new_gain_vals:\n",
    "    for new_noise in new_noise_vals:\n",
    "        for r in r_vals:\n",
    "            conf = basic_dict.copy()\n",
    "            conf['new_gain'] = new_gain\n",
    "            conf['new_noise'] = new_noise\n",
    "            conf['r'] = r\n",
    "            varis.append(conf)\n",
    "            \n",
    "arg_dicts = []\n",
    "for cfg in cfg_list:\n",
    "    for var_dict in varis:\n",
    "        cfgd = {'cfg':cfg}\n",
    "        cfgd.update(var_dict)\n",
    "        arg_dicts.append(cfgd.copy())\n",
    "        cfgd['use_syne'] = True\n",
    "        arg_dicts.append(cfgd)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_new_targets(net,\n",
    "                    pops_dict,\n",
    "                    r,\n",
    "                    t_pres,\n",
    "                    n_trgs,\n",
    "                    n_rounds,\n",
    "                    pds,\n",
    "                    permute_targets=True):\n",
    "    \"\"\" Set new targets for reaching.\n",
    "        \n",
    "        Args:\n",
    "            net: the draculab network\n",
    "            pops_dict: dictionary with unit IDs for each population\n",
    "            r: distance from center to targets \n",
    "            t_pres: new presentation time\n",
    "            n_trgs: number of targets\n",
    "            n_rounds: number of times the n_trgs targets will be presented\n",
    "            pds : parameter dictionaries used to calculate SP values.\n",
    "            permute_targets: (boolean) are targets presented permuted?\n",
    "            \n",
    "        Returns:\n",
    "            hand_coords : list with target coordinates (each a numpy 2-array).\n",
    "            targets : list, only with targets\n",
    "            center : only the center coordinates\n",
    "            trg_ids: target used for each presentation\n",
    "            m_idxs: indexes in 'hand_coords' for the sequence of targets\n",
    "    \"\"\"\n",
    "    \n",
    "    start_t = net.sim_time # starting time for new simulation\n",
    "    # 8 radial targets in sequence, from 0 to 315 degrees\n",
    "    center = np.array([0.3, 0.3]) # initial hand location\n",
    "    angs = np.linspace(0., 2.*np.pi, n_trgs+1)[:-1]\n",
    "    circle = np.array([np.array([np.cos(ang),np.sin(ang)]) for ang in angs])\n",
    "    targets = center + r*circle # coordinates of the targets\n",
    "\n",
    "    if permute_targets:\n",
    "        # version with permuted targets, all are seen every 8 presentations\n",
    "        trg_ids = np.random.permutation(n_trgs*n_rounds)%8 # target for each presentation\n",
    "        hand_coords = []\n",
    "        for idx in trg_ids:\n",
    "            hand_coords += [center, targets[idx]]\n",
    "        hand_coords += [center] # to avoid indexes out of bounds\n",
    "    else:    \n",
    "        # version with sequential targets\n",
    "        hand_coords = [center, targets[0],\n",
    "                       center, targets[1],\n",
    "                       center, targets[2],\n",
    "                       center, targets[3],\n",
    "                       center, targets[4],\n",
    "                       center, targets[5],\n",
    "                       center, targets[6],\n",
    "                       center, targets[7]]\n",
    "        hand_coords = n_rounds * hand_coords # many repetitions of the same sequence\n",
    "        hand_coords += [center] # to avoid indexes out of bounds\n",
    "        trg_ids = np.arange(len(hand_coords))%8 # target for each presentation\n",
    "\n",
    "    SP = pops_dict['SP']\n",
    "    A = pops_dict['A']\n",
    "    P = pops_dict['P']\n",
    "    #### next is a copy-pasta of the code to set the SP values\n",
    "    # list with muscle lengths corresponding to the hand coordinates\n",
    "    m_lengths = []\n",
    "    for coord in hand_coords:\n",
    "        m_lengths.append(net.plants[P].coords_to_lengths(coord))\n",
    "    m_lengths = np.array(m_lengths)\n",
    "    #(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)\n",
    "    # We need to translate these lengths to corresponding SF activity levels.\n",
    "    # For that it is necessary to recreate all their transformations\n",
    "    # The first transformation is from length to II afferent activity.\n",
    "    ### OUT OF THE 36 AFFERENT SIGNALS, WE TAKE II ###\n",
    "    par = net.plants[P].m_params\n",
    "    # steady state tensions in the static and dynamic bag fibers (no gamma inputs)\n",
    "    Ts_ss = (par['k_se_s']/(par['k_se_s']+par['k_pe_s'])) * (\n",
    "             par['k_pe_s']*(m_lengths - par['l0_s']))\n",
    "    Td_ss = (par['k_se_d']/(par['k_se_d']+par['k_pe_d'])) * (\n",
    "             par['k_pe_d']*(m_lengths - par['l0_d']))\n",
    "    # steady state afferent outputs (no gamma inputs)\n",
    "    #Ia_ss = par['fs']*(Ts_ss/par['k_se_s']) + (1.-par['fs'])*(Td_ss/par['k_se_d'])\n",
    "    II_ss = par['se_II']*(Ts_ss/par['k_se_s']) + ((1.-par['se_II'])/par['k_pe_s'])*Ts_ss\n",
    "    #Ia_ss *= par['Ia_gain']\n",
    "    II_ss *= par['II_gain']\n",
    "    #Ia_II_ss = np.concatenate((Ia_ss, II_ss), axis=1)\n",
    "    # Next transformation is through the afferent units\n",
    "    P__A_ws = np.array(pds['P__A_syn']['init_w'][12:18])\n",
    "    #Ia_II_avgs = np.mean(Ia_II_ss, axis=0)  # when using hundreds of random targets\n",
    "    # target averages\n",
    "    A_thr = np.array([net.units[u].thresh for u in A[12:18]])\n",
    "    A_II = np.log(1. + np.maximum((II_ss)*P__A_ws - A_thr, 0.))\n",
    "    #(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)\n",
    "    # Next is from A to SF\n",
    "    SF_arg = pds['A__SF_syn']['init_w']*A_II\n",
    "    SF_params = pds['SF_params']\n",
    "    SF_out = 1./ (1. + np.exp(-SF_params['slope']*(SF_arg - SF_params['thresh'])))\n",
    "    SF_params['init_val'] = SF_out # this might cause a smooth start\n",
    "    # now we set the values in SP\n",
    "    #m_idxs = np.random.randint(len(hand_coords), size=1000) # index of all targets\n",
    "    m_idxs = list(range(len(hand_coords)+1)) # reach list targets sequentially\n",
    "        #m_idxs[0] = 0 # for testing\n",
    "    A_us = [net.units[u] for u in A]\n",
    "\n",
    "    def SF_sigmo(idx, arg):\n",
    "        \"\"\" The sigmoidal function for SF unit with index SF[idx]. \"\"\"\n",
    "        return 1./ (1. + np.exp(-SF_params['slope'][idx]*(arg - SF_params['thresh'][idx])))\n",
    "\n",
    "    def cur_target(t):\n",
    "        \"\"\" Returns the index of the target at time t. \"\"\"\n",
    "        return m_idxs[int(np.floor((t-start_t)/t_pres))]\n",
    "\n",
    "    def make_fun(idx):\n",
    "        \"\"\" create a function for the SP unit with index 'idx'. \"\"\"\n",
    "        return lambda t: SF_sigmo(idx, \n",
    "                            pds['A__SF_syn']['init_w'][idx] * (\n",
    "                            np.log(1. + max(II_ss[cur_target(t)][idx] * P__A_ws[idx] - \n",
    "                            net.units[A[12+idx]].thresh, 0.))))\n",
    "\n",
    "    for idx, u in enumerate(SP):\n",
    "        net.units[u].set_function(make_fun(idx))\n",
    "        \n",
    "    return hand_coords, targets, center, trg_ids, m_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to arrange the multiunit data from the various reaches\n",
    "def get_tensor(series, J, K, T, trg_ids):\n",
    "    \"\"\" Given a simulation time series, arrange it by reach and repetition.\n",
    "    \n",
    "        Args:\n",
    "            series : a time series, spanning simtime2 seconds, in min_delay steps.\n",
    "            J : number of targets in the simulation.\n",
    "            K : number of repetitions of each target.\n",
    "            T : number of time steps that each target is presented.\n",
    "            \n",
    "        Returns:\n",
    "            tensor : a 3-dimensional array. tensor(j,k,t) is the value of the series\n",
    "                     for target j, on the k-th repetition, at time step t. Time steps\n",
    "                     begin anew for each reach.\n",
    "    \"\"\"\n",
    "    tensor = np.zeros((J, K, T))\n",
    "    trg_reps = np.zeros(J, dtype=int) # how many repetitions of each target we have filled\n",
    "\n",
    "    for reach, trg in enumerate(trg_ids):\n",
    "        init_tid = (1 + 2 * reach) * T\n",
    "        final_tid = init_tid + T # t_pres seconds later\n",
    "        tensor[trg, trg_reps[trg],:] = series[init_tid:final_tid]\n",
    "        trg_reps[trg] += 1\n",
    "        \n",
    "    return tensor\n",
    "\n",
    "def get_pop_tensor(pop, J, K, T, t_strt, t_pres, trg_ids, data, md):\n",
    "    \"\"\" Returns a tensor with the activities in a population arranged by target and repetition.\n",
    "    \n",
    "        Args:\n",
    "            pop : a list with the IDs of the populations units in the 'data' matrix.\n",
    "            J : number of targets in the simulation.\n",
    "            K : number of repetitions of each target.\n",
    "            T : number of time points to sample per reach (min_delay units)\n",
    "            t_pres : time that a full reach lasts (seconds)\n",
    "            t_strt : time after target onset when sample begins (seconds)\n",
    "            trg_ids : trg_ids[i] is the target at the i-th reach\n",
    "            data: the simulation data (unit activities)\n",
    "            md : network time step (e.g. net.min_delay)\n",
    "        Returns:\n",
    "            tensor : a 4-dimensional array. tensor(i,j,k,t) is the activity of the i-th\n",
    "                     unit, when reaching target j, on the k-th repetition, at time step t.\n",
    "    \"\"\"\n",
    "    idx_strt = int(t_strt / md)\n",
    "    I = len(pop)\n",
    "    tensor = np.zeros((I,J,K,T), dtype=np.float_)\n",
    "\n",
    "    trg_reps = np.zeros(J, dtype=int) # how many repetitions of each target we have filled\n",
    "    pt_per_pres = int(round(t_pres / md))\n",
    "    for reach, trg in enumerate(trg_ids):\n",
    "        init_tid = (1 + 2 * reach) * pt_per_pres + idx_strt\n",
    "        final_tid = init_tid + T\n",
    "        tensor[:, trg, trg_reps[trg],:] = data[pop, init_tid:final_tid]\n",
    "        trg_reps[trg] += 1\n",
    "        \n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Circular dynamics analyisis in a function\n",
    "def jpca_ana(pop, t_smp, t_strt, t_pres, J, K, trg_ids, data, md, wid=0.4, normalize=True):\n",
    "    \"\"\" Returns some measures relate to the level of circular dynamics.\n",
    "    \n",
    "        Args:\n",
    "            pop: IDs of the units to be analyzed.\n",
    "            t_smp: how many seconds will be included per reach.\n",
    "            t_strt: time after target presentation when sample begins.\n",
    "            t_pres: duration of a full reach (seconds)\n",
    "            J: number of targets.\n",
    "            K: number of presentations per target.\n",
    "            trg_ids: IDs of the targets presented in radial reaching\n",
    "            data: the simulation data (unit activites)\n",
    "            md : network time step (e.g. net.min_delay)\n",
    "            wid: width of window around pi/2 where angles are \"orthogonal\".\n",
    "            normalize: if True, the population responses are normalized.\n",
    "        Returns:\n",
    "            A dictionary with the following entries:\n",
    "            'var_per':array with variance percentages for the 3 jPCA planes.\n",
    "            'orth_fr': fraction of \"orthogonal\" angles between X, Xp\n",
    "            'R2_uncons': coefficent of determination for unconstrained model\n",
    "            'R2_skew:: Coef. of Det. for skew-symmetric model.\n",
    "            't_smp': the t_smp argument.\n",
    "            't_strt': the t_strt argument.\n",
    "    \"\"\"\n",
    "    idx_strt = int(t_strt / md)\n",
    "    I = len(pop)\n",
    "    T = int(t_smp / md) # number of time points\n",
    "    m_tensor = get_pop_tensor(pop, J, K, T, t_strt, t_pres, trg_ids, data, md)\n",
    "    \n",
    "    # Normalize responses in m_tensor\n",
    "    if normalize:\n",
    "        for i in range(I):\n",
    "            for j in range(J):\n",
    "                for k in range(K):\n",
    "                    norm = np.linalg.norm(m_tensor[i,j,k,:])\n",
    "                    m_tensor[i,j,k,:] = m_tensor[i,j,k,:] / norm\n",
    "                \n",
    "    # Now we average across repetitions for the same target\n",
    "    p_tensor = m_tensor.sum(axis=2) / K\n",
    "\n",
    "    # We obtain the across-condition average\n",
    "    a_tensor = p_tensor.sum(axis=1) / J\n",
    "\n",
    "    # Obtained a normalized average trace per condition\n",
    "    c_tensor = np.zeros_like(p_tensor)\n",
    "    for trg in range(J):\n",
    "        c_tensor[:, trg, :] = p_tensor[:, trg, :] - a_tensor\n",
    "    X = np.zeros((J*T,I))\n",
    "    for j in range(J):\n",
    "        X[j*T:(j+1)*T, :] = c_tensor[:,j,:].transpose()\n",
    "    # create the block-matrix version of X\n",
    "    Xtilde = sp.linalg.block_diag(*([X]*I))\n",
    "    # Create the H matrix\n",
    "    n = X.shape[1]\n",
    "    ct = X.shape[0]\n",
    "    L = np.zeros((n,n), dtype=int)\n",
    "    c = 0\n",
    "    for row in range(n):\n",
    "        for col in range(row+1, n):\n",
    "            L[row, col] = c\n",
    "            L[col, row] = c\n",
    "            c += 1\n",
    "    H = np.zeros((n*n, int(0.5*n*(n-1))))\n",
    "    for col in range(n):\n",
    "        for row in range(n):\n",
    "            if col > row:\n",
    "                H[n*col+row, L[col,row]] = 1.\n",
    "            elif row > col:\n",
    "                H[n*col+row, L[col,row]] = -1.\n",
    "    # Approximate the derivatives of X\n",
    "    Xp = np.zeros_like(X)\n",
    "    Xp[1:,:] = (X[1:,:] - X[:-1,:]) / md\n",
    "    xp = Xp.flatten('F')\n",
    "    kstar = np.matmul(np.linalg.pinv(np.matmul(Xtilde, H)), xp)\n",
    "    # reconstruct the matrix that generated the data\n",
    "    Mstar = np.matmul(H, kstar).reshape(n,n)\n",
    "    # Next,extract the eigenvalues of M.\n",
    "    eig_vals, eig_vecs = np.linalg.eig(Mstar)\n",
    "    eig_vals_norms = np.sqrt((eig_vals * eig_vals.conj()).real)\n",
    "    ev_sum = eig_vals_norms.sum()/2.1\n",
    "    var_percentages = eig_vals_norms[np.array([0,2,4])] / ev_sum\n",
    "    \n",
    "    # 1) average angle between X and Xp\n",
    "    X_norms = np.linalg.norm(X, axis=1)\n",
    "    Xp_norms = np.linalg.norm(Xp, axis=1)\n",
    "    X_cos = (X * Xp).sum(axis=1) / (X_norms * Xp_norms + 1e-10)\n",
    "    X_angs = np.arccos(X_cos)\n",
    "\n",
    "    orth_fr = ((X_angs < np.pi/2+wid) & (X_angs > np.pi/2-wid)).sum() / len(X_angs)\n",
    "\n",
    "    # 2) Coefficients of determination\n",
    "    # 2.1) Obtain unconstrained M matrix\n",
    "    M_uncons = np.matmul(np.linalg.pinv(X), Xp)\n",
    "    # 2.2) Reconstruct Xp with M_uncons\n",
    "    Xp_uncons = np.matmul(X, M_uncons)\n",
    "    # 2.3) Reconstruct Xp with Mstar\n",
    "    Xp_skew = np.matmul(X, Mstar)\n",
    "    # 2.4) Calculate residual sums of squares\n",
    "    SSres_uncons = ((Xp - Xp_uncons) * (Xp - Xp_uncons)).sum()\n",
    "    SSres_skew = ((Xp - Xp_skew) * (Xp - Xp_skew)).sum()\n",
    "    # 2.5) Calculate the total sum of squares\n",
    "    SStot = ((Xp-Xp.mean())*(Xp-Xp.mean())).sum()\n",
    "    # 2.6) Calculate coefficients of determination\n",
    "    R2_uncons = 1. - (SSres_uncons / SStot)\n",
    "    R2_skew = 1. - (SSres_skew / SStot)\n",
    "    \n",
    "    return {'var_per': var_percentages,\n",
    "            'orth_fr': orth_fr,\n",
    "            'R2_uncons': R2_uncons,\n",
    "            'R2_skew': R2_skew,\n",
    "            't_smp': t_smp,\n",
    "            't_strt': t_strt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reach_ana(arg_dict):\n",
    "    \"\"\" Given a parameter configuration, analyze the reacing it produces.\n",
    "    \n",
    "        Args:\n",
    "        arg_dict: a dictionary with these entries\n",
    "        cfg: parameter dictionary\n",
    "        n_reaches_l: number of targets presented for initial learning\n",
    "        use_syne: whether to use the version of the network with \"synergies\"\n",
    "        set_C_delay: whether set C_cid using analytical approach\n",
    "        rand_targets: whether to train using a large number of random targets\n",
    "        C_noise: whether C units are noisy (use euler_maru integrator)\n",
    "        M__C_rand_w: whether to randomly intialize weights for the M__C connections\n",
    "        permute_targets: whether to permut target order during radial reaches\n",
    "        new_noise: C unit sigma value during radial reaching.\n",
    "        new_gain: muscle gain multiplier during radial reaching.\n",
    "        t_pres: target presentation time during radial reaching.\n",
    "        n_rounds: number of times to present each radial target.\n",
    "        r: distance of radial targets from center.\n",
    "        t_smp: time to sample M for preferred directions (seconds)\n",
    "\n",
    "        Returns:\n",
    "        A dictionary with these entries:\n",
    "        error_l: mean distance to target during the learning phase.\n",
    "        error_r: mean distance to target during radial reaches.\n",
    "        cs_of_d: Coeffs. of determination for M direction tuning.\n",
    "        n_tuned: Number of M units with significant direction tuning.\n",
    "        pred_frac: Fraction of reaches predicted with M preferred directions.\n",
    "        bimodal: whether the big PD vectors have a bimodal distribution (Boolean)\n",
    "        max_var_per: maximum variance percentage in first jPCA plane\n",
    "        max_orth_fr: maximum fraction of orthogonal angles between X and Xp\n",
    "        max_vp_of: maximum of the product of max_var_per*max_orth_fr\n",
    "\n",
    "    \"\"\"\n",
    "    cfg = arg_dict['cfg']\n",
    "    n_reaches_l=arg_dict['n_reaches_l']\n",
    "    use_syne=arg_dict['use_syne']\n",
    "    set_C_delay=arg_dict['set_C_delay']\n",
    "    rand_targets=arg_dict['rand_targets']\n",
    "    C_noise=arg_dict['C_noise']\n",
    "    M__C_rand_w=arg_dict['M__C_rand_w']\n",
    "    permute_targets=arg_dict['permute_targets']\n",
    "    new_noise=arg_dict['new_noise']\n",
    "    new_gain=arg_dict['new_gain']\n",
    "    t_pres=arg_dict['t_pres']\n",
    "    n_rounds=arg_dict['n_rounds']\n",
    "    r=arg_dict['r']\n",
    "    t_smp=arg_dict['t_smp']\n",
    "    \n",
    "    t_pres_l = cfg['t_pres'] # target presentation time during initial learning\n",
    "    par_heter = cfg['par_heter'] # level of parameter heterogeneity\n",
    "    M_size = 12 # number of units in the M population\n",
    "    SPF_size = 12 # number of units in the SPF population\n",
    "    n_trgs = 8 # number of targets in radial reaching\n",
    "\n",
    "    \n",
    "    t_pres_l = cfg['t_pres'] \n",
    "    par_heter = cfg['par_heter']\n",
    "\n",
    "    if not use_syne:\n",
    "        net, pops_dict, hand_coords, m_idxs, pds = net_from_cfg(cfg,\n",
    "                                                           t_pres = t_pres_l,\n",
    "                                                           par_heter = par_heter,\n",
    "                                                           set_C_delay = set_C_delay,\n",
    "                                                           rand_targets = rand_targets,\n",
    "                                                           track_weights = False,\n",
    "                                                           track_ips = False,\n",
    "                                                           C_noise = C_noise,\n",
    "                                                           M__C_rand_w = M__C_rand_w)\n",
    "    else:\n",
    "        net, pops_dict, hand_coords, m_idxs, pds = syne_net(cfg,\n",
    "                                                       t_pres = t_pres,\n",
    "                                                       par_heter = par_heter,\n",
    "                                                       set_C_delay = set_C_delay,\n",
    "                                                       rand_targets = rand_targets,\n",
    "                                                       track_weights = False,\n",
    "                                                       track_ips = False,\n",
    "                                                       C_noise = C_noise,\n",
    "                                                       M__C_rand_w = M__C_rand_w)\n",
    "\n",
    "#     for name in pops_dict:\n",
    "#         exec(\"%s = %s\"% (name, str(pops_dict[name])))\n",
    "    M = pops_dict['M']\n",
    "    P = pops_dict['P']\n",
    "    if use_syne:\n",
    "        SYNE = pops_dict['SYNE']\n",
    "        SYNI = pops_dict['SYNI']\n",
    "    else:\n",
    "        CE = pops_dict['CE']\n",
    "        CI = pops_dict['CI']\n",
    "    \n",
    "    # paramter dictionaries used to change targets\n",
    "    P__A_syn = pds['P__A_syn']\n",
    "    P__A_ws = np.array(P__A_syn['init_w'][12:18])\n",
    "    A__SF_syn = pds['A__SF_syn']\n",
    "    SF_params = pds['SF_params']\n",
    "    \n",
    "    # Run simulation for initial learning\n",
    "    sim_time_l = t_pres_l * n_reaches_l\n",
    "    times_l, data_l, plant_data_l  = net.flat_run(sim_time_l)\n",
    "    data_l = np.array(data_l)\n",
    "    \n",
    "    # obtain error for initial learning\n",
    "    plant = net.plants[P]\n",
    "    q1 = plant_data_l[P][:,0]\n",
    "    q2 = plant_data_l[P][:,2]\n",
    "    q12 = q1+q2\n",
    "    c_elbow = np.array((plant.l_arm*np.cos(q1), plant.l_arm*np.sin(q1)))\n",
    "    c_hand = np.array((c_elbow[0] + plant.l_farm*np.cos(q12),\n",
    "                       c_elbow[1] + plant.l_farm*np.sin(q12))).transpose()\n",
    "    coord_idxs = np.floor(times_l/t_pres_l).astype(int)  # before resetting the targets\n",
    "    # desired coordinates at each moment in time:\n",
    "    des_coords = np.array(hand_coords)[m_idxs[coord_idxs],:] \n",
    "    hand_error = np.linalg.norm(c_hand-des_coords, axis=1)\n",
    "    error_l = hand_error.sum()/hand_error.size\n",
    "    \n",
    "    # Set targets for radial reaching\n",
    "    start_t = net.sim_time # starting time for new simulation\n",
    "    hand_coords, targets, center, trg_ids, m_idxs = set_new_targets(\n",
    "        net, pops_dict, r, t_pres, n_trgs, n_rounds, pds)\n",
    "    \n",
    "    # adjust noise from C/SYNE units\n",
    "    if use_syne:\n",
    "        for u in [net.units[c] for c in SYNE+SYNI]:\n",
    "            u.sigma = new_noise\n",
    "    else:\n",
    "        for u in [net.units[c] for c in CE+CI]:\n",
    "            u.sigma = new_noise\n",
    "\n",
    "    # adjust the muscle gains\n",
    "    for idx in range(6):\n",
    "        net.plants[0].inp_syns[idx][0].w = new_gain\n",
    "        \n",
    "    # Run simulation for radial reaches\n",
    "    sim_time2 = 2 * t_pres * n_trgs * n_rounds\n",
    "    times, data, plant_data  = net.flat_run(sim_time2)\n",
    "    data = np.array(data)\n",
    "    \n",
    "    # Obtain error for radial reaches\n",
    "    q1 = plant_data[P][:,0]\n",
    "    q2 = plant_data[P][:,2]\n",
    "    q12 = q1+q2\n",
    "    c_elbow = np.array((plant.l_arm*np.cos(q1), plant.l_arm*np.sin(q1)))\n",
    "    c_hand = np.array((c_elbow[0] + plant.l_farm*np.cos(q12),\n",
    "                       c_elbow[1] + plant.l_farm*np.sin(q12))).transpose()\n",
    "    coord_idxs = np.floor((times-start_t)/t_pres).astype(int) # after resetting the targets\n",
    "    des_coords = np.array([hand_coords[idx] for idx in [m_idxs[cid] for cid in coord_idxs]])\n",
    "    hand_error = np.linalg.norm(c_hand-des_coords, axis=1)\n",
    "    error_r = np.mean(hand_error)\n",
    "    \n",
    "    # Obtaining the prefered directions for the units in M\n",
    "    \n",
    "    # Find the mean rate for each target\n",
    "    I = 12 # number of units to consider in M\n",
    "    J = n_trgs # number of targets in radial reaching\n",
    "    K = n_rounds # number of times each target is presented\n",
    "    T = int(t_smp / net.min_delay) # number of time points\n",
    "    t_st_pd = 0.01 # time to start sampling for PD analysis\n",
    "    md = net.min_delay\n",
    "    m_tensor = get_pop_tensor(M[:I], J, K, T, t_st_pd, t_pres, trg_ids, data, md)\n",
    "    # m_tensor[i,j,k,t] is the activity of the i-th M \n",
    "    # unit for target j, on the k-th repetition, at time point t.\n",
    "    # Now we average across repetitions for the same target\n",
    "    m_tensor_trg_avg = m_tensor.sum(axis=2) / K\n",
    "    # m_tensor_trg_avg[i,j,t]: average activity of i-th M unit when\n",
    "    # reaching for target j, at time point t\n",
    "    m_avg_rates = np.zeros((I,J)) # average rate at each unit for each target\n",
    "    for trg in range(J):\n",
    "        m_avg_rates[:, trg] = np.mean(m_tensor_trg_avg[:, trg, :], axis=1)\n",
    "    m_means = np.mean(m_avg_rates, axis=1) # mean rate for each unit\n",
    "\n",
    "    # Obtaining preferred directions by fitting the rates using a plane, and using\n",
    "    # the angle of the projection of this plane's normal vector onto the XY plane\n",
    "    # The coefficients of the normal vector can be calculated using the least-squares\n",
    "    # method, which leads to a 3x3 linear system that is readily reduced to a 2x2 system.\n",
    "    trgs = targets - center # targets centered at the origin\n",
    "    xs = trgs[:,0]\n",
    "    ys = trgs[:,1]\n",
    "    cxs = xs - np.mean(xs) # \"centered\" x coordinates\n",
    "    cys = ys - np.mean(ys) #  \"centered\" Y coordinates\n",
    "\n",
    "    X11 = (xs * cxs).sum()\n",
    "    X12 = (xs * cys).sum()\n",
    "    X21 = (ys * cxs).sum()\n",
    "    X22 = (ys * cys).sum()\n",
    "\n",
    "    Amat = np.array([[X11, X12], [X21, X22]])\n",
    "    detA = np.linalg.det(Amat) # determinant of Amat\n",
    "    if detA == 0.:\n",
    "        raise ValueError('Indeterminate system found!')\n",
    "    invA = np.linalg.inv(Amat)\n",
    "\n",
    "    prf_angs = np.zeros(len(M)) # preferred angles, in radians\n",
    "    normal_vecs = [] # list with the vectors normal to the plane fitting the rates\n",
    "\n",
    "    cs_of_d = np.zeros(I)\n",
    "    for uid in range(I):\n",
    "        r1 = (xs * (m_avg_rates[uid, :] - m_means[uid])).sum()\n",
    "        r2 = (ys * (m_avg_rates[uid, :] - m_means[uid])).sum()\n",
    "        n = np.matmul(invA, np.array([r1,r2]))\n",
    "        # boils down to\n",
    "        #n = [a,b] where b = r2/X22, a = r1/X11\n",
    "        #normal_vecs.append(n / np.linalg.norm(n)) # appending normalized vector\n",
    "        normal_vecs.append(n)\n",
    "        prf_angs[uid] = np.arctan2(n[1], n[0]) # preferred angle\n",
    "        #print(\"n1=%f, n0=%f, prf_ang=%f\"%(n[1],n[0]))\n",
    "        c = np.mean(m_avg_rates[uid,:] - n[0]*xs - n[1]*ys)\n",
    "        # obtaining residuals, coefficient of determination, R^2\n",
    "        residuals = m_avg_rates[uid,:] - n[0]*xs - n[1]*ys - c\n",
    "        devs = m_avg_rates[uid,:] - m_means[uid]\n",
    "        SSr = (residuals * residuals).sum()\n",
    "        SSt = (devs * devs).sum()\n",
    "        R = 1. - (SSr/SSt)\n",
    "        cs_of_d[uid] = R\n",
    "\n",
    "    normal_vecs = np.array(normal_vecs)\n",
    "\n",
    "    # Get the lengths of the preferred direction vectors\n",
    "    pd_norms = np.linalg.norm(normal_vecs, axis=1)\n",
    "    \n",
    "    # Assess if significantly tuned to direction\n",
    "\n",
    "    N_shuffles = 10000 # number of target permutations to produce\n",
    "    sh_pd_norms = np.zeros((I, N_shuffles)) # magnitude of PD vector for each cell, all shuffles\n",
    "    #m_rates_tensor = np.mean(m_tensor, axis=3)\n",
    "    # m_rates_tensor[i,j,k]: mean firing rate for cell i on k-th presentation of target j\n",
    "    for n_idx in range(N_shuffles):\n",
    "        sh_trg_ids = np.random.permutation(trg_ids)\n",
    "        sh_m_tensor = get_pop_tensor(M[:I], J, K, \n",
    "                                     T, t_st_pd, t_pres, sh_trg_ids, data, md)\n",
    "        sh_m_tensor_trg_avg = sh_m_tensor.sum(axis=2) / K\n",
    "        sh_m_avg_rates = np.zeros((I,J)) # average rate at each unit for each target\n",
    "        for trg in range(J):\n",
    "            sh_m_avg_rates[:, trg] = np.mean(sh_m_tensor_trg_avg[:, trg, :], axis=1)\n",
    "        sh_m_means = np.mean(sh_m_avg_rates, axis=1) # mean rate for each unit\n",
    "        sh_normal_vecs = np.zeros((I,2))\n",
    "        for uid in range(I):\n",
    "            r1 = (xs * (sh_m_avg_rates[uid, :] - sh_m_means[uid])).sum()\n",
    "            r2 = (ys * (sh_m_avg_rates[uid, :] - sh_m_means[uid])).sum()\n",
    "            n = np.matmul(invA, np.array([r1,r2]))\n",
    "            sh_normal_vecs[uid,:] = n\n",
    "        sh_pd_norms[:, n_idx] = np.linalg.norm(sh_normal_vecs, axis=1)\n",
    "\n",
    "    # Now, for each unit, find vlue of sh_pd_norms that is larger than 99%\n",
    "    sig_threshs = np.percentile(sh_pd_norms, 99.9, axis=1)\n",
    "\n",
    "    # For each unit, check if it is significantly tuned\n",
    "    tuned = pd_norms >= sig_threshs\n",
    "    n_tuned = tuned.sum()\n",
    "    \n",
    "    # Decode which target was reached based on the  M firing rate\n",
    "    # We get the sum of prefered directions, each modulated by the\n",
    "    # average firing rate. The desired target will be the one closest to\n",
    "    # the decoded direction.\n",
    "    # We can do this for all individual reaches\n",
    "\n",
    "    normal_normal_vecs = [vec / np.linalg.norm(vec) for vec in normal_vecs]\n",
    "    pred_dirs = np.empty((J,K), dtype=np.int_)\n",
    "    #pred_dirs(j,k): predicted direction on reach j, repetition k\n",
    "    for target in range(J):\n",
    "        for reach in range(K):\n",
    "            # we obtain the average firing rate for all cells at that reach\n",
    "            m_reach_avg = np.mean(m_tensor[:, target, reach, :], axis=1).reshape(12,1)\n",
    "            pd = (m_reach_avg * normal_normal_vecs).sum(axis=0).reshape(2,1) # predicted direction\n",
    "            # We select the target with the largest inner product with pd\n",
    "            # trgs (obtained before) has the targets centered at the origin\n",
    "            dot_prods = np.matmul(trgs, pd)\n",
    "            pred_dir = np.argmax(dot_prods)\n",
    "            pred_dirs[target, reach] = pred_dir\n",
    "    \n",
    "    pdrows, pdcols = pred_dirs.shape\n",
    "    pred_frac = (np.tile(np.arange(pdrows).reshape(pdrows,1), pdcols) == \n",
    "                 pred_dirs).sum() / pred_dirs.size\n",
    "    \n",
    "    # Obtaining bimodal Rayleigh r statistics, big norms\n",
    "    # These are twice the value of those in Lillicrap and Scott 2013\n",
    "    big_norms_idx = np.arange(len(pd_norms))[pd_norms > np.mean(pd_norms)]\n",
    "    sig_prf_angs = prf_angs[big_norms_idx]\n",
    "    r_bimod = 2. * ((np.sin(2.*sig_prf_angs).sum())**2 + \n",
    "                    (np.cos(2.*sig_prf_angs).sum())**2) / sig_prf_angs.size\n",
    "    # Significance threshold\n",
    "    a = .99\n",
    "    x_c = np.sqrt(-2.*np.log(1 - a))\n",
    "    bimodal = r_bimod > x_c\n",
    "\n",
    "    # jPCA analysis\n",
    "    grid_len = 50\n",
    "    t_smpl_grid = np.linspace(0.1, 2., grid_len)\n",
    "    t_strt_grid = np.linspace(0., 1.2, grid_len)\n",
    "    jpca_anas_norm = []\n",
    "    vp_norm_grid = np.zeros((grid_len, grid_len))\n",
    "    of_norm_grid = np.zeros((grid_len, grid_len))\n",
    "    r2_norm_grid = np.zeros((grid_len, grid_len))\n",
    "    for smp_idx, t_smpl in enumerate(t_smpl_grid):\n",
    "        for strt_idx, t_strt in enumerate(t_strt_grid):\n",
    "            jpca_anas_norm.append(jpca_ana(M[:I], t_smpl, t_strt,\n",
    "                                           t_pres, J, K, trg_ids, data, md, normalize=True))\n",
    "            vp_norm_grid[smp_idx, strt_idx] = jpca_anas_norm[-1]['var_per'][0]\n",
    "            of_norm_grid[smp_idx, strt_idx] = jpca_anas_norm[-1]['orth_fr']\n",
    "            r2_norm_grid[smp_idx, strt_idx] = jpca_anas_norm[-1]['R2_skew']\n",
    "            \n",
    "#     jpca_anas_unorm = []\n",
    "#     vp_unorm_grid = np.zeros((len(t_smpl_grid), len(t_strt_grid)))\n",
    "#     of_unorm_grid = np.zeros((len(t_smpl_grid), len(t_strt_grid)))\n",
    "#     r2_unorm_grid = np.zeros((len(t_smpl_grid), len(t_strt_grid)))\n",
    "#     for smp_idx, t_smpl in enumerate(t_smpl_grid):\n",
    "#         for strt_idx, t_strt in enumerate(t_strt_grid):\n",
    "#             jpca_anas_unorm.append(jpca_ana(M[:I], t_smpl, t_strt,\n",
    "#                                             t_pres, J, K, trg_ids, data, md, normalize=False))\n",
    "#             vp_unorm_grid[smp_idx, strt_idx] = jpca_anas_unorm[-1]['var_per'][0]\n",
    "#             of_unorm_grid[smp_idx, strt_idx] = jpca_anas_unorm[-1]['orth_fr']\n",
    "#             r2_unorm_grid[smp_idx, strt_idx] = jpca_anas_unorm[-1]['R2_skew']\n",
    "    \n",
    "    max_var_per = max(vp_norm_grid.flatten())\n",
    "    max_orth_fr = max(of_norm_grid.flatten())\n",
    "    max_vp_of = max((vp_norm_grid*of_norm_grid).flatten())\n",
    "    \n",
    "    results = {\n",
    "            'error_l':error_l, \n",
    "            'error_r':error_r, \n",
    "            'cs_of_d':cs_of_d, \n",
    "            'n_tuned':n_tuned,\n",
    "            'pred_frac':pred_frac,\n",
    "            'bimodal':bimodal,\n",
    "            'max_var_per':max_var_per,\n",
    "            'max_orth_fr':max_orth_fr,\n",
    "            'max_vp_of':max_vp_of }\n",
    "    \n",
    "    print(\"Run of reach_ana finished, new_gain=%f, new_noise=%f, r=%f\"%(new_gain, new_noise,r))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 32 processes\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.100000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.100000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.100000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.000000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.000000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.000000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.100000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.100000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.100000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.100000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.000000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.100000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.000000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.000000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.000000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.000000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.000000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.000000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.000000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.100000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.100000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.000000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.300000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.300000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.300000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.300000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.300000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.300000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.300000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.300000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.300000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.300000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.000000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.100000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.000000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.100000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.000000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.100000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.000000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.100000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.000000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.100000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.100000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.100000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.000000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.100000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.000000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.000000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.100000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.000000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.000000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.000000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.000000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.100000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.300000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.300000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.300000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.300000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.300000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.300000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.300000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.300000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.300000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.300000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.000000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.100000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.000000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.100000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.000000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.300000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.300000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.000000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.100000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.100000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.300000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.100000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.300000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.000000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.100000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.000000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.300000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.300000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.100000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.100000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.300000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.300000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.000000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.000000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.100000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.100000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.000000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.000000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.300000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.300000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.100000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.100000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.000000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.000000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.000000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.100000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.000000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.100000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.000000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.300000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.100000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.300000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.100000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.100000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.300000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.000000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.300000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.100000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.300000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.300000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.100000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.100000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.000000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.000000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.100000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.300000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.300000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.100000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.000000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.000000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.300000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.300000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.100000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.100000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.000000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.100000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.000000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.100000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.300000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.300000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.000000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.000000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.000000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.000000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.100000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.100000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.300000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.300000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.300000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.100000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.300000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.100000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.300000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.000000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.300000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.000000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.100000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.100000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.300000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.300000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.000000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.000000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.100000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.100000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.300000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.300000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.000000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.300000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.000000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.100000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.100000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.000000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.000000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.300000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.100000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.100000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.000000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.000000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.100000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.100000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.300000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.000000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.000000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.300000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.100000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.100000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.300000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.300000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.300000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.300000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.300000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.300000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.000000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.000000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.100000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.100000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.300000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.000000, new_noise=0.300000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.000000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.100000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.100000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.300000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.000000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.300000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.000000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.000000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.100000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.300000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.300000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.100000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.000000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.000000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.100000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.100000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.300000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.200000, new_noise=0.300000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.000000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.000000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.100000, r=0.100000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.300000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.100000, r=0.120000\n",
      "Run of reach_ana finished, new_gain=1.500000, new_noise=0.300000, r=0.100000\n",
      "****** Processing finished after 76931.19603729248 seconds ******\n"
     ]
    }
   ],
   "source": [
    "# Parallel runs of 'reach_ana'\n",
    "n_procs = 32\n",
    "print('Starting %d processes' % (n_procs))\n",
    "start_time = time.time()\n",
    "with Pool(n_procs) as p:\n",
    "    all_results = list(p.map(reach_ana, arg_dicts))\n",
    "    p.close()\n",
    "    p.join()\n",
    "print('****** Processing finished after %s seconds ******' % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results\n",
    "import pickle\n",
    "fname = time.strftime('/home/z/Dropbox (OIST)/saves/reach_anaB_%Y-%m-%d', time.localtime())\n",
    "with open(fname, 'wb') as f:\n",
    "    pickle.dump(all_results, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort results according to a \"score\" value\n",
    "def score_result(res):\n",
    "    \"\"\" Test the properties of a simulation result.\n",
    "    \n",
    "        In this context, good properties (which raise the score) are:\n",
    "        1* error_l and error_r are below threshold values.\n",
    "        2* At least 80% of the cs_of_d values are above a threshold.\n",
    "        3* n_tuned is 8 or more (assuming tuning analysis uses 12 units).\n",
    "        4* pred_frac is above 0.4\n",
    "        5* bimodal PD distribution\n",
    "        6* max_var_per, max_orth_fr, and max_vp_of are beyond threshold values\n",
    "        Args:\n",
    "            res : results dictionary as returned by reach_ana.\n",
    "            \n",
    "        Returns:\n",
    "            A score indicating the \"goodness\" of the simulation.\n",
    "    \"\"\"\n",
    "    score = 0.\n",
    "    if res['error_l'] < 0.1: score += 1.\n",
    "    if res['error_r'] < 0.05: score += 4.\n",
    "    score += 3.*(0.5 - res['error_r'])\n",
    "    cod = res['cs_of_d']\n",
    "    if (cod > 0.8).sum() / cod.size > 0.8: score += 1.\n",
    "    if res['pred_frac'] > 0.4: score += 1.\n",
    "    if res['bimodal']: score += 3.\n",
    "    if res['max_var_per'] > 0.7: score += 1 + res['max_var_per']\n",
    "    if res['max_orth_fr'] > 0.4: score += 1 + res['max_orth_fr']\n",
    "    if res['max_vp_of'] > 0.2: score += 2 + 2 * res['max_vp_of']\n",
    "        \n",
    "    return score\n",
    "\n",
    "# make sure you join the argument dictionary, or the result will be lost after sorting\n",
    "for idx, result in enumerate(all_results):\n",
    "    result['arg_dict'] = arg_dicts[idx]\n",
    "\n",
    "# sort\n",
    "all_results.sort(key = score_result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215: cfg = pso28_0,\t use_syne = True,\t gain = 1.5,\t noise=0.0,\t r=0.1,\t error_r=0.042,\t score=11.75\n",
      "214: cfg = pso28_0,\t use_syne = True,\t gain = 1.5,\t noise=0.1,\t r=0.1,\t error_r=0.035,\t score=11.67\n",
      "213: cfg = pso28_0,\t use_syne = True,\t gain = 1.2,\t noise=0.3,\t r=0.1,\t error_r=0.046,\t score=11.61\n",
      "212: cfg = pso28_0,\t use_syne = True,\t gain = 1.5,\t noise=0.1,\t r=0.1,\t error_r=0.042,\t score=11.55\n",
      "211: cfg = pso28_0,\t use_syne = True,\t gain = 1.2,\t noise=0.3,\t r=0.1,\t error_r=0.039,\t score=11.55\n",
      "210: cfg = gene17_1,\t use_syne = False,\t gain = 1.0,\t noise=0.0,\t r=0.1,\t error_r=0.043,\t score=11.54\n",
      "209: cfg = pso28_0,\t use_syne = True,\t gain = 1.2,\t noise=0.1,\t r=0.1,\t error_r=0.044,\t score=11.50\n",
      "208: cfg = pso28_0,\t use_syne = True,\t gain = 1.5,\t noise=0.3,\t r=0.1,\t error_r=0.047,\t score=11.47\n",
      "207: cfg = pso28_0,\t use_syne = True,\t gain = 1.2,\t noise=0.0,\t r=0.1,\t error_r=0.043,\t score=11.45\n",
      "206: cfg = pso28_0,\t use_syne = True,\t gain = 1.5,\t noise=0.3,\t r=0.1,\t error_r=0.040,\t score=11.45\n",
      "205: cfg = gene17_1,\t use_syne = False,\t gain = 1.0,\t noise=0.1,\t r=0.1,\t error_r=0.043,\t score=11.45\n",
      "204: cfg = gene17_1,\t use_syne = True,\t gain = 1.0,\t noise=0.3,\t r=0.1,\t error_r=0.050,\t score=11.28\n",
      "203: cfg = gene17_1,\t use_syne = False,\t gain = 1.5,\t noise=0.0,\t r=0.1,\t error_r=0.041,\t score=10.68\n",
      "202: cfg = pso28_0,\t use_syne = True,\t gain = 1.2,\t noise=0.0,\t r=0.1,\t error_r=0.035,\t score=10.60\n",
      "201: cfg = pso28_0,\t use_syne = True,\t gain = 1.2,\t noise=0.1,\t r=0.1,\t error_r=0.035,\t score=10.56\n",
      "200: cfg = gene17_1,\t use_syne = False,\t gain = 1.0,\t noise=0.3,\t r=0.1,\t error_r=0.044,\t score=10.55\n",
      "199: cfg = gene17_1,\t use_syne = False,\t gain = 1.5,\t noise=0.1,\t r=0.1,\t error_r=0.042,\t score=10.53\n",
      "198: cfg = gene17_1,\t use_syne = False,\t gain = 1.2,\t noise=0.0,\t r=0.1,\t error_r=0.042,\t score=10.53\n",
      "197: cfg = gene17_1,\t use_syne = False,\t gain = 1.5,\t noise=0.1,\t r=0.1,\t error_r=0.050,\t score=10.49\n",
      "196: cfg = gene17_1,\t use_syne = False,\t gain = 1.5,\t noise=0.0,\t r=0.1,\t error_r=0.050,\t score=10.48\n",
      "195: cfg = gene17_1,\t use_syne = False,\t gain = 1.2,\t noise=0.3,\t r=0.1,\t error_r=0.043,\t score=10.48\n",
      "194: cfg = gene17_1,\t use_syne = False,\t gain = 1.2,\t noise=0.1,\t r=0.1,\t error_r=0.042,\t score=10.43\n",
      "193: cfg = gene17_1,\t use_syne = False,\t gain = 1.5,\t noise=0.3,\t r=0.1,\t error_r=0.043,\t score=10.40\n",
      "192: cfg = gene17_0,\t use_syne = False,\t gain = 1.2,\t noise=0.1,\t r=0.1,\t error_r=0.050,\t score=10.40\n",
      "191: cfg = gene17_0,\t use_syne = False,\t gain = 1.2,\t noise=0.0,\t r=0.1,\t error_r=0.050,\t score=10.38\n"
     ]
    }
   ],
   "source": [
    "# basic info of the best configurations\n",
    "\n",
    "# assuming this is the list of the paramter dictionaries tested\n",
    "# cfg_list = [pop_gene17[0], pop_gene17[1],\n",
    "#             pop_gene24[0], pop_gene24[1],\n",
    "#             pop_pso24[0], pop_pso28[0]]\n",
    "cfg_names = ['gene17_0', 'gene17_1',\n",
    "             'gene24_0',  'gene24_1',\n",
    "             'pso24_0', 'pso28_0']\n",
    "\n",
    "all_scores = [score_result(res) for res in all_results]\n",
    "cfg_name = \"unidentified\"\n",
    "for rid in 215 - np.arange(25):\n",
    "    ar = all_results[rid]\n",
    "    ad = ar['arg_dict']\n",
    "    for idx, cfg in enumerate(cfg_list):\n",
    "        if ad['cfg'] == cfg:\n",
    "            cfg_name = cfg_names[idx]\n",
    "    #print('\\n')\n",
    "    print(\"%d: cfg = %s,\\t use_syne = %r,\\t gain = %.1f,\\t noise=%.1f,\\t r=%.1f,\\t error_r=%.3f,\\t score=%.2f\" % \n",
    "          (rid, cfg_name, ad['use_syne'], ad['new_gain'], ad['new_noise'], ad['r'], ar['error_r'], all_scores[rid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, result in enumerate(all_results):\n",
    "    if result['bimodal']:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error_l': 0.37641192736506235,\n",
       " 'error_r': 0.44895453941903074,\n",
       " 'cs_of_d': array([0.93880991, 0.97002661, 0.5562339 , 0.93072628, 0.94691601,\n",
       "        0.64231311, 0.92057769, 0.76750475, 0.95084783, 0.91440479,\n",
       "        0.76911759, 0.65425921]),\n",
       " 'n_tuned': 12,\n",
       " 'pred_frac': 0.1875,\n",
       " 'bimodal': False,\n",
       " 'max_var_per': 0.5354324157063616,\n",
       " 'max_orth_fr': 0.39646464646464646,\n",
       " 'max_vp_of': 0.1825966733151716,\n",
       " 'arg_dict': {'cfg': {'A__M_lrate': 20.0,\n",
       "   'A__M_w_max_frac': 0.2908290235697395,\n",
       "   'A__M_w_sum': 1.0,\n",
       "   'AL_thresh': 0.5463030332331179,\n",
       "   'b_e': 1.5168901388725107,\n",
       "   'C__C_antag': 1.4821065342072308,\n",
       "   'C__C_p_antag': 0.25626159224477324,\n",
       "   'C__C_p_syne': 0.27659708981208353,\n",
       "   'C__C_syne': 1.000068222850724,\n",
       "   'C_adapt_amp': 0.4008114255784593,\n",
       "   'C_cid': 0.1453285368074421,\n",
       "   'C_sigma': 0.5224555092418061,\n",
       "   'C_slope': 2.0817328914108666,\n",
       "   'C_tau': 0.23039414970020586,\n",
       "   'C_tau_slow': 40.0,\n",
       "   'C_thresh': 0.9311160472269504,\n",
       "   'CE__CI_w': 0.49724117010271823,\n",
       "   'CI__CE_w': -1.3124989560985578,\n",
       "   'g_e_03': 24.84611964744491,\n",
       "   'CI_slope': 3.5977915332991786,\n",
       "   'CI_tau': 0.016775099735600475,\n",
       "   'CI_thresh': 1.4111834093154787,\n",
       "   'g_e_factor': 3.1959608945233744,\n",
       "   'II_g_03': 3.165933092061302,\n",
       "   'M__C_lrate': 205.61246754927993,\n",
       "   'M__C_w_sum': 2.486428727542015,\n",
       "   'M__M_w': 0.0,\n",
       "   'M_cid': 1.09924838078524,\n",
       "   'M_des_out_w_abs_sum': 3.0691051128467475,\n",
       "   'M_tau': 0.023580007413035398,\n",
       "   'SF_thresh_03': 0.6257670563340819,\n",
       "   'SPF__SPF_w': -1.4973503041848586,\n",
       "   'fitness': 0.03133777291847325,\n",
       "   'n_evals': 1,\n",
       "   't_pres': 40,\n",
       "   'par_heter': 0.01},\n",
       "  'n_reaches_l': 16,\n",
       "  'use_syne': True,\n",
       "  'set_C_delay': False,\n",
       "  'rand_targets': True,\n",
       "  'C_noise': True,\n",
       "  'M__C_rand_w': True,\n",
       "  'permute_targets': True,\n",
       "  'new_noise': 0.3,\n",
       "  'new_gain': 1.0,\n",
       "  't_pres': 5.0,\n",
       "  'n_rounds': 6,\n",
       "  'r': 0.1,\n",
       "  't_smp': 1.0}}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
